{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5f3af42",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ade18",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to scale numerical features within a specific range. The goal of Min-Max scaling is to transform the features so that they all have values between a minimum and maximum value, typically between 0 and 1. This scaling method is particularly useful when features have different ranges and units, which can impact the performance of machine learning algorithms.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "\\[ Scaled Value = (Original Value- Min Value) / (Max Value - Min Value) \\]\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\(Original Value) is the original value of the feature.\n",
    "\n",
    "- \\(Min Value\\) is the minimum value of the feature in the dataset.\n",
    "\n",
    "- \\(Max Value\\) is the maximum value of the feature in the dataset.\n",
    "\n",
    "The scaled value will fall between 0 and 1, inclusive.\n",
    "\n",
    "**Importance of Min-Max Scaling:**\n",
    "Min-Max scaling is important for several reasons:\n",
    "1. **Feature Scaling:** It ensures that all features are on the same scale, preventing features with larger magnitudes from dominating those with smaller magnitudes during the modeling process.\n",
    "2. **Algorithms Sensitivity:** Some machine learning algorithms are sensitive to the scale of the input features. Min-Max scaling helps these algorithms perform better.\n",
    "3. **Convergence:** Scaling can help algorithms converge faster during training, particularly for optimization algorithms that rely on gradient descent.\n",
    "\n",
    "**Example:**\n",
    "Consider a dataset with a feature representing age, where ages range from 20 to 60. By applying Min-Max scaling, you can transform the age values to a range between 0 and 1. If the age of a person is 30, after scaling, it would become:\n",
    "\n",
    "\\[ Scaled Age = {30 - 20} / {60 - 20} = 0.25 \\]\n",
    "\n",
    "And if another person's age is 45, after scaling, it would become:\n",
    "\\[ Scaled Age = {45 - 20} / {60 - 20} = 0.5 \\]\n",
    "\n",
    "Min-Max scaling is a simple yet effective technique to ensure that numerical features are appropriately scaled for machine learning algorithms, helping to improve the overall performance and convergence of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93939b0",
   "metadata": {},
   "source": [
    "Min-Max scaling is used in data preprocessing to transform numerical features within a specific range (typically between 0 and 1), making them suitable for machine learning algorithms. Here's how Min-Max scaling is used in the data preprocessing pipeline:\n",
    "\n",
    "1. **Data Collection:** Gather the raw data, including both the features (attributes) and the target variable.\n",
    "\n",
    "2. **Data Exploration:** Understand the distribution and range of each feature. Some features may have different scales, which can lead to biased results in machine learning algorithms.\n",
    "\n",
    "3. **Min-Max Scaling:** Apply Min-Max scaling to each numerical feature using the formula:\n",
    "\n",
    "\\[ Scaled Value = (Original Value- Min Value) / (Max Value - Min Value) \\]\n",
    "\n",
    "   where the minimum and maximum values are calculated based on the entire dataset or specific feature.\n",
    "\n",
    "4. **Normalized Data:** After applying Min-Max scaling, the features are transformed to a common scale between 0 and 1.\n",
    "\n",
    "5. **Feature Transformation:** Replace the original feature values with the scaled values. This ensures that all features are on the same scale, mitigating the influence of features with larger magnitudes.\n",
    "\n",
    "6. **Machine Learning:** Use the scaled features as input to machine learning algorithms. Algorithms that use distance metrics, such as K-nearest neighbors or clustering algorithms, benefit from scaled features, as features with smaller scales are not overshadowed by those with larger scales.\n",
    "\n",
    "7. **Model Evaluation:** Train and evaluate the machine learning models using the scaled features. The scaled features help algorithms converge faster and improve overall model performance.\n",
    "\n",
    "8. **Prediction:** When making predictions on new data, remember to scale the new data using the same scaling factors derived from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd7cd0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92cdfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data : \n",
      "    Age  Income\n",
      "0   25   50000\n",
      "1   30   60000\n",
      "2   35   75000\n",
      "3   40   80000\n",
      "4   45   90000\n",
      "\n",
      "\n",
      "Scaled Data : \n",
      "     Age  Income\n",
      "0  0.00   0.000\n",
      "1  0.25   0.250\n",
      "2  0.50   0.625\n",
      "3  0.75   0.750\n",
      "4  1.00   1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataset\n",
    "data = {'Age': [25, 30, 35, 40, 45],\n",
    "        'Income': [50000, 60000, 75000, 80000, 90000]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert scaled data back to DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "print(\"Original Data : \\n\", df)\n",
    "print(\"\\n\\nScaled Data : \\n\", scaled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e82642d",
   "metadata": {},
   "source": [
    "In this example, the `MinMaxScaler` scales the 'Age' and 'Income' features to the range [0, 1].\n",
    "\n",
    "Min-Max scaling is a crucial step in data preprocessing to ensure that features are appropriately scaled, leading to better performance and more reliable results from machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e0434",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e1a9b",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Vector Normalization or L2 Normalization, is a method used to scale feature vectors in such a way that they have a Euclidean norm (magnitude) of 1 while preserving their original direction. This technique is commonly used to ensure that features with varying scales contribute equally to the analysis and to improve the performance of machine learning algorithms that are sensitive to the magnitude of feature values.\n",
    "\n",
    "Here's how the Unit Vector technique works:\n",
    "\n",
    "1. **Calculation of Euclidean Norm:** For each feature vector (row) in your dataset, you calculate its Euclidean norm, which is the square root of the sum of squared values of its components. Mathematically, for a feature vector x, its Euclidean norm ||x|| is calculated as:\n",
    "\n",
    "   ```\n",
    "   ||x|| = sqrt(x1^2 + x2^2 + ... + xn^2)\n",
    "   ```\n",
    "\n",
    "   Where x1, x2, ..., xn are the components (features) of the vector.\n",
    "\n",
    "2. **Normalization Process:** The feature vector is then scaled by dividing each of its components by its Euclidean norm. This process ensures that the transformed vector has a magnitude of 1 while retaining its original direction. Mathematically, the normalized feature vector x_unit is calculated as:\n",
    "\n",
    "   ```\n",
    "   x_unit = x / ||x||\n",
    "   ```\n",
    "\n",
    "   Where x_unit is the normalized feature vector and x is the original feature vector.\n",
    "\n",
    "The primary advantage of the Unit Vector technique is that it helps in reducing the impact of varying scales among features, making the dataset more suitable for algorithms that calculate distances or similarities between feature vectors. It's particularly useful in scenarios such as text classification, recommendation systems, and clustering, where the similarity between data points matters more than the absolute scale of their values.\n",
    "\n",
    "By ensuring that all features have similar magnitudes, the Unit Vector technique can prevent features with larger scales from dominating the analysis or influencing certain algorithms disproportionately.\n",
    "\n",
    "It's important to note that this technique is different from standard scaling methods like Min-Max Scaling or Z-score Scaling, which transform feature values to specific ranges or distributions. The Unit Vector technique focuses on ensuring that the vectors themselves have a consistent magnitude while maintaining their direction.\n",
    "\n",
    "The Unit Vector technique and Min-Max Scaling are both methods of feature scaling, but they serve different purposes and work in different ways:\n",
    "\n",
    "1. **Purpose:**\n",
    "   - **Unit Vector Technique:** The primary purpose of the Unit Vector technique (L2 Normalization) is to ensure that the vectors have a consistent magnitude of 1 while preserving their direction. It's often used in scenarios where the relative relationships between feature vectors matter more than their absolute scales, such as in similarity-based algorithms.\n",
    "   - **Min-Max Scaling:** The purpose of Min-Max Scaling is to transform feature values to a specific range, typically between 0 and 1, by linearly mapping the original values. It's used to make sure that all features are on a similar scale, which is important for algorithms that consider the magnitudes of feature values.\n",
    "\n",
    "2. **Scaling Method:**\n",
    "   - **Unit Vector Technique:** The Unit Vector technique scales the entire feature vector by dividing it by its Euclidean norm (magnitude), ensuring that the transformed vector has a magnitude of 1. This keeps the direction of the vector intact while adjusting its length.\n",
    "   - **Min-Max Scaling:** Min-Max Scaling transforms each feature independently by subtracting the minimum value and then dividing by the range (difference between the maximum and minimum values). This method ensures that the scaled values are within a specific range, typically between 0 and 1.\n",
    "\n",
    "3. **Effect on Features:**\n",
    "   - **Unit Vector Technique:** The Unit Vector technique doesn't change the direction of the feature vector but only adjusts its magnitude. The relationship between the components of the vector remains the same.\n",
    "   - **Min-Max Scaling:** Min-Max Scaling transforms the values of each feature to a specific range. This can change the distribution of the feature values and the relationship between them.\n",
    "\n",
    "4. **Use Cases:**\n",
    "   - **Unit Vector Technique:** It's commonly used in scenarios where cosine similarity or other similarity measures are important, such as in natural language processing (text classification, recommendation systems) or clustering.\n",
    "   - **Min-Max Scaling:** It's used in scenarios where the absolute values of features matter, and you want to ensure that all features are on a similar scale, like in algorithms that involve distance calculations or gradients in optimization.\n",
    "\n",
    "Certainly! Let's consider a practical example to illustrate the application of the Unit Vector technique (L2 Normalization) in feature scaling:\n",
    "\n",
    "**Example: Document Classification**\n",
    "\n",
    "Imagine you're working on a text classification task where you need to classify documents into different categories. You're using a technique called Term Frequency-Inverse Document Frequency (TF-IDF) to represent the documents as feature vectors. TF-IDF calculates the importance of words in a document relative to a corpus of documents.\n",
    "\n",
    "In this scenario, you have a collection of documents, and each document is represented as a TF-IDF vector. The length of each vector (magnitude) represents the importance of the words in the document.\n",
    "\n",
    "Here's how the Unit Vector technique can be applied:\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - You've already transformed your documents into TF-IDF vectors, resulting in a matrix where each row corresponds to a document, and each column corresponds to a word's TF-IDF value.\n",
    "\n",
    "2. **Unit Vector Technique:**\n",
    "   - For each TF-IDF vector (document), apply the Unit Vector technique (L2 Normalization).\n",
    "   - For each vector, divide all its components by the Euclidean norm of the vector (its magnitude).\n",
    "\n",
    "3. **Effect:**\n",
    "   - By applying the Unit Vector technique, you're ensuring that the length (magnitude) of each TF-IDF vector becomes 1 while preserving the direction of the vector. This ensures that the vectors are unit vectors, meaning they all have the same length of 1.\n",
    "\n",
    "**Why Use Unit Vector Technique:**\n",
    "- In text classification, the relative importance of words within a document matters more than the absolute values. By using unit vectors, you're effectively emphasizing the relative importance of words within each document.\n",
    "- The cosine similarity between two unit vectors becomes a meaningful measure of how similar two documents are in terms of their word usage.\n",
    "\n",
    "By applying the Unit Vector technique, you create a standardized representation of documents where their lengths are normalized, ensuring that differences in document length don't dominate the similarity calculations.\n",
    "\n",
    "In contrast, if you were using Min-Max Scaling, you'd change the actual TF-IDF values, potentially altering the relative importance of words within the documents. This might not be desirable in the context of text classification where word importance is crucial.\n",
    "\n",
    " the choice of scaling technique depends on the specific characteristics of your data and the requirements of your machine learning task.\n",
    "\n",
    "In summary, while both the Unit Vector technique and Min-Max Scaling are methods of feature scaling, they serve different purposes and have different effects on the features. The Unit Vector technique ensures consistent vector magnitudes, while Min-Max Scaling transforms feature values to a specific range to ensure similar scales across features. The choice between them depends on the specific requirements of your machine learning task and the nature of your data. the Unit Vector technique in feature scaling is a method used to normalize feature vectors by adjusting their magnitudes to 1 while preserving their original directions. It's particularly useful for scenarios where the relative relationships between feature vectors matter more than their absolute scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f91363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a12911d",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf96bccd",
   "metadata": {},
   "source": [
    "PCA, which stands for Principal Component Analysis, is a dimensionality reduction technique used in data analysis and machine learning. It's commonly used to transform high-dimensional data into a lower-dimensional space while preserving as much of the original data's variance as possible. PCA achieves this by identifying the principal components (or directions) in the data that capture the most important information.\n",
    "\n",
    "Here's a more detailed explanation of PCA:\n",
    "\n",
    "1. **Motivation:**\n",
    "   In high-dimensional datasets, there may be redundant or less informative features that can lead to increased computation time and overfitting. PCA aims to reduce the number of features while retaining the essential characteristics of the data.\n",
    "\n",
    "2. **Principal Components:**\n",
    "   Principal components are orthogonal (uncorrelated) linear combinations of the original features. The first principal component captures the most variance in the data, and each subsequent component captures the highest variance orthogonal to the previous ones. These components are sorted by the amount of variance they explain.\n",
    "\n",
    "3. **Steps of PCA:**\n",
    "   - **Step 1: Standardization:** Standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all features have the same scale.\n",
    "   - **Step 2: Covariance Matrix:** Compute the covariance matrix of the standardized data. The covariance matrix describes the relationships between features.\n",
    "   - **Step 3: Eigenvectors and Eigenvalues:** Calculate the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors are the directions (principal components) that capture the variance, and eigenvalues represent the amount of variance along those directions.\n",
    "   - **Step 4: Selecting Principal Components:** Choose the top k eigenvectors based on their corresponding eigenvalues. These eigenvectors become the new axes in the lower-dimensional space.\n",
    "   - **Step 5: Projection:** Project the original data onto the new axes (principal components) to obtain the lower-dimensional representation.\n",
    "\n",
    "4. **Applications:**\n",
    "   - **Dimensionality Reduction:** PCA is used to reduce the number of dimensions while preserving the data's variability. This is useful for visualization and speeding up computations.\n",
    "   - **Noise Reduction:** By ignoring components with low variance, PCA can help reduce noise and focus on the most informative aspects of the data.\n",
    "   - **Feature Engineering:** PCA can be used as a preprocessing step to create new features that capture the most significant variation in the data.\n",
    "\n",
    "5. **Trade-offs:**\n",
    "   - Reducing dimensions might lead to loss of interpretability in the transformed features.\n",
    "   - The first few principal components typically capture most of the data's variance, but some information might still be lost.\n",
    "\n",
    "PCA is widely used for dimensionality reduction, which involves reducing the number of features or variables in a dataset while retaining as much of the original data's variability as possible. This is particularly beneficial in scenarios where high-dimensional data can lead to computational inefficiency, increased noise, or overfitting in machine learning models. Here's how PCA is used in dimensionality reduction:\n",
    "\n",
    "1. **Data Standardization:** The first step is to standardize the data by subtracting the mean and dividing by the standard deviation. This ensures that all features are on the same scale, preventing features with larger magnitudes from dominating the analysis.\n",
    "\n",
    "2. **Covariance Matrix:** The covariance matrix of the standardized data is computed. The covariance matrix describes the relationships and interactions between the features.\n",
    "\n",
    "3. **Eigenvalue Decomposition:** PCA involves calculating the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions (principal components) that capture the most variance in the data, and eigenvalues represent the amount of variance explained by each component.\n",
    "\n",
    "4. **Choosing Principal Components:** The eigenvectors are ranked by their corresponding eigenvalues, and the top k eigenvectors are selected. These eigenvectors correspond to the principal components that explain the most variance in the data.\n",
    "\n",
    "5. **Projection:** The original data is projected onto the selected principal components to obtain a lower-dimensional representation of the data. Each data point is transformed into a new set of coordinates along the principal components.\n",
    "\n",
    "By retaining the principal components that capture the most variance, PCA effectively reduces the dimensionality of the data while minimizing the loss of information. This lower-dimensional representation can be used for various purposes, including visualization, clustering, and machine learning.\n",
    "\n",
    "**Benefits of PCA for Dimensionality Reduction:**\n",
    "\n",
    "1. **Reduced Computation:** Working with a lower-dimensional dataset reduces the computational complexity of algorithms and models, leading to faster training and testing.\n",
    "\n",
    "2. **Noise Reduction:** Lower-variance dimensions (principal components) that capture noise or less meaningful variation can be discarded, leading to cleaner and more interpretable data.\n",
    "\n",
    "3. **Visualization:** High-dimensional data is challenging to visualize. PCA can project data into a 2D or 3D space, making it easier to understand and interpret.\n",
    "\n",
    "4. **Overfitting Prevention:** Reducing the number of features can help mitigate overfitting, as models are less likely to memorize noise in the data.\n",
    "\n",
    "5. **Data Interpretation:** The reduced dataset is easier to interpret and analyze, which is valuable for extracting insights and making informed decisions.\n",
    "\n",
    "PCA is a fundamental tool for dimensionality reduction in various domains, including image processing, genetics, natural language processing, and more. It helps in simplifying complex datasets while preserving the key patterns and relationships within the data.it is a powerful tool for data analysis and visualization. It helps in understanding the structure of complex data and can be applied to a wide range of domains, including image analysis, genetics, finance, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7f79b",
   "metadata": {},
   "source": [
    " example of using PCA for dimensionality reduction in image processing. Imagine you have a dataset of grayscale images of handwritten digits, each represented as a 28x28 pixel matrix. Each pixel can have values ranging from 0 to 255, indicating the intensity of the pixel.\n",
    "\n",
    "In this example, we'll use PCA to reduce the dimensionality of these images while retaining as much information as possible.\n",
    "\n",
    "**Step 1: Data Preparation**\n",
    "\n",
    "1. Collect a dataset of grayscale images of handwritten digits (e.g., the MNIST dataset).\n",
    "\n",
    "**Step 2: Data Preprocessing**\n",
    "\n",
    "1. Flatten each 28x28 image matrix into a vector of length 784.\n",
    "2. Standardize the pixel values by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "**Step 3: Apply PCA**\n",
    "\n",
    "1. Calculate the covariance matrix of the standardized image data.\n",
    "2. Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "3. Rank the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "4. Choose the top k eigenvectors that capture a significant amount of variance (e.g., 95%).\n",
    "\n",
    "**Step 4: Projection**\n",
    "\n",
    "1. Project the standardized image data onto the selected principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e65d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e172b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio for each component :\n",
      " [0.00445955 0.0044098  0.0042954  0.00427214 0.00424877 0.00424014\n",
      " 0.00422699 0.0041781  0.00416235 0.00411457 0.00406704 0.00405573\n",
      " 0.0040259  0.00398186 0.00396938 0.00394784 0.00393573 0.00392137\n",
      " 0.0038936  0.00387836 0.00384749 0.00383789 0.00381833 0.0037957\n",
      " 0.00378414 0.00374465 0.00370403 0.00369804 0.00368348 0.00366656\n",
      " 0.00364533 0.00361667 0.00358685 0.00356444 0.00355263 0.00353212\n",
      " 0.00352634 0.00349194 0.00348098 0.00347293 0.00344193 0.00342671\n",
      " 0.00341738 0.0034068  0.00337946 0.00335643 0.00333341 0.00329081\n",
      " 0.0032788  0.00324935]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Generate synthetic image data (replace with actual data)\n",
    "num_samples = 1000\n",
    "num_pixels = 784\n",
    "data = np.random.randint(0, 256, size=(num_samples, num_pixels))\n",
    "\n",
    "# Standardize the data\n",
    "data_standardized = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "\n",
    "# Apply PCA\n",
    "num_components = 50  # Number of principal components to keep\n",
    "pca = PCA(n_components=num_components)\n",
    "data_pca = pca.fit_transform(data_standardized)\n",
    "\n",
    "# Calculate the explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "print(f\"Explained variance ratio for each component :\\n {explained_variance_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9fe9fb",
   "metadata": {},
   "source": [
    "In this example, `data_pca` contains the lower-dimensional representation of the images, and `explained_variance_ratio` provides information about the amount of variance captured by each principal component.\n",
    "\n",
    "The result of this PCA application is a reduced-dimension dataset that captures the most important features of the images while discarding less informative dimensions. This reduced dataset can be used for tasks like classification, clustering, or visualization, with the advantage of lower computational complexity and potentially improved model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e128b07",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957e696f",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in the field of dimensionality reduction and machine learning. They both aim to reduce the number of features in a dataset while preserving as much relevant information as possible. However, they serve slightly different purposes and have distinct techniques.\n",
    "\n",
    "**PCA:**\n",
    "- PCA is a specific technique used for dimensionality reduction.\n",
    "- It's an unsupervised learning technique.\n",
    "- PCA finds a new set of orthogonal axes (principal components) in the original feature space that maximize the variance of the data projected onto them.\n",
    "- The first principal component captures the most variance, the second captures the second most, and so on.\n",
    "- By retaining a subset of these principal components, you can represent the data in a lower-dimensional space.\n",
    "- PCA is primarily used for reducing the dimensions of numeric data.\n",
    "\n",
    "**Feature Extraction:**\n",
    "- Feature extraction is a broader concept that involves transforming the original features into a new set of features.\n",
    "- It can include techniques beyond PCA, such as using domain-specific knowledge to derive new features.\n",
    "- Feature extraction can also involve techniques like wavelet transforms, Fourier transforms, or other mathematical operations to extract meaningful features from the data.\n",
    "- It can be both supervised (using class labels) and unsupervised (without class labels).\n",
    "- Feature extraction can be used for different purposes, including improving model performance, enhancing interpretability, and reducing noise.\n",
    "\n",
    "**Relationship:**\n",
    "- PCA can be thought of as a specific instance of feature extraction, where the new features are linear combinations of the original features.\n",
    "- PCA is often used as a feature extraction method when the primary goal is dimensionality reduction and retaining as much variance as possible.\n",
    "- While PCA focuses on maximizing variance and decorrelating features, other feature extraction methods might focus on specific domain knowledge or extracting features that are not necessarily orthogonal.\n",
    "- Feature extraction can involve a wider range of techniques, including nonlinear methods, while PCA is specifically a linear technique.\n",
    "\n",
    "PCA (Principal Component Analysis) can be used for feature extraction by transforming the original features into a new set of features, known as principal components. The primary goal of PCA is to reduce the dimensionality of the data while retaining as much variance as possible. Here's how PCA can be applied for feature extraction:\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Standardize or normalize the data to have a mean of 0 and a standard deviation of 1. This step ensures that features with larger scales do not dominate the PCA process.\n",
    "\n",
    "2. **Calculate Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized data. The covariance matrix represents the relationships between the original features.\n",
    "\n",
    "3. **Calculate Eigenvectors and Eigenvalues:**\n",
    "   - Calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors are the directions of maximum variance (principal components), and the eigenvalues represent the amount of variance explained by each component.\n",
    "\n",
    "4. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Sort the eigenvectors in descending order of their corresponding eigenvalues. This step ensures that the most important components (those explaining the most variance) come first.\n",
    "\n",
    "5. **Choose Principal Components:**\n",
    "   - Select a subset of the top principal components based on the desired level of dimensionality reduction. You can decide how many components to retain based on the cumulative explained variance or some other criterion.\n",
    "\n",
    "6. **Projection:**\n",
    "   - Project the original data onto the selected principal components. This transformation results in a new set of features, which are linear combinations of the original features.\n",
    "\n",
    "PCA can be used for various purposes in feature extraction:\n",
    "\n",
    "- **Dimensionality Reduction:** By selecting a smaller number of principal components, you reduce the dimensionality of the data while retaining most of its variance. This can help speed up training and improve model performance.\n",
    "\n",
    "- **Noise Reduction:** Principal components with low eigenvalues capture noise in the data. By excluding components with low eigenvalues, you can reduce the impact of noise in your analysis.\n",
    "\n",
    "- **Visualization:** PCA can be used to project high-dimensional data onto lower dimensions for visualization. This is particularly useful when dealing with complex datasets that are hard to visualize directly.\n",
    "\n",
    "- **Data Compression:** PCA can be used to compress data while preserving its essential characteristics. This is useful for storage and transmission of large datasets.\n",
    "\n",
    "PCA is a powerful tool, it assumes that the underlying structure in the data is best represented by a linear combination of the original features. In cases where the data has nonlinear relationships, other dimensionality reduction techniques like t-SNE or LLE (Locally Linear Embedding) might be more appropriate for feature extraction.\n",
    "\n",
    "\n",
    "In summary, PCA is a form of feature extraction, but feature extraction is a broader concept that encompasses various techniques beyond PCA. The choice between using PCA or other feature extraction methods depends on the specific problem, the nature of the data, and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b8591",
   "metadata": {},
   "source": [
    "example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Imagine you have a dataset containing information about houses, including features like the size of the house (in square feet), the number of bedrooms, the age of the house (in years), and the price. Your goal is to reduce the dimensionality of this dataset for analysis and visualization.\n",
    "\n",
    "Original Dataset:\n",
    "\n",
    "| House Size | Bedrooms | Age | Price |\n",
    "|------------|----------|-----|-------|\n",
    "| 1500       | 3        | 10  | 200K  |\n",
    "| 2000       | 4        | 15  | 250K  |\n",
    "| 1800       | 3        | 8   | 220K  |\n",
    "| ...        | ...      | ... | ...   |\n",
    "\n",
    "1. **Data Preprocessing:**\n",
    "   - Standardize the features (except for the target variable, \"Price\") to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Calculate Covariance Matrix:**\n",
    "   - Calculate the covariance matrix of the standardized features.\n",
    "\n",
    "3. **Calculate Eigenvectors and Eigenvalues:**\n",
    "   - Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "4. **Sort Eigenvectors by Eigenvalues:**\n",
    "   - Sort the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "\n",
    "5. **Choose Principal Components:**\n",
    "   - Suppose you decide to retain the top two principal components, which explain a significant portion of the variance in the data.\n",
    "\n",
    "6. **Projection:**\n",
    "   - Project the original standardized features onto the first two principal components.\n",
    "\n",
    "After performing PCA and projecting the data onto the selected principal components, you will have a reduced-dimensional dataset that captures the most important information while discarding less important variations. This reduced-dimensional dataset can be used for analysis and visualization.\n",
    "\n",
    "For instance, you might find that the first principal component is strongly correlated with the size of the house, while the second principal component is related to the number of bedrooms. This information can provide insights into the key factors that influence the price of the house.\n",
    "\n",
    "you would often use machine learning libraries like scikit-learn in Python to perform PCA. The process involves standardizing the data, using the PCA algorithm to fit and transform the data, and then using the transformed data for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "310ca8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratios: [8.62893108e-01 1.37106892e-01 7.33034811e-33]\n",
      "Reduced-Dimensional Data:\n",
      "[[-1.32512728 -0.7373146 ]\n",
      " [ 2.26445229 -0.08878821]\n",
      " [-0.93932501  0.82610281]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Original dataset (features only)\n",
    "data = np.array([\n",
    "    [1500, 3, 10],\n",
    "    [2000, 4, 15],\n",
    "    [1800, 3, 8],\n",
    "    # ... more data\n",
    "])\n",
    "\n",
    "# Step 1: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Step 2: Perform PCA\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(standardized_data)\n",
    "\n",
    "# Explained variance ratio of each component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Choose top two principal components\n",
    "num_components = 2\n",
    "selected_components = principal_components[:, :num_components]\n",
    "\n",
    "# The reduced-dimensional dataset\n",
    "reduced_data = selected_components\n",
    "\n",
    "print(\"Explained Variance Ratios:\", explained_variance_ratio)\n",
    "print(\"Reduced-Dimensional Data:\")\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc697fb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this example, we first standardize the features using `StandardScaler`, then perform PCA using `PCA` from scikit-learn. We choose to retain the top two principal components. The `explained_variance_ratio_` attribute tells us the proportion of variance explained by each principal component.\n",
    "\n",
    "The `reduced_data` contains the data projected onto the selected principal components. This reduced-dimensional data can be used for further analysis, visualization, or even as input to machine learning models. The explained variance ratios can help you understand how much information is retained in the reduced dataset.\n",
    "\n",
    "Remember that while this example focuses on feature extraction, PCA can also be used for dimensionality reduction in cases where the number of features is very high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ea8e4",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ac5f13",
   "metadata": {},
   "source": [
    "using Min-Max scaling to preprocess the data for recommendation system project.\n",
    "\n",
    "Min-Max scaling (also known as normalization) is a technique used to scale features to a specific range, typically between 0 and 1. It's useful when features have different scales and you want to bring them to a common scale without losing their relative relationships.\n",
    "\n",
    "Here's how you would use Min-Max scaling to preprocess your food delivery service dataset:\n",
    "\n",
    "1. **Understand the Data:** Start by understanding the features in your dataset, such as price, rating, and delivery time.\n",
    "\n",
    "2. **Import Libraries:** Import the necessary libraries. You'll need scikit-learn for Min-Max scaling.\n",
    "\n",
    "3. **Load the Data:** Load your dataset into a suitable data structure, like a Pandas DataFrame.\n",
    "\n",
    "4. **Select Features:** Choose the features you want to scale. In this case, you'll select features like price, rating, and delivery time.\n",
    "\n",
    "5. **Apply Min-Max Scaling:**\n",
    "   - Instantiate the `MinMaxScaler` from scikit-learn.\n",
    "   - Fit the scaler to your selected features to compute the minimum and maximum values.\n",
    "   - Transform the selected features using the computed minimum and maximum values.\n",
    "\n",
    "6. **Replace Original Data:** Replace the original values of the selected features with the scaled values.\n",
    "\n",
    "7. **Use the Preprocessed Data:** The preprocessed data with scaled features can now be used for building your recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f3ce3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Data : \n",
      "            category  price  rating  delivery_time (Hs)\n",
      "0         paw-bhaji    200       2                 2.0\n",
      "1      masala-dhosa    300       3                 0.5\n",
      "2        dal-makhni    450       5                 0.6\n",
      "3             Pizza    900       3                 0.7\n",
      "4      masala-dhosa    300       2                 0.1\n",
      "5         dal-tadka    400       1                 3.0\n",
      "6            Burger    560       2                 4.0\n",
      "7     panjabi-thali    430       3                 5.0\n",
      "8        rajma-rice    250       4                 6.5\n",
      "9          dal-rice    100       4                 2.3\n",
      "10          paratha     50       5                 1.2\n",
      "11            pepsi    330       3                 0.2\n",
      "12             bhog    456       2                 0.3\n",
      "13    chicken-gravy    234       3                 0.4\n",
      "14  chicken-tandury    567       4                 4.5\n",
      "15           mutton    765       5                 0.4\n",
      "16             Beef    897       3                 0.5\n",
      "17          shwarma    152       4                 0.6\n",
      "\n",
      "Scaled Data : \n",
      "        price  rating  delivery_time (Hs)         category\n",
      "0   0.176471    0.25            0.296875        paw-bhaji\n",
      "1   0.294118    0.50            0.062500     masala-dhosa\n",
      "2   0.470588    1.00            0.078125       dal-makhni\n",
      "3   1.000000    0.50            0.093750            Pizza\n",
      "4   0.294118    0.25            0.000000     masala-dhosa\n",
      "5   0.411765    0.00            0.453125        dal-tadka\n",
      "6   0.600000    0.25            0.609375           Burger\n",
      "7   0.447059    0.50            0.765625    panjabi-thali\n",
      "8   0.235294    0.75            1.000000       rajma-rice\n",
      "9   0.058824    0.75            0.343750         dal-rice\n",
      "10  0.000000    1.00            0.171875          paratha\n",
      "11  0.329412    0.50            0.015625            pepsi\n",
      "12  0.477647    0.25            0.031250             bhog\n",
      "13  0.216471    0.50            0.046875    chicken-gravy\n",
      "14  0.608235    0.75            0.687500  chicken-tandury\n",
      "15  0.841176    1.00            0.046875           mutton\n",
      "16  0.996471    0.50            0.062500             Beef\n",
      "17  0.120000    0.75            0.078125          shwarma\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# data = pd.read_csv(\"food_delivery_data.csv\")\n",
    "\n",
    "#or\n",
    "data= pd.DataFrame({ \"category\" : [\"paw-bhaji\" , \"masala-dhosa\" ,\"dal-makhni\" , \"Pizza\" , \"masala-dhosa\" ,\"dal-tadka\" ,\"Burger\" ,\"panjabi-thali\"\n",
    "                                   , \"rajma-rice\" ,\"dal-rice\" ,\"paratha\" ,\"pepsi\"  ,\"bhog\" ,\"chicken-gravy\" ,\"chicken-tandury\" ,\"mutton\" ,\"Beef\" ,\"shwarma\"],\n",
    "                     \"price\" : [200 ,300,450,900,300,400,560,430,250,100,50,330,456,234,567,765,897,152],\n",
    "                    \n",
    "                    \"rating\"  : [2,3,5,3,2,1,2,3,4,4,5,3,2,3,4,5,3,4] ,\n",
    "                    f\"delivery_time (Hs)\" : [ 2 , .5 , .6 , .7 ,.1 , 3,4,5,6.5,2.3,1.2,.2,.3,.4,4.5 ,.4,.5,.6] })\n",
    "\n",
    "print(\"Actual Data : \\n\" ,data)\n",
    "# Select features to be scaled\n",
    "selected_features = [\"price\", \"rating\", f\"delivery_time (Hs)\"]\n",
    "\n",
    "# Instantiate the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the selected features\n",
    "scaled_features = scaler.fit_transform(data[selected_features])\n",
    "\n",
    "# Create a DataFrame with the scaled features\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=selected_features)\n",
    "\n",
    "# Replace the original features with scaled features 0in the original DataFrame\n",
    "scaled_df[\"category\"] = data[\"category\"]\n",
    "print(\"\\nScaled Data : \\n\" , scaled_df)\n",
    "# Use the preprocessed data for building the recommendation system\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5282232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63e5db50",
   "metadata": {},
   "source": [
    "In this example, the Min-Max scaling ensures that all the selected features are brought within the range [0, 1], preserving their relative relationships. This can help your recommendation system perform better since features with different scales won't dominate the recommendations disproportionately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f48680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee547ca4",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 6 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ab1e79",
   "metadata": {},
   "source": [
    " using Principal Component Analysis (PCA) to reduce the dimensionality of your stock price prediction dataset.\n",
    "\n",
    "PCA is a dimensionality reduction technique that helps to transform high-dimensional data into a lower-dimensional space while preserving as much of the original data's variance as possible. This reduction in dimensionality can lead to more efficient and effective modeling, especially when dealing with datasets with a large number of features.\n",
    "\n",
    "Here's how you would use PCA to reduce the dimensionality of your stock price prediction dataset:\n",
    "\n",
    "1. **Understand the Data:** Begin by understanding the features in your dataset, such as company financial data and market trends.\n",
    "\n",
    "2. **Import Libraries:** Import the necessary libraries. You'll need scikit-learn for PCA.\n",
    "\n",
    "3. **Load the Data:** Load your dataset into a suitable data structure, like a Pandas DataFrame.\n",
    "\n",
    "4. **Preprocessing:** Before applying PCA, it's important to preprocess your data by handling missing values, scaling, and other necessary steps.\n",
    "\n",
    "5. **Apply PCA:**\n",
    "   - Choose the number of principal components you want to retain. This decision can be based on the explained variance ratio or your domain knowledge.\n",
    "   - Instantiate the `PCA` class from scikit-learn with the chosen number of components.\n",
    "   - Fit the PCA model to your preprocessed data.\n",
    "   - Transform your data using the fitted PCA model to obtain the principal components.\n",
    "\n",
    "6. **Analyze Explained Variance:** After fitting the PCA model, you can analyze the explained variance ratio of each principal component to understand how much variance each component captures.\n",
    "\n",
    "7. **Reduce Dimensionality:** Select the top `k` principal components that capture the desired amount of variance. These components will form the new lower-dimensional representation of your data.\n",
    "\n",
    "8. **Use Reduced-Dimensional Data:** The reduced-dimensional data can now be used for building your stock price prediction model. You can directly use the principal components as features or use them in combination with other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae9820a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "data = pd.read_csv(\"stock_price_data.csv\")\n",
    "\n",
    "# Select features for PCA\n",
    "selected_features = [\"feature1\", \"feature2\", \"feature3\", ...]\n",
    "\n",
    "# Instantiate the PCA class with desired number of components\n",
    "num_components = 3  # Choose the number of components\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# Preprocess the data (handle missing values, scaling, etc.)\n",
    "\n",
    "# Fit the PCA model to the preprocessed data\n",
    "pca.fit(preprocessed_data[selected_features])\n",
    "\n",
    "# Transform the data using the fitted PCA model\n",
    "reduced_data = pca.transform(preprocessed_data[selected_features])\n",
    "\n",
    "# Analyze explained variance ratio of each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Use the reduced-dimension data for building the stock price prediction model\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa6a950",
   "metadata": {},
   "source": [
    "In this example, PCA is used to transform the original features into a lower-dimensional space represented by the selected number of principal components. The transformed data captures as much variance as possible while reducing the dimensionality, which can improve the efficiency and effectiveness of your stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ff93ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d178c4c",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 7 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db92970",
   "metadata": {},
   "source": [
    "to perform Min-Max scaling on the given dataset to transform the values to a range of -1 to 1, you can use the following formula:\n",
    "\n",
    "\n",
    "Scaled Value = \\[ (Original Value- Min Value) / (Max Value - Min Value) \\] * 2 - 1\n",
    "\n",
    "Original dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "Calculate the Min and Max values of the dataset:\n",
    "\n",
    "Min Value: 1\n",
    "Max Value: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b385e7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [ 1  5 10 15 20]\n",
      "Scaled data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.        , -0.57894737, -0.05263158,  0.47368421,  1.        ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = -1 + 2 * (data - min_value) / (max_value - min_value)\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"Scaled data:\")\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf00b1b",
   "metadata": {},
   "source": [
    "after Min-Max scaling, the values in the dataset [1, 5, 10, 15, 20] would be transformed to approximately [-1, -0.5, 0, 0.5, 1], which fall within the range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a8255",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 8 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82271c8e",
   "metadata": {},
   "source": [
    "Feature extraction using PCA involves transforming the original features into a new set of orthogonal (uncorrelated) features called principal components. The number of principal components to retain depends on the desired level of variance retention and the specific application.\n",
    "\n",
    "Here are the steps you would typically follow:\n",
    "\n",
    "1. **Data Preprocessing:** Standardize the data (mean=0, standard deviation=1) to ensure that all features have the same scale.\n",
    "\n",
    "2. **Compute Covariance Matrix:** Calculate the covariance matrix of the standardized data.\n",
    "\n",
    "3. **Calculate Eigenvectors and Eigenvalues:** Compute the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the direction of the principal components, and eigenvalues represent the amount of variance explained by each component.\n",
    "\n",
    "4. **Sort Eigenvectors:** Sort the eigenvectors in descending order of their corresponding eigenvalues. This allows you to choose the principal components that explain the most variance in the data.\n",
    "\n",
    "5. **Choose Number of Principal Components:** Determine the number of principal components to retain. A common approach is to retain enough components to explain a certain percentage (e.g., 95% or 99%) of the total variance. You can calculate the cumulative explained variance and decide where to cut off based on your desired threshold.\n",
    "\n",
    "6. **Project Data:** Project the original data onto the selected principal components to obtain the transformed features.\n",
    "\n",
    "7. **Finalize the Reduced Dataset:** The transformed dataset will have a reduced dimensionality based on the number of principal components you chose.\n",
    "\n",
    "As for how many principal components to retain, it's a trade-off between reducing dimensionality and retaining sufficient information. Retaining fewer principal components results in simpler and more interpretable models, but may sacrifice some information. On the other hand, retaining more components captures more information but may lead to overfitting.\n",
    "\n",
    "In practice, it's common to retain enough principal components to explain a high percentage (e.g., 95% or more) of the total variance. This decision can be made by plotting the cumulative explained variance against the number of components and choosing the point where the curve starts to level off.\n",
    "\n",
    "For example, if the dataset contains 5 features (height, weight, age, gender, blood pressure), you might calculate the eigenvalues, sort them, and then choose the number of principal components that collectively explain at least 95% of the variance. The specific number of components may vary depending on the data and the desired trade-off between dimensionality reduction and information retention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "239f3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.95422459 0.03796553]\n",
      "Transformed data:\n",
      "[[ 12.05145536   1.90247767]\n",
      " [-10.70426977  -4.77102163]\n",
      " [ 26.97917733  -2.51023842]\n",
      " [-25.63199173  -0.35830554]\n",
      " [ -2.69437119   5.73708791]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample dataset (rows represent samples, columns represent features)\n",
    "data = np.array([\n",
    "    [160, 65, 30, 0, 120],\n",
    "    [175, 70, 45, 1, 130],\n",
    "    [155, 55, 25, 0, 110],\n",
    "    [180, 80, 50, 1, 140],\n",
    "    [170, 75, 35, 1, 125]\n",
    "])\n",
    "\n",
    "# Create a PCA instance and specify the number of components to keep\n",
    "n_components = 2  # Choose the number of principal components to retain\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Fit PCA to the data and transform it\n",
    "pca_result = pca.fit_transform(data)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the transformed data\n",
    "print(\"Transformed data:\")\n",
    "print(pca_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2e012",
   "metadata": {},
   "source": [
    "The choice of how many principal components to retain in PCA depends on the specific goals of your analysis and the trade-off between dimensionality reduction and retaining sufficient information. Here are some considerations for determining the number of principal components to keep:\n",
    "\n",
    "1. **Explained Variance Ratio:** One common approach is to examine the explained variance ratio associated with each principal component. The explained variance ratio indicates the proportion of the total variance in the data that is captured by each principal component. Retaining components that collectively explain a high percentage of the variance (e.g., 95% or 99%) can be a good strategy.\n",
    "\n",
    "2. **Scree Plot:** A scree plot is a graphical representation of the explained variance ratio for each principal component. The point at which the explained variance starts to level off suggests a reasonable number of components to retain. This \"elbow\" point often corresponds to the point where adding more components provides diminishing returns in terms of explained variance.\n",
    "\n",
    "3. **Domain Knowledge:** If you have domain knowledge about the dataset and the underlying features, it can guide your decision. Certain features may be more important than others, and you might choose to retain principal components that capture the variability in these critical features.\n",
    "\n",
    "4. **Model Performance:** You can use cross-validation and machine learning models to assess the impact of different numbers of principal components on the performance of your final model. Experiment with different numbers and observe how the model's performance (e.g., accuracy, regression metrics) changes.\n",
    "\n",
    "5. **Computational Efficiency:** Sometimes, reducing the number of principal components can significantly reduce computational time for downstream analyses and modeling, which can be advantageous for large datasets.\n",
    "\n",
    "It's important to strike a balance between reducing dimensionality and retaining enough information to avoid losing important features or patterns. Experiment with different numbers of principal components and evaluate their impact on your specific analysis to make an informed decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ee2581",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a304b69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738f7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38d9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76589c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24dd264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
