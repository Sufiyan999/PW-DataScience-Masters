{"cells":[{"cell_type":"markdown","source":["<a id=\"1\"></a>\n"," # <p style=\"padding:10px;background-color: #000000 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Run it in colab</p>"],"metadata":{"id":"8WcjpPX1eo1D"},"id":"8WcjpPX1eo1D"},{"cell_type":"markdown","id":"fc98da40","metadata":{"id":"fc98da40"},"source":["<a id=\"1\"></a>\n"," # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p>"]},{"cell_type":"markdown","id":"cea08263","metadata":{"id":"cea08263"},"source":["## 1."]},{"cell_type":"markdown","id":"ab443dc5","metadata":{"id":"ab443dc5"},"source":["Batch normalization is a technique used in artificial neural networks to improve the training stability and speed by normalizing the input of each layer during training. It aims to mitigate the internal covariate shift, which refers to the change in distribution of layer inputs as the model learns, making the training process more efficient.\n","\n","In a neural network, the distribution of inputs to each layer changes as the parameters of previous layers are updated during training. This can lead to slower convergence and gradient vanishing/exploding problems. Batch normalization addresses this issue by normalizing the input of each layer to have zero mean and unit variance.\n","\n","The basic idea of batch normalization is to normalize the activations of each layer using the mean and variance computed over a batch of training examples. This normalization process is performed for each mini-batch during training. After normalization, the normalized values are scaled and shifted using learnable parameters to ensure that the network can still learn and adapt to the data.\n","\n","The batch normalization process can be summarized as follows:\n","\n","1. Calculate the mean and variance of the activations within a mini-batch.\n","2. Normalize the activations by subtracting the mean and dividing by the square root of the variance.\n","3. Scale and shift the normalized activations using learnable parameters (gamma and beta).\n","4. The scaled and shifted values are then passed through the activation function.\n","\n","Batch normalization provides several benefits, including:\n","\n","1. **Stable Training:** Batch normalization helps mitigate the vanishing/exploding gradient problem, allowing for more stable and faster training.\n","2. **Increased Learning Rates:** It enables the use of higher learning rates, which can accelerate the convergence of the training process.\n","3. **Regularization Effect:** Batch normalization has a slight regularization effect, reducing the need for dropout or other regularization techniques.\n","4. **Reduced Sensitivity to Initialization:** Batch normalization reduces the sensitivity of the network to the choice of initial weights.\n","5. **Generalization:** It can improve the generalization performance of the model on validation and test data.\n","\n","In summary, batch normalization helps neural networks train faster and more reliably by maintaining stable activations throughout the network during training."]},{"cell_type":"markdown","id":"627e502b","metadata":{"id":"627e502b"},"source":["## 2."]},{"cell_type":"markdown","id":"b062882a","metadata":{"id":"b062882a"},"source":["Using batch normalization during training offers several benefits that contribute to more stable and efficient neural network training. Some of these benefits include:\n","\n","1. **Faster Convergence:** Batch normalization helps the training process converge faster. By normalizing the inputs, it reduces the internal covariate shift, allowing the network to learn more quickly and efficiently. As a result, the number of training iterations required to reach a certain level of accuracy is often reduced.\n","\n","2. **Stable Gradients:** Batch normalization helps mitigate the vanishing and exploding gradient problems. It maintains activations within a certain range, preventing gradients from becoming too small or too large. This stability in gradients allows for more consistent and effective weight updates during backpropagation.\n","\n","3. **Higher Learning Rates:** Batch normalization enables the use of larger learning rates without causing the training process to diverge. Larger learning rates can help the model converge more quickly, as long as they are chosen carefully.\n","\n","4. **Regularization Effect:** Batch normalization acts as a form of regularization by adding noise to the activations during training. This noise introduces some randomness, similar to dropout, which can help prevent overfitting and improve generalization to unseen data.\n","\n","5. **Reduced Dependency on Initialization:** Batch normalization reduces the need for careful weight initialization. It helps counteract the effects of poor initialization choices by normalizing the activations, making the network less sensitive to initial weights.\n","\n","6. **Improved Gradient Flow:** By maintaining stable activations, batch normalization ensures smoother and more consistent gradient flow throughout the network. This leads to more efficient weight updates and better convergence.\n","\n","7. **Better Generalization:** Batch normalization can improve the generalization performance of the model on unseen data. It regularizes the network and reduces the chances of overfitting, resulting in a model that performs well on both training and validation/test datasets.\n","\n","8. **Effective Training of Deep Networks:** Batch normalization is especially valuable when training deep networks with many layers. It helps prevent issues that can arise due to vanishing/exploding gradients in deep architectures.\n","\n","9. **Adaptability to Different Datasets:** Batch normalization adjusts the activations for each batch, making the network adaptable to variations in data distributions and characteristics within the training dataset.\n","\n","In summary, batch normalization is a powerful technique that addresses several challenges associated with training neural networks. Its ability to stabilize activations, improve gradient flow, and enhance generalization makes it an essential tool for accelerating and optimizing the training process."]},{"cell_type":"markdown","id":"b7eec685","metadata":{"id":"b7eec685"},"source":["## 3."]},{"cell_type":"markdown","id":"b0e96e9d","metadata":{"id":"b0e96e9d"},"source":["Batch normalization works by normalizing the activations of a neural network's hidden layers within each mini-batch of training data. This normalization helps mitigate the internal covariate shift, where the distribution of activations changes during training. The working principle of batch normalization involves two main steps: the normalization step and the introduction of learnable parameters for scaling and shifting.\n","\n","1. **Normalization Step:**\n","   In the normalization step, the activations of each layer within a mini-batch are normalized to have zero mean and unit variance. This is done to ensure that the activations are centered around zero and have a similar scale. The normalization is performed using the following formula:\n","\n","   \\[   X̂_i     =  ( x_i - μ )   / sqrt( σ ^2 +  ε )   \\]\n","\n","   Here, \\( X̂_i \\) is the normalized activation of the \\(i\\)th unit, \\(x_i\\) is the original activation, \\( μ \\) is the mean of the mini-batch, \\( σ \\) is the standard deviation of the mini-batch, and \\(  ε  \\) is a small constant to avoid division by zero.\n","\n","2. **Learnable Parameters:**\n","   After normalization, the activations are further scaled and shifted using learnable parameters. This introduces flexibility to the normalization process and allows the network to adapt the normalized activations as needed for optimal training. The scaled and shifted activations are given by:\n","\n","   \\[\n","   y_i = γ   * X̂   +   β\n","   \\]\n","\n","   Here, \\(y_i\\) is the final normalized and scaled activation, \\( γ \\) is a learnable scaling parameter, and \\( β \\) is a learnable shifting parameter. These parameters are updated during training using backpropagation.\n","\n","3. **Integration into Neural Network Architecture:**\n","   Batch normalization is typically applied right after the linear transformation and before the activation function in each layer of the neural network. During training, the normalization and learnable parameter updates are performed independently for each mini-batch. However, during inference, batch normalization is often applied differently to ensure that the model's behavior remains consistent.\n","   \n","\n","In summary, batch normalization ensures that activations within each mini-batch are centered and scaled, reducing the internal covariate shift during training. The learnable parameters \\( γ \\) and \\( β \\) introduce adaptability to the normalization process, allowing the network to learn the optimal scaling and shifting for improved training and convergence."]},{"cell_type":"code","execution_count":null,"id":"028ce157","metadata":{"id":"028ce157"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"caf37247","metadata":{"id":"caf37247"},"source":["<a id=\"2\"></a>\n"," # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p>"]},{"cell_type":"markdown","id":"749d0f56","metadata":{"id":"749d0f56"},"source":["## 1."]},{"cell_type":"code","execution_count":106,"id":"875a0388","metadata":{"id":"875a0388","executionInfo":{"status":"ok","timestamp":1693126938157,"user_tz":-330,"elapsed":944,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"code","execution_count":107,"id":"d3616bd6","metadata":{"id":"d3616bd6","executionInfo":{"status":"ok","timestamp":1693126939153,"user_tz":-330,"elapsed":26,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"outputs":[],"source":["import numpy as np                   # advanced math library\n","import matplotlib.pyplot as plt      # MATLAB like plotting routines\n","import random                        # for generating random numbers\n","\n","from keras.datasets import mnist     # MNIST dataset is included in Keras\n","from keras.models import Sequential  # Model type to be used\n","\n","from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model\n","from keras.utils import np_utils                         # NumPy related tools"]},{"cell_type":"code","execution_count":108,"id":"c224670c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c224670c","executionInfo":{"status":"ok","timestamp":1693126939153,"user_tz":-330,"elapsed":26,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"4dac7867-6cb2-4546-968f-c99b4fd0e698"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape (60000, 28, 28)\n","y_train shape (60000,)\n","X_test shape (10000, 28, 28)\n","y_test shape (10000,)\n"]}],"source":["# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","print(\"X_train shape\", X_train.shape)\n","print(\"y_train shape\", y_train.shape)\n","print(\"X_test shape\", X_test.shape)\n","print(\"y_test shape\", y_test.shape)"]},{"cell_type":"code","execution_count":109,"id":"15905046","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"15905046","executionInfo":{"status":"ok","timestamp":1693126940135,"user_tz":-330,"elapsed":984,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"b56aafe1-0590-436c-c214-f576305afd77"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 900x900 with 3 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n","\n","for i in range(3):\n","    plt.subplot(3,3,i+1)\n","    num = random.randint(0, len(X_train))\n","    plt.imshow(X_train[num], cmap='gray', interpolation='none')\n","    plt.title(\"Class {}\".format(y_train[num]))\n","\n","plt.tight_layout()"]},{"cell_type":"code","execution_count":110,"id":"0afb2c2f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0afb2c2f","executionInfo":{"status":"ok","timestamp":1693126940135,"user_tz":-330,"elapsed":8,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"902d1274-969f-42a2-ac07-19ae22f1d673"},"outputs":[{"output_type":"stream","name":"stdout","text":["0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    1   16  130  206  255  254  255  254  224  130   32    0    0    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    1   86  253  253  253  253  253  253  253  253  253  219   98    3    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0   77  253  253  253  251  235  213  111  111  202  249  253  253   52    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0  203  253  251  216   92    0    0    0    0    0  113  248  253  166    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0  255  253  219    0    0    0    0    0    0    0    9  205  253  166    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0  156  210   41    0    0    0    0    0    0    0   87  253  253  148    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0   11  203  253  253   43    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0   11  160  253  253  237   34    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0   84  253  253  253  190   37    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0   13  198  253  253  253  253  230  140   63    6    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0   12  197  253  253  253  253  253  253  253  146   39    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0   39   55   55   55   55  158  206  253  253  229  141    2  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   18   98  246  253  253   45  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   91  247  253  233  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  175  253  253  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0   42  243  253  253  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0   24  106  106   75    0    0   39  106  228  253  253  201  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0  171  253  253  248  236  236  242  253  253  253  201   12  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0   62  253  253  253  253  253  253  253  253  201   12    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    2    5    5  103  229  253  253  144   17    3    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0  0  \n","0  0  0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0  0  0  0  0  0  \n"]}],"source":["# just a little function for pretty printing a matrix\n","def matprint(mat, fmt=\"g\"):\n","    col_maxes = [max([len((\"{:\"+fmt+\"}\").format(x)) for x in col]) for col in mat.T]\n","    for x in mat:\n","        for i, y in enumerate(x):\n","            print((\"{:\"+str(col_maxes[i])+fmt+\"}\").format(y), end=\"  \")\n","        print(\"\")\n","\n","matprint(X_train[num])"]},{"cell_type":"markdown","source":["## Formatting the input data layer\n","\n","Instead of a 28 x 28 matrix, we build our network to accept a 784-length vector.\n","\n","Each image needs to be then reshaped (or flattened) into a vector. We'll also normalize the inputs to be in the range [0-1] rather than [0-255]. Normalizing inputs is generally recommended, so that any additional dimensions (for other network architectures) are of the same scale."],"metadata":{"id":"XRFrmRHrhrbt"},"id":"XRFrmRHrhrbt"},{"cell_type":"code","source":["X_train = X_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.\n","X_test = X_test.reshape(10000, 784)   # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.\n","\n","X_train = X_train.astype('float32')   # change integers to 32-bit floating point numbers\n","X_test = X_test.astype('float32')\n","\n","X_train /= 255                        # normalize each value for each pixel for the entire vector for each input\n","X_test /= 255\n","\n","print(\"Training matrix shape\", X_train.shape)\n","print(\"Testing matrix shape\", X_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yoIbUf1hvOL","executionInfo":{"status":"ok","timestamp":1693126940135,"user_tz":-330,"elapsed":5,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"0daec283-5d5d-45bc-d4fe-157c2ea166a8"},"id":"2yoIbUf1hvOL","execution_count":111,"outputs":[{"output_type":"stream","name":"stdout","text":["Training matrix shape (60000, 784)\n","Testing matrix shape (10000, 784)\n"]}]},{"cell_type":"markdown","source":["We then modify our classes (unique digits) to be in the one-hot format, i.e.\n","\n","```\n","0 -> [1, 0, 0, 0, 0, 0, 0, 0, 0]\n","1 -> [0, 1, 0, 0, 0, 0, 0, 0, 0]\n","2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0]\n","etc.\n","```\n","\n","If the final output of our network is very close to one of these classes, then it is most likely that class. For example, if the final output is:\n","\n","```\n","[0, 0.94, 0, 0, 0, 0, 0.06, 0, 0]\n","```\n","then it is most probable that the image is that of the digit `1`."],"metadata":{"id":"Re8VxmFniGff"},"id":"Re8VxmFniGff"},{"cell_type":"code","source":["nb_classes = 10 # number of unique digits\n","\n","Y_train = np_utils.to_categorical(y_train, nb_classes)\n","Y_test = np_utils.to_categorical(y_test, nb_classes)"],"metadata":{"id":"jbGaGCW9hvRC","executionInfo":{"status":"ok","timestamp":1693126941092,"user_tz":-330,"elapsed":960,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"jbGaGCW9hvRC","execution_count":112,"outputs":[]},{"cell_type":"code","source":["val_X = X_train[50000 : ]\n","val_y = Y_train[50000 : ]\n","\n","X_train =  X_train[ :50000 ]\n","Y_train =  Y_train[ :50000 ]"],"metadata":{"id":"gEsRwpj4hvTp","executionInfo":{"status":"ok","timestamp":1693126941093,"user_tz":-330,"elapsed":3,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"gEsRwpj4hvTp","execution_count":113,"outputs":[]},{"cell_type":"code","source":["print(\"Training matrix shape\", X_train.shape)\n","print(\"validation matrix shape\", val_X.shape)\n","print(\"Testing matrix shape\", X_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59m8GYrfNzdy","executionInfo":{"status":"ok","timestamp":1693126941093,"user_tz":-330,"elapsed":3,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"8a04bbcc-4641-4826-9d53-31781da89c87"},"id":"59m8GYrfNzdy","execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["Training matrix shape (50000, 784)\n","validation matrix shape (10000, 784)\n","Testing matrix shape (10000, 784)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"_GKZ9k0NjXFt"},"id":"_GKZ9k0NjXFt"},{"cell_type":"markdown","source":["## Building a 3-layer fully connected network (FCN)"],"metadata":{"id":"MULIZAsDjXIz"},"id":"MULIZAsDjXIz"},{"cell_type":"code","source":["model = Sequential()   # The Sequential model is a linear stack of layers and is very common."],"metadata":{"id":"53W1FwVlhvWN","executionInfo":{"status":"ok","timestamp":1693126941093,"user_tz":-330,"elapsed":3,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"53W1FwVlhvWN","execution_count":115,"outputs":[]},{"cell_type":"markdown","source":["## The first hidden layer"],"metadata":{"id":"6nlI2FO2jZSt"},"id":"6nlI2FO2jZSt"},{"cell_type":"code","source":["# The first hidden layer is a set of 512 nodes (artificial neurons).\n","# Each node will receive an element from each input vector and apply some weight and bias to it.\n","\n","model.add(Dense(512, input_shape=(784,)))"],"metadata":{"id":"KusR-76FhvYq","executionInfo":{"status":"ok","timestamp":1693126941093,"user_tz":-330,"elapsed":3,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"KusR-76FhvYq","execution_count":116,"outputs":[]},{"cell_type":"code","source":["# An \"activation\" is a non-linear function applied to the output of the layer above.\n","# It checks the new value of the node, and decides whether that artifical neuron has fired.\n","# The Rectified Linear Unit (ReLU) converts all negative inputs to nodes in the next layer to be zero.\n","# Those inputs are then not considered to be fired.\n","# Positive values of a node are unchanged.\n","\n","model.add(Activation('relu'))"],"metadata":{"id":"YCgl0RV-hvbB","executionInfo":{"status":"ok","timestamp":1693126941093,"user_tz":-330,"elapsed":3,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"YCgl0RV-hvbB","execution_count":117,"outputs":[]},{"cell_type":"markdown","source":["$$f(x) = max (0,x)$$"],"metadata":{"id":"_ihP7LuLi5bb"},"id":"_ihP7LuLi5bb"},{"cell_type":"code","source":["# Dropout zeroes a selection of random outputs (i.e., disables their activation)\n","# Dropout helps protect the model from memorizing or \"overfitting\" the training data.\n","model.add(Dropout(0.2))"],"metadata":{"id":"EnswGT_ahvdQ","executionInfo":{"status":"ok","timestamp":1693126941093,"user_tz":-330,"elapsed":2,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"EnswGT_ahvdQ","execution_count":118,"outputs":[]},{"cell_type":"markdown","source":["## Adding the second hidden layer"],"metadata":{"id":"QEe_8NmLjS7M"},"id":"QEe_8NmLjS7M"},{"cell_type":"code","source":["# The second hidden layer appears identical to our first layer.\n","# However, instead of each of the 512-node receiving 784-inputs from the input image data,\n","# they receive 512 inputs from the output of the first 512-node layer.\n","\n","model.add(Dense(512))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))"],"metadata":{"id":"YHhGkMgejIRU","executionInfo":{"status":"ok","timestamp":1693126941093,"user_tz":-330,"elapsed":2,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"YHhGkMgejIRU","execution_count":119,"outputs":[]},{"cell_type":"markdown","source":["## The Final Output Layer"],"metadata":{"id":"XT9naNWpjjb7"},"id":"XT9naNWpjjb7"},{"cell_type":"code","source":["# The final layer of 10 neurons in fully-connected to the previous 512-node layer.\n","# The final layer of a FCN should be equal to the number of desired classes (10 in this case).\n","model.add(Dense(10))"],"metadata":{"id":"qrM0oNjkjIUR","executionInfo":{"status":"ok","timestamp":1693126941093,"user_tz":-330,"elapsed":2,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"qrM0oNjkjIUR","execution_count":120,"outputs":[]},{"cell_type":"code","source":["# The \"softmax\" activation represents a probability distribution over K different possible outcomes.\n","# Its values are all non-negative and sum to 1.\n","\n","model.add(Activation('softmax'))"],"metadata":{"id":"gXfJx_tbjIW6","executionInfo":{"status":"ok","timestamp":1693126942054,"user_tz":-330,"elapsed":24,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"gXfJx_tbjIW6","execution_count":121,"outputs":[]},{"cell_type":"code","source":["# Summarize the built model\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-RoaiaO2jIZb","executionInfo":{"status":"ok","timestamp":1693126942054,"user_tz":-330,"elapsed":24,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"9c43d6d2-6794-4d48-b20b-c8409befab58"},"id":"-RoaiaO2jIZb","execution_count":122,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_22 (Dense)            (None, 512)               401920    \n","                                                                 \n"," activation_22 (Activation)  (None, 512)               0         \n","                                                                 \n"," dropout_15 (Dropout)        (None, 512)               0         \n","                                                                 \n"," dense_23 (Dense)            (None, 512)               262656    \n","                                                                 \n"," activation_23 (Activation)  (None, 512)               0         \n","                                                                 \n"," dropout_16 (Dropout)        (None, 512)               0         \n","                                                                 \n"," dense_24 (Dense)            (None, 10)                5130      \n","                                                                 \n"," activation_24 (Activation)  (None, 10)                0         \n","                                                                 \n","=================================================================\n","Total params: 669,706\n","Trainable params: 669,706\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"BnPqLtkWjIb5","executionInfo":{"status":"ok","timestamp":1693126942054,"user_tz":-330,"elapsed":4,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"BnPqLtkWjIb5","execution_count":123,"outputs":[]},{"cell_type":"markdown","source":["## Train the neural network  without using batch normalization"],"metadata":{"id":"fcbX0ITA8W_z"},"id":"fcbX0ITA8W_z"},{"cell_type":"code","source":["history = model.fit(X_train, Y_train, validation_data=(val_X, val_y),\n","          batch_size=128, epochs=5,\n","          verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WrjQTLmyjIhC","executionInfo":{"status":"ok","timestamp":1693126989362,"user_tz":-330,"elapsed":47312,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"04f012db-2c67-45d4-fcd4-5082c7a55916"},"id":"WrjQTLmyjIhC","execution_count":124,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","391/391 [==============================] - 11s 25ms/step - loss: 0.2793 - accuracy: 0.9153 - val_loss: 0.1166 - val_accuracy: 0.9674\n","Epoch 2/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.1106 - accuracy: 0.9659 - val_loss: 0.0942 - val_accuracy: 0.9720\n","Epoch 3/5\n","391/391 [==============================] - 9s 23ms/step - loss: 0.0747 - accuracy: 0.9763 - val_loss: 0.0843 - val_accuracy: 0.9762\n","Epoch 4/5\n","391/391 [==============================] - 9s 22ms/step - loss: 0.0587 - accuracy: 0.9810 - val_loss: 0.0884 - val_accuracy: 0.9724\n","Epoch 5/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.0465 - accuracy: 0.9849 - val_loss: 0.0865 - val_accuracy: 0.9775\n"]}]},{"cell_type":"markdown","source":["## Evaluate Model's Accuracy on Test Data"],"metadata":{"id":"FxEOWnHS8DUD"},"id":"FxEOWnHS8DUD"},{"cell_type":"code","source":["score = model.evaluate(X_test, Y_test)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-FzriPG5jIjp","executionInfo":{"status":"ok","timestamp":1693126991046,"user_tz":-330,"elapsed":1709,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"6ff3a794-e016-431e-bed6-14aaa17a58ea"},"id":"-FzriPG5jIjp","execution_count":125,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.0752 - accuracy: 0.9776\n","Test score: 0.07516911625862122\n","Test accuracy: 0.9775999784469604\n"]}]},{"cell_type":"code","source":["print(\"Training Accuracy:\", history.history['accuracy'][-1])\n","print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9WzEXl3fUlZN","executionInfo":{"status":"ok","timestamp":1693128487098,"user_tz":-330,"elapsed":694,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"e280f890-1994-40b3-a578-566d15ce3bf0"},"id":"9WzEXl3fUlZN","execution_count":149,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 0.9849399924278259\n","Validation Accuracy: 0.9775000214576721\n"]}]},{"cell_type":"markdown","source":["## Inspecting the output"],"metadata":{"id":"invlE_sM8Kt_"},"id":"invlE_sM8Kt_"},{"cell_type":"code","source":["# The predict_classes function outputs the highest probability class\n","# according to the trained classifier for each input example.\n","predicted_classes = model.predict(X_test)\n","\n","predicted_classes = np_utils.to_categorical(np.argmax(predicted_classes ,axis =-1), nb_classes)[0]\n","\n","\n","# Check which items we got right / wrong\n","correct_indices = np.nonzero(predicted_classes == y_test)[0]\n","\n","incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2RaEuVwjIlw","executionInfo":{"status":"ok","timestamp":1693126993704,"user_tz":-330,"elapsed":2669,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"63d903e7-4db7-4af1-8ee0-5677a39aa9a7"},"id":"W2RaEuVwjIlw","execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 4ms/step\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-126-34451b71a601>:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n","  correct_indices = np.nonzero(predicted_classes == y_test)[0]\n","<ipython-input-126-34451b71a601>:11: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n","  incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n"]}]},{"cell_type":"code","source":["plt.figure()\n","for i, correct in enumerate(correct_indices[:9]):\n","    plt.subplot(3,3,i+1)\n","    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n","\n","plt.tight_layout()\n","\n","plt.figure()\n","for i, incorrect in enumerate(incorrect_indices[:9]):\n","    plt.subplot(3,3,i+1)\n","    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n","\n","plt.tight_layout()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"JgBOVUSyjIoS","executionInfo":{"status":"ok","timestamp":1693131362147,"user_tz":-330,"elapsed":669,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"9c49f0b5-ee98-491d-e87c-e85998e74ab3"},"id":"JgBOVUSyjIoS","execution_count":164,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 900x900 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 900x900 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"QWtlN2ZzJLYx","executionInfo":{"status":"ok","timestamp":1693126994602,"user_tz":-330,"elapsed":19,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"QWtlN2ZzJLYx","execution_count":128,"outputs":[]},{"cell_type":"markdown","source":["## 4.  Implementing batch normalization layers in the neural network"],"metadata":{"id":"tutohYn5JPbq"},"id":"tutohYn5JPbq"},{"cell_type":"markdown","source":[],"metadata":{"id":"USX7CXLDF1wz"},"id":"USX7CXLDF1wz"},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense, BatchNormalization\n","import numpy as np                   # advanced math library\n","import matplotlib.pyplot as plt      # MATLAB like plotting routines\n","import random                        # for generating random numbers\n","\n","from keras.datasets import mnist     # MNIST dataset is included in Keras\n","from keras.models import Sequential  # Model type to be used\n","\n","from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model\n","from keras.utils import np_utils\n","\n","model = Sequential()\n","\n","model.add(Dense(512, input_shape=(784,)))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","\n","model.add(Dense(512))\n","model.add(Activation('relu'))\n","model.add(Dropout(0.2))\n","model.add(BatchNormalization())\n","\n","model.add(Dense(10))\n","model.add(Activation('softmax'))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eO3ypa3gjItA","executionInfo":{"status":"ok","timestamp":1693126995347,"user_tz":-330,"elapsed":764,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"8926522f-1d3c-4766-8de3-eab444824595"},"id":"eO3ypa3gjItA","execution_count":129,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_9\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_25 (Dense)            (None, 512)               401920    \n","                                                                 \n"," activation_25 (Activation)  (None, 512)               0         \n","                                                                 \n"," dropout_17 (Dropout)        (None, 512)               0         \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 512)              2048      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_26 (Dense)            (None, 512)               262656    \n","                                                                 \n"," activation_26 (Activation)  (None, 512)               0         \n","                                                                 \n"," dropout_18 (Dropout)        (None, 512)               0         \n","                                                                 \n"," batch_normalization_7 (Batc  (None, 512)              2048      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_27 (Dense)            (None, 10)                5130      \n","                                                                 \n"," activation_27 (Activation)  (None, 10)                0         \n","                                                                 \n","=================================================================\n","Total params: 673,802\n","Trainable params: 671,754\n","Non-trainable params: 2,048\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"],"metadata":{"id":"gcBA-GNBjIyS","executionInfo":{"status":"ok","timestamp":1693126995348,"user_tz":-330,"elapsed":6,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"gcBA-GNBjIyS","execution_count":130,"outputs":[]},{"cell_type":"code","source":["history_BN = model.fit(X_train, Y_train, validation_data=(val_X, val_y),\n","          batch_size=128, epochs=5,\n","          verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCtfDdzejI1V","executionInfo":{"status":"ok","timestamp":1693127080012,"user_tz":-330,"elapsed":84669,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"be602aea-4b43-4282-c993-eda48262aa28"},"id":"RCtfDdzejI1V","execution_count":131,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","391/391 [==============================] - 12s 24ms/step - loss: 0.2386 - accuracy: 0.9268 - val_loss: 0.0998 - val_accuracy: 0.9707\n","Epoch 2/5\n","391/391 [==============================] - 10s 26ms/step - loss: 0.1095 - accuracy: 0.9670 - val_loss: 0.0836 - val_accuracy: 0.9750\n","Epoch 3/5\n","391/391 [==============================] - 10s 26ms/step - loss: 0.0843 - accuracy: 0.9728 - val_loss: 0.0822 - val_accuracy: 0.9763\n","Epoch 4/5\n","391/391 [==============================] - 10s 26ms/step - loss: 0.0662 - accuracy: 0.9790 - val_loss: 0.0784 - val_accuracy: 0.9771\n","Epoch 5/5\n","391/391 [==============================] - 10s 25ms/step - loss: 0.0553 - accuracy: 0.9814 - val_loss: 0.0788 - val_accuracy: 0.9780\n"]}]},{"cell_type":"code","source":["score_BN = model.evaluate(X_test, Y_test)\n","print('Test score:', score_BN[0])\n","print('Test accuracy:', score_BN[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y5tY-6KBjI3o","executionInfo":{"status":"ok","timestamp":1693127082723,"user_tz":-330,"elapsed":2724,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"8b695578-5884-4eb4-ac52-40d74923962e"},"id":"y5tY-6KBjI3o","execution_count":132,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 1s 4ms/step - loss: 0.0789 - accuracy: 0.9773\n","Test score: 0.07890582084655762\n","Test accuracy: 0.9772999882698059\n"]}]},{"cell_type":"code","source":["print(\"Training Accuracy:\", history_BN.history['accuracy'][-1])\n","print(\"Validation Accuracy:\", history_BN.history['val_accuracy'][-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M65kzBItUp2z","executionInfo":{"status":"ok","timestamp":1693128503422,"user_tz":-330,"elapsed":7,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"e7689ff0-2417-4c9a-dd12-669f37265f85"},"id":"M65kzBItUp2z","execution_count":150,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Accuracy: 0.981440007686615\n","Validation Accuracy: 0.9779999852180481\n"]}]},{"cell_type":"code","source":["# The predict_classes function outputs the highest probability class\n","# according to the trained classifier for each input example.\n","predicted_classes = model.predict(X_test)\n","\n","predicted_classes = np_utils.to_categorical(np.argmax(predicted_classes ,axis =-1), nb_classes)[0]\n","# Check which items we got right / wrong\n","correct_indices = np.nonzero(predicted_classes == y_test)[0]\n","\n","incorrect_indices = np.nonzero(predicted_classes != y_test)[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7vxkOrbRjI6K","executionInfo":{"status":"ok","timestamp":1693127085031,"user_tz":-330,"elapsed":2314,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"87c90a16-0edf-4658-cdfb-295666d5b506"},"id":"7vxkOrbRjI6K","execution_count":133,"outputs":[{"output_type":"stream","name":"stdout","text":["313/313 [==============================] - 2s 6ms/step\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-133-7d04e6d43f3f>:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n","  correct_indices = np.nonzero(predicted_classes == y_test)[0]\n","<ipython-input-133-7d04e6d43f3f>:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n","  incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n"]}]},{"cell_type":"code","source":["plt.figure()\n","for i, correct in enumerate(correct_indices[:9]):\n","    plt.subplot(3,3,i+1)\n","    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n","\n","plt.tight_layout()\n","\n","plt.figure()\n","for i, incorrect in enumerate(incorrect_indices[:9]):\n","    plt.subplot(3,3,i+1)\n","    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n","\n","plt.tight_layout()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"NAHFbhNXjI8Y","executionInfo":{"status":"ok","timestamp":1693131373252,"user_tz":-330,"elapsed":1227,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"aa5a5194-e609-43e0-c060-bcca3ccdce0f"},"id":"NAHFbhNXjI8Y","execution_count":165,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 900x900 with 0 Axes>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<Figure size 900x900 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAATAAAAFCCAYAAAB2EYE5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjlElEQVR4nO3de1RU5f4/8PegMCCXQe6MAqJ5SwSTxEzlkJAKZppamq4WdvyKGZpKmaKmaXroWCstRctqwenkLSs1OR5KEVALKW+RaSREAYuLZjqjKBfh+f3hzzmOXGYGB4ZH3q+19lrO3p+992d29nZfHjYKIYQAEZGErCzdABFRczHAiEhaDDAikhYDjIikxQAjImkxwIhIWgwwIpIWA4yIpMUAIyJpMcAk1a1bN0yfPl33OSMjAwqFAhkZGRbr6W5393i/mj59Orp162bpNtolBlgzJCcnQ6FQ6CZbW1v06tULc+bMQXl5uaXbM8n+/fvx+uuvW7oNfPXVVxg4cCBsbW3h6+uLFStW4ObNm0atW1dXh7Vr18Lf3x+2trYIDAzE9u3b77knrVaLlStXIigoCA4ODrCzs0NAQAAWLVqEkpKSe95+S7r77+jd09atWy3doll0tHQDMlu1ahX8/f1RWVmJo0ePYvPmzdi/fz/OnDmDTp06tWovoaGhuHHjBmxsbExab//+/UhMTLRoiP33v//F+PHjERYWhg0bNuCnn37C6tWrceHCBWzevNng+kuXLsWbb76JmTNnYtCgQdi7dy+mTp0KhUKBKVOmNKun3377DRERESgsLMTTTz+NmJgY2NjYICcnBx9//DF2796NX3/9tVnbbg2hoaH497//XW/+unXr8OOPPyI8PNwCXbUAQSZLSkoSAMQPP/ygNz8uLk4AENu2bWt03WvXrpmlBz8/PxEdHX3P24mNjRUt9dfA2B4ffPBBERQUJGpqanTzli5dKhQKhTh37lyT6xYXFwtra2sRGxurm1dXVyeGDx8uunbtKm7evGly3zU1NSIoKEh06tRJHDlypN5yjUYjlixZovscHR0t/Pz8TN5Pa7t+/bpwdHQUjz/+uKVbMRteQprRiBEjAAAFBQUAbt0bcXBwQH5+PqKiouDo6Ihp06YBuHXZs379evTr1w+2trbw9PTErFmzcPnyZb1tCiGwevVqdO3aFZ06dcJjjz2Gn3/+ud6+G7sHlp2djaioKHTu3Bn29vYIDAzEu+++q+svMTERAPQuL24zd48NOXv2LM6ePYuYmBh07Pi/C4IXX3wRQgh8/vnnTa6/d+9e1NTU4MUXX9TNUygUmD17NoqLi5GVlWVUH3f64osv8OOPP2Lp0qUYNmxYveVOTk5Ys2ZNk9t4++238eijj8LV1RV2dnYIDg5u8LscOHAAw4YNg7OzMxwcHNC7d28sWbJEr2bDhg3o168fOnXqhM6dO+Phhx/Gtm3bTP5e+/btw9WrV3V/B+8HvIQ0o/z8fACAq6urbt7NmzcxatQoDBs2DG+//bbu0nLWrFlITk7G888/j5deegkFBQXYuHEjTp06hW+//RbW1tYAgOXLl2P16tWIiopCVFQUTp48iZEjR6K6utpgPwcOHMATTzwBb29vzJs3D15eXjh37hxSUlIwb948zJo1CyUlJThw4ECDlxut0eOpU6cAAA8//LDefLVaja5du+qWN7W+vb09+vbtqzc/JCREt7yhEGrKV199BQB47rnnTFrvTu+++y6efPJJTJs2DdXV1dixYweefvpppKSkYMyYMQCAn3/+GU888QQCAwOxatUqKJVK5OXl4dtvv9Vt58MPP8RLL72ESZMmYd68eaisrEROTg6ys7MxdepUk3raunUr7OzsMGHChGZ/rzbHwmeAUrp9CXnw4EFx8eJFUVRUJHbs2CFcXV2FnZ2dKC4uFkLcurQAIBYvXqy3/pEjRwQAsXXrVr35qampevMvXLggbGxsxJgxY0RdXZ2ubsmSJQKA3uVZenq6ACDS09OFEELcvHlT+Pv7Cz8/P3H58mW9/dy5rcYuIVuix4a89dZbAoAoLCyst2zQoEHikUceaXL9MWPGiO7du9ebX1FR0eCxN8ZDDz0kVCqV0fUNXUJev35d73N1dbUICAgQI0aM0M1bt26dACAuXrzY6LbHjRsn+vXrZ3Qvjbl06ZKwsbERzzzzzD1vqy3hJeQ9iIiIgLu7O3x8fDBlyhQ4ODhg9+7d6NKli17d7Nmz9T7v2rULKpUKjz/+OP7880/dFBwcDAcHB6SnpwMADh48iOrqasydO1fv0m7+/PkGezt16hQKCgowf/58ODs76y27c1uNaY0eAeDGjRsAAKVSWW+Zra2tbnlT6ze27p3bN4VWq4Wjo6PJ693Jzs5O9+fLly9Do9Fg+PDhOHnypG7+7f8ue/fuRV1dXYPbcXZ2RnFxMX744Yd76ufzzz9HdXX1fXX5CPAS8p4kJiaiV69e6NixIzw9PdG7d29YWen/m9CxY0d07dpVb9758+eh0Wjg4eHR4HYvXLgAAPjjjz8AAD179tRb7u7ujs6dOzfZ2+3L2YCAAOO/UCv3CPzvf/Sqqqp6yyorK/WCoLH1G1v3zu2bwsnJCb/99pvJ690pJSUFq1evxunTp/X6uzPkJ0+ejI8++gj/93//h8WLFyM8PBwTJkzApEmTdH+PFi1ahIMHDyIkJAQPPPAARo4cialTp2Lo0KEm9bN161a4uLggMjLynr5XW8MAuwchISH17t3cTalU1gu1uro6eHh4NDoWx93d3Ww9Nldr9ejt7Q0AKC0thY+Pj96y0tJS3b2sptZPT0+HEEIvHEpLSwHcupdmqj59+uDUqVMoKiqq15Mxjhw5gieffBKhoaHYtGkTvL29YW1tjaSkJL2b73Z2djh8+DDS09Pxn//8B6mpqdi5cydGjBiBb775Bh06dEDfvn2Rm5uLlJQUpKam4osvvsCmTZuwfPlyrFy50qh+CgsLceTIEcTExOjuW94veAlpAT169MClS5cwdOhQRERE1JuCgoIAAH5+fgBunQ3d6eLFi/WeBDa0DwA4c+ZMk3WNXU62Ro8AMGDAAADA8ePH9eaXlJSguLhYt7yp9a9fv45z587pzc/OztbbvinGjh0LAPj0009NXhe49RTT1tYWX3/9Nf7+978jMjISERERDdZaWVkhPDwc77zzDs6ePYs1a9bg0KFDukt0ALC3t8fkyZORlJSEwsJCjBkzBmvWrNGdZRqyfft2CCHuu8tHgAFmEc888wxqa2vxxhtv1Ft28+ZNXLlyBcCte2zW1tbYsGEDxB2/e2X9+vUG9zFw4ED4+/tj/fr1uu3ddue27O3tAaBeTWv0CAD9+vVDnz59sGXLFtTW1urmb968GQqFApMmTdLN02g0+OWXX6DRaHTzxo0bB2tra2zatEnv+73//vvo0qULHn30UaP6uNOkSZPQv39/rFmzpsFhGFevXsXSpUsbXb9Dhw5QKBR63+f333/Hnj179Or++uuveuveDtzbl52XLl3SW25jY4MHH3wQQgjU1NQY9X22bdsGX19fk5/GSsGCDxCk1dhA1rtFR0cLe3v7BpfNmjVLABCRkZFi3bp1YuPGjWLevHlCrVaLXbt26eri4+MFABEVFSU2btwoZsyYIdRqtXBzc2vyKaQQt54YWltbCz8/P/H666+LDz74QCxYsECMHDlSV/PZZ58JAOK5554Tn376qdi+fXuL9diYffv2CYVCIUaMGCG2bNkiXnrpJWFlZSVmzpypV3f7uCclJenNX7hwoQAgYmJixIcffijGjBnT4BPUxtZvyPnz54Wfn5/o2LGjmDp1qkhMTBRbtmwR8+bNE+7u7qJXr1662rufQqalpQkAYvjw4WLz5s1i5cqVwsPDQwQGBuo98Z03b5546KGHxLJly8SHH34o1qxZI7p06SK6du0qrly5IoQQYuDAgSIqKkqsWbNGfPTRR+Lll18WSqVSjB071uB3EEKIn376qdlPY2XAAGsGcwSYEEJs2bJFBAcHCzs7O+Ho6Cj69+8vXn31VVFSUqKrqa2tFStXrhTe3t7Czs5OhIWFiTNnztQb5d5QgAkhxNGjR8Xjjz8uHB0dhb29vQgMDBQbNmzQLb9586aYO3eucHd3FwqFot6QCnP22JTdu3eLAQMGCKVSKbp27SqWLVsmqqur9WoaC6Da2lrxj3/8Q/j5+QkbGxvRr18/8emnn9bbx4YNGwQAkZqaalRPly9fFsuXLxf9+/cXnTp1Era2tiIgIEDEx8eL0tJSXV1Dwyg+/vhj0bNnT6FUKkWfPn1EUlKSWLFihd7xTUtLE+PGjRNqtVrY2NgItVotnn32WfHrr7/qaj744AMRGhoqXF1dhVKpFD169BALFy4UGo3GqO+wePFiAUDk5OQYVS8bhRD8vZDUPjzzzDP4/fff8f3331u6FTITPoWkdkEIgYyMjGbfmKe2iWdgRCQtPoUkImkxwIhIWgwwIpIWA4yIpNXmnkLW1dWhpKQEjo6ORr01gYjuL0IIXL16FWq1ut7PETdU3CI2btwo/Pz8hFKpFCEhISI7O9uo9YqKigQATpw4tfOpqKjIYF60yCXkzp07ERcXhxUrVuDkyZMICgrCqFGjdK9gacq9voeJiO4PRmXBvZ5pNSQkJETvlyzU1tYKtVotEhISDK6r0WgsnvycOHGy/GTMj0uZ/QysuroaJ06c0Ht9iJWVFSIiIhr8yf6qqipotVq9iYjIGGYPsD///BO1tbXw9PTUm+/p6YmysrJ69QkJCVCpVLqpOS+QI6L2yeLDKOLj46HRaHRTUVGRpVsiIkmYfRiFm5sbOnTogPLycr355eXl8PLyqlevVCob/KUMRESGmP0MzMbGBsHBwUhLS9PNq6urQ1paGoYMGWLu3RFRO9YiA1nj4uIQHR2Nhx9+GCEhIVi/fj0qKirw/PPPt8TuiKidapEAmzx5Mi5evIjly5ejrKwMAwYMQGpqar0b+0RE96LNvQ9Mq9VCpVJZug0isjCNRgMnJ6cmayz+FJKIqLkYYEQkLQYYEUmLAUZE0mKAEZG0GGBEJC0GGBFJiwFGRNJigBGRtBhgRCQtBhgRSYsBRkTSYoARkbQYYEQkLQYYEUmLAUZE0mKAEZG0GGBEJC0GGBFJiwFGRNJigBGRtBhgRCQtBhgRSYsBRkTSYoARkbQYYEQkLQYYEUmLAUZE0mKAEZG0GGBEJC0GGBFJiwFGRNJigBGRtBhgRCQtBhgRScvsAfb6669DoVDoTX369DH3boiI0LElNtqvXz8cPHjwfzvp2CK7IaJ2rkWSpWPHjvDy8mqJTRMR6bTIPbDz589DrVaje/fumDZtGgoLCxutraqqglar1ZuIiIxh9gAbPHgwkpOTkZqais2bN6OgoADDhw/H1atXG6xPSEiASqXSTT4+PuZuiYjuUwohhGjJHVy5cgV+fn545513MGPGjHrLq6qqUFVVpfus1WoZYkQEjUYDJyenJmta/O66s7MzevXqhby8vAaXK5VKKJXKlm6DiO5DLT4O7Nq1a8jPz4e3t3dL74qI2hmzB9grr7yCzMxM/P777/juu+/w1FNPoUOHDnj22WfNvSsiaufMfglZXFyMZ599FpcuXYK7uzuGDRuGY8eOwd3d3dy7IqJ2rsVv4ptKq9VCpVJZug0isjBjbuLzZyGJSFoMMCKSFgOMiKTFACMiaTHAiEhaDDAikhZf1NXKJk2aZLBm5syZBmtKSkqM2l9lZaXBmq1btxqsKSsrM1jT2I+LEbUUnoERkbQYYEQkLQYYEUmLAUZE0mKAEZG0GGBEJC0GGBFJiwFGRNLi+8Ba2W+//Wawplu3bi3fiIka+61Sd/r5559boRP5FRcXG6xZu3atwZrjx4+bo502i+8DI6L7GgOMiKTFACMiaTHAiEhaDDAikhYDjIikxQAjImkxwIhIWgwwIpIWXyndyox5XXRgYKDBmnPnzhm1v759+xqsGThwoMGasLAwgzWPPPKIwZqioiKDNT4+PgZrzOnmzZsGay5evGiwxtvb2xztAAAKCwsN1tzvI/GNwTMwIpIWA4yIpMUAIyJpMcCISFoMMCKSFgOMiKTFACMiaTHAiEhaHMjaytLS0sxSY6zU1FSzbKdz584GawYMGGCw5sSJEwZrBg0aZExLZlNZWWmw5tdffzVYY+zgYhcXF4M1+fn5Rm2rvTP5DOzw4cMYO3Ys1Go1FAoF9uzZo7dcCIHly5fD29sbdnZ2iIiIwPnz583VLxGRjskBVlFRgaCgICQmJja4fO3atXjvvffw/vvvIzs7G/b29hg1apRR/8oREZnC5EvIyMhIREZGNrhMCIH169dj2bJlGDduHADgk08+gaenJ/bs2YMpU6bcW7dERHcw6038goIClJWVISIiQjdPpVJh8ODByMrKanCdqqoqaLVavYmIyBhmDbCysjIAgKenp958T09P3bK7JSQkQKVS6abWfhMBEcnL4sMo4uPjodFodJMxr1shIgLMHGBeXl4AgPLycr355eXlumV3UyqVcHJy0puIiIxh1gDz9/eHl5eX3jgmrVaL7OxsDBkyxJy7IiIy/SnktWvXkJeXp/tcUFCA06dPw8XFBb6+vpg/fz5Wr16Nnj17wt/fH6+99hrUajXGjx9vzr6plV2+fNlgTXp6uln2Zc6BvOYyceJEgzXGDPYFgJ9++slgzc6dO43aVntncoAdP34cjz32mO5zXFwcACA6OhrJycl49dVXUVFRgZiYGFy5cgXDhg1DamoqbG1tzdc1ERGaEWBhYWEQQjS6XKFQYNWqVVi1atU9NUZEZIjFn0ISETUXA4yIpMUAIyJpMcCISFoMMCKSFgOMiKTFN7JSu+fh4WGwZtOmTQZrrKyMOx8wZojRX3/9ZdS22juegRGRtBhgRCQtBhgRSYsBRkTSYoARkbQYYEQkLQYYEUmLAUZE0uJAVmr3YmNjDda4u7sbrDHmrbUAkJuba1QdGcYzMCKSFgOMiKTFACMiaTHAiEhaDDAikhYDjIikxQAjImkxwIhIWhzISve1oUOHGqxZvHixWfY1fvx4o+rOnDljlv0Rz8CISGIMMCKSFgOMiKTFACMiaTHAiEhaDDAikhYDjIikxQAjImkxwIhIWhyJT/e1qKgogzXW1tYGa9LS0gzWZGVlGdUTmY/JZ2CHDx/G2LFjoVaroVAosGfPHr3l06dPh0Kh0JtGjx5trn6JiHRMDrCKigoEBQUhMTGx0ZrRo0ejtLRUN23fvv2emiQiaojJl5CRkZGIjIxsskapVMLLy6vZTRERGaNFbuJnZGTAw8MDvXv3xuzZs3Hp0qVGa6uqqqDVavUmIiJjmD3ARo8ejU8++QRpaWn45z//iczMTERGRqK2trbB+oSEBKhUKt3k4+Nj7paI6D5l9qeQU6ZM0f25f//+CAwMRI8ePZCRkYHw8PB69fHx8YiLi9N91mq1DDEiMkqLjwPr3r073NzckJeX1+BypVIJJycnvYmIyBgtHmDFxcW4dOkSvL29W3pXRNTOmHwJee3aNb2zqYKCApw+fRouLi5wcXHBypUrMXHiRHh5eSE/Px+vvvoqHnjgAYwaNcqsjRPZ2dkZrDFmDGJ1dbXBmhUrVhisqampMVhD5mVygB0/fhyPPfaY7vPt+1fR0dHYvHkzcnJy8K9//QtXrlyBWq3GyJEj8cYbb0CpVJqvayIiNCPAwsLCIIRodPnXX399Tw0RERmLP8xNRNJigBGRtBhgRCQtBhgRSYsBRkTSYoARkbT4RlaS1sKFCw3WPPTQQwZrUlNTDdZ89913RvVErYtnYEQkLQYYEUmLAUZE0mKAEZG0GGBEJC0GGBFJiwFGRNJigBGRtDiQldqcMWPGGFX32muvGawx5tf0rVq1yqj9UdvDMzAikhYDjIikxQAjImkxwIhIWgwwIpIWA4yIpMUAIyJpMcCISFocyEqtytXV1WDNe++9Z9S2OnToYLBm//79BmuOHTtm1P6o7eEZGBFJiwFGRNJigBGRtBhgRCQtBhgRSYsBRkTSYoARkbQYYEQkLQYYEUmLI/HJbIwZGZ+ammqwxt/f36j95efnG6wx5rXTJC+TzsASEhIwaNAgODo6wsPDA+PHj0dubq5eTWVlJWJjY+Hq6goHBwdMnDgR5eXlZm2aiAgwMcAyMzMRGxuLY8eO4cCBA6ipqcHIkSNRUVGhq1mwYAH27duHXbt2ITMzEyUlJZgwYYLZGyciMukS8u7T/+TkZHh4eODEiRMIDQ2FRqPBxx9/jG3btmHEiBEAgKSkJPTt2xfHjh3DI488Yr7Oiajdu6eb+BqNBgDg4uICADhx4gRqamoQERGhq+nTpw98fX2RlZXV4Daqqqqg1Wr1JiIiYzQ7wOrq6jB//nwMHToUAQEBAICysjLY2NjA2dlZr9bT0xNlZWUNbichIQEqlUo3+fj4NLclImpnmh1gsbGxOHPmDHbs2HFPDcTHx0Oj0eimoqKie9oeEbUfzRpGMWfOHKSkpODw4cPo2rWrbr6Xlxeqq6tx5coVvbOw8vJyeHl5NbgtpVIJpVLZnDaIqJ0z6QxMCIE5c+Zg9+7dOHToUL3xOsHBwbC2tkZaWppuXm5uLgoLCzFkyBDzdExE9P+ZdAYWGxuLbdu2Ye/evXB0dNTd11KpVLCzs4NKpcKMGTMQFxcHFxcXODk5Ye7cuRgyZAifQLYDPXr0MFgTHBxstv3FxcUZrDFmsCvJy6QA27x5MwAgLCxMb35SUhKmT58OAFi3bh2srKwwceJEVFVVYdSoUdi0aZNZmiUiupNJASaEMFhja2uLxMREJCYmNrspIiJj8Ie5iUhaDDAikhYDjIikxQAjImkxwIhIWgwwIpIW38hKRvHz8zNY880335hlXwsXLjSqLiUlxSz7I3nxDIyIpMUAIyJpMcCISFoMMCKSFgOMiKTFACMiaTHAiEhaDDAikhYHspJRYmJiDNb4+vqaZV+ZmZlG1Rnzfjq6v/EMjIikxQAjImkxwIhIWgwwIpIWA4yIpMUAIyJpMcCISFoMMCKSFgeyEoYNG2awZu7cua3QCZFpeAZGRNJigBGRtBhgRCQtBhgRSYsBRkTSYoARkbQYYEQkLQYYEUmLA1kJw4cPN1jj4OBgln3l5+cbrLl27ZpZ9kX3P56BEZG0TAqwhIQEDBo0CI6OjvDw8MD48eORm5urVxMWFgaFQqE3vfDCC2ZtmogIMDHAMjMzERsbi2PHjuHAgQOoqanByJEjUVFRoVc3c+ZMlJaW6qa1a9eatWkiIsDEe2Cpqal6n5OTk+Hh4YETJ04gNDRUN79Tp07w8vIyaptVVVWoqqrSfdZqtaa0RETt2D3dA9NoNAAAFxcXvflbt26Fm5sbAgICEB8fj+vXrze6jYSEBKhUKt3k4+NzLy0RUTvS7KeQdXV1mD9/PoYOHYqAgADd/KlTp8LPzw9qtRo5OTlYtGgRcnNz8eWXXza4nfj4eMTFxek+a7VahhgRGaXZARYbG4szZ87g6NGjevPv/AWo/fv3h7e3N8LDw5Gfn48ePXrU245SqYRSqWxuG0TUjjXrEnLOnDlISUlBeno6unbt2mTt4MGDAQB5eXnN2RURUaNMOgMTQmDu3LnYvXs3MjIy4O/vb3Cd06dPAwC8vb2b1SARUWNMCrDY2Fhs27YNe/fuhaOjI8rKygAAKpUKdnZ2yM/Px7Zt2xAVFQVXV1fk5ORgwYIFCA0NRWBgYIt8AWo7fvzxR4M14eHhBmv++usvc7RD7YBJAbZ582YAtwar3ikpKQnTp0+HjY0NDh48iPXr16OiogI+Pj6YOHEili1bZraGiYhuM/kSsik+Pj7IzMy8p4aIiIzFn4UkImkxwIhIWgwwIpIWA4yIpMUAIyJpMcCISFoKYWhsRCvTarVQqVSWboOILEyj0cDJyanJGp6BEZG0GGBEJC0GGBFJiwFGRNJigBGRtBhgRCQtBhgRSavNBVgbG5ZGRBZiTBa0uQC7evWqpVsgojbAmCxocyPx6+rqUFJSAkdHRygUCgD/+1VrRUVFBkfmtiXsu/XI2DPAvhsihMDVq1ehVqthZdX0OVazf61aS7Gysmr0Nx05OTlJ9R/5NvbdemTsGWDfdzP2xwnb3CUkEZGxGGBEJC0pAkypVGLFihXS/QZv9t16ZOwZYN/3qs3dxCciMpYUZ2BERA1hgBGRtBhgRCQtBhgRSYsBRkTSavMBlpiYiG7dusHW1haDBw/G999/b+mWmvT6669DoVDoTX369LF0W/UcPnwYY8eOhVqthkKhwJ49e/SWCyGwfPlyeHt7w87ODhERETh//rxlmr2Dob6nT59e7/iPHj3aMs3eISEhAYMGDYKjoyM8PDwwfvx45Obm6tVUVlYiNjYWrq6ucHBwwMSJE1FeXm6hjo3rOSwsrN7xfuGFF1qtxzYdYDt37kRcXBxWrFiBkydPIigoCKNGjcKFCxcs3VqT+vXrh9LSUt109OhRS7dUT0VFBYKCgpCYmNjg8rVr1+K9997D+++/j+zsbNjb22PUqFGorKxs5U71GeobAEaPHq13/Ldv396KHTYsMzMTsbGxOHbsGA4cOICamhqMHDkSFRUVupoFCxZg37592LVrFzIzM1FSUoIJEya06Z4BYObMmXrHe+3ata3XpGjDQkJCRGxsrO5zbW2tUKvVIiEhwYJdNW3FihUiKCjI0m2YBIDYvXu37nNdXZ3w8vISb731lm7elStXhFKpFNu3b7dAhw27u28hhIiOjhbjxo2zSD+muHDhggAgMjMzhRC3jq+1tbXYtWuXrubcuXMCgMjKyrJUm3ru7lkIIf72t7+JefPmWaynNnsGVl1djRMnTiAiIkI3z8rKChEREcjKyrJgZ4adP38earUa3bt3x7Rp01BYWGjplkxSUFCAsrIyvWOvUqkwePDgNn/sASAjIwMeHh7o3bs3Zs+ejUuXLlm6pXo0Gg0AwMXFBQBw4sQJ1NTU6B3zPn36wNfXt80c87t7vm3r1q1wc3NDQEAA4uPjcf369Vbrqc29jeK2P//8E7W1tfD09NSb7+npiV9++cVCXRk2ePBgJCcno3fv3igtLcXKlSsxfPhwnDlzBo6OjpZuzyhlZWUA0OCxv72srRo9ejQmTJgAf39/5OfnY8mSJYiMjERWVhY6dOhg6fYA3Hpl1Pz58zF06FAEBAQAuHXMbWxs4OzsrFfbVo55Qz0DwNSpU+Hn5we1Wo2cnBwsWrQIubm5+PLLL1ulrzYbYLKKjIzU/TkwMBCDBw+Gn58fPvvsM8yYMcOCnbUPU6ZM0f25f//+CAwMRI8ePZCRkYHw8HALdvY/sbGxOHPmTJu8N9qYxnqOiYnR/bl///7w9vZGeHg48vPz0aNHjxbvq81eQrq5uaFDhw71nsKUl5fDy8vLQl2ZztnZGb169UJeXp6lWzHa7eMr+7EHgO7du8PNza3NHP85c+YgJSUF6enpeu+98/LyQnV1Na5cuaJX3xaOeWM9N2Tw4MEA0GrHu80GmI2NDYKDg5GWlqabV1dXh7S0NAwZMsSCnZnm2rVryM/Ph7e3t6VbMZq/vz+8vLz0jr1Wq0V2drZUxx4AiouLcenSJYsffyEE5syZg927d+PQoUPw9/fXWx4cHAxra2u9Y56bm4vCwkKLHXNDPTfk9OnTANB6x9tijw+MsGPHDqFUKkVycrI4e/asiImJEc7OzqKsrMzSrTXq5ZdfFhkZGaKgoEB8++23IiIiQri5uYkLFy5YujU9V69eFadOnRKnTp0SAMQ777wjTp06Jf744w8hhBBvvvmmcHZ2Fnv37hU5OTli3Lhxwt/fX9y4caPN9n316lXxyiuviKysLFFQUCAOHjwoBg4cKHr27CkqKyst2vfs2bOFSqUSGRkZorS0VDddv35dV/PCCy8IX19fcejQIXH8+HExZMgQMWTIkDbbc15enli1apU4fvy4KCgoEHv37hXdu3cXoaGhrdZjmw4wIYTYsGGD8PX1FTY2NiIkJEQcO3bM0i01afLkycLb21vY2NiILl26iMmTJ4u8vDxLt1VPenq6AFBvio6OFkLcGkrx2muvCU9PT6FUKkV4eLjIzc21bNOi6b6vX78uRo4cKdzd3YW1tbXw8/MTM2fObBP/4DXUMwCRlJSkq7lx44Z48cUXRefOnUWnTp3EU089JUpLS9tsz4WFhSI0NFS4uLgIpVIpHnjgAbFw4UKh0WharUe+D4yIpNVm74ERERnCACMiaTHAiEhaDDAikhYDjIikxQAjImkxwIhIWgwwIpIWA4yIpMUAIyJpMcCISFr/D/wcXzer1lzPAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":["\n","print(\"Accuracy without BN\")\n","print(\"-\" * 35)\n","print(\"Training Accuracy:\", history.history['accuracy'][-1])\n","print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])\n","\n","print(\"-\" * 150)\n","print(\"-\" * 150)\n","\n","print(\"Accuracy with BN\")\n","print(\"-\" * 35)\n","print(\"Training Accuracy:\", history_BN.history['accuracy'][-1])\n","print(\"Validation Accuracy:\", history_BN.history['val_accuracy'][-1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9aQZxcU6jJDR","executionInfo":{"status":"ok","timestamp":1693128430410,"user_tz":-330,"elapsed":647,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"27619141-1a6c-48e7-fa78-3855a7ad47b8"},"id":"9aQZxcU6jJDR","execution_count":148,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy without BN\n","-----------------------------------\n","Training Accuracy: 0.9849399924278259\n","Validation Accuracy: 0.9775000214576721\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","Accuracy with BN\n","-----------------------------------\n","Training Accuracy: 0.981440007686615\n","Validation Accuracy: 0.9779999852180481\n"]}]},{"cell_type":"code","source":["print(\"\\nScore without BN\")\n","print(\"-\" * 35)\n","print('Test score:', score[0])\n","print('Test accuracy:', score[1])\n","\n","print(\"-\" * 150)\n","print(\"-\" * 150)\n","\n","print(\"\\nScore with BN\")\n","print(\"-\" * 35)\n","print('Test score:', score_BN[0])\n","print('Test accuracy:', score_BN[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"geFiGpJ4MIg2","executionInfo":{"status":"ok","timestamp":1693128384362,"user_tz":-330,"elapsed":676,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"cf671813-3e9b-4c81-fba0-3d4fee23137a"},"id":"geFiGpJ4MIg2","execution_count":147,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Score without BN\n","-----------------------------------\n","Test score: 0.07516911625862122\n","Test accuracy: 0.9775999784469604\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Score with BN\n","-----------------------------------\n","Test score: 0.07890582084655762\n","Test accuracy: 0.9772999882698059\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"JOS047skMZEO","executionInfo":{"status":"ok","timestamp":1693127085033,"user_tz":-330,"elapsed":13,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"JOS047skMZEO","execution_count":137,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RAQEu4XPMZHy","executionInfo":{"status":"ok","timestamp":1693127085034,"user_tz":-330,"elapsed":14,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"RAQEu4XPMZHy","execution_count":137,"outputs":[]},{"cell_type":"markdown","source":["## 5."],"metadata":{"id":"qvioGeraROck"},"id":"qvioGeraROck"},{"cell_type":"markdown","source":["\n","**Model without Batch Normalization:**\n","- Training Loss (Epoch 5): 0.0465\n","- Training Accuracy (Epoch 5): 0.9849\n","- Validation Loss (Epoch 5): 0.0865\n","- Validation Accuracy (Epoch 5): 0.9775\n","\n","**Model with Batch Normalization:**\n","- Training Loss (Epoch 5): 0.0553\n","- Training Accuracy (Epoch 5): 0.9814\n","- Validation Loss (Epoch 5): 0.0788\n","- Validation Accuracy (Epoch 5): 0.9780\n","\n","From the provided results, we can observe the following:\n","- Both models achieved high training and validation accuracy, indicating that they have learned well from the data.\n","- The model without batch normalization achieved slightly lower training and validation loss compared to the model with batch normalization.\n","- The model with batch normalization achieved slightly higher training and validation accuracy compared to the model without batch normalization.\n","\n","Overall, both models seem to perform well on the MNIST dataset. The model with batch normalization might have a slight edge in terms of accuracy, but it's important to note that the difference in performance is not significant. Batch normalization can help in achieving better convergence speed and potentially improving the model's generalization to new data."],"metadata":{"id":"K5-YiDV-QgS7"},"id":"K5-YiDV-QgS7"},{"cell_type":"code","source":[],"metadata":{"id":"Hn-xvEY4MZLX","executionInfo":{"status":"ok","timestamp":1693127085034,"user_tz":-330,"elapsed":14,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}}},"id":"Hn-xvEY4MZLX","execution_count":137,"outputs":[]},{"cell_type":"markdown","source":["## 6."],"metadata":{"id":"GHm8G1kaRQzE"},"id":"GHm8G1kaRQzE"},{"cell_type":"markdown","source":["Batch normalization has a significant impact on the training process and the performance of neural networks. It is a technique that normalizes the activations of each layer in a neural network, which helps in improving the convergence speed, stabilizing training, and ultimately enhancing the network's overall performance. Here are some key impacts of batch normalization:\n","\n","1. **Faster Convergence:** Batch normalization reduces the internal covariate shift by normalizing the activations within each mini-batch. This leads to faster convergence during training, as the network can learn more efficiently with normalized inputs.\n","\n","2. **Stability of Training:** Neural networks can suffer from vanishing and exploding gradients, leading to unstable training. Batch normalization mitigates these issues by ensuring that the input to each layer has a consistent scale, which helps in smoother weight updates and better gradient flow.\n","\n","3. **Increased Learning Rates:** Batch normalization allows for the use of higher learning rates without the risk of divergence. This is because it normalizes activations and reduces the sensitivity of the network to the initial values of the weights.\n","\n","4. **Regularization Effect:** Batch normalization introduces a regularization effect by adding noise to the activations within each mini-batch. This can act as a form of regularization, reducing overfitting and improving the model's ability to generalize to new data.\n","\n","5. **Reduction of Internal Covariate Shift:** Internal covariate shift refers to the change in the distribution of layer activations during training. Batch normalization combats this by normalizing activations, making the training process more stable and less dependent on the specific parameter initialization.\n","\n","6. **Network Architectures:** Batch normalization makes it easier to design and train deep networks. It enables the use of deeper architectures by mitigating the challenges associated with training very deep networks.\n","\n","7. **Less Sensitivity to Weight Initialization:** With batch normalization, the model is less sensitive to the choice of initial weights. This allows for faster and more reliable convergence, even when using suboptimal weight initializations.\n","\n","8. **Reduced Dependency on Hyperparameters:** Batch normalization reduces the dependence on hyperparameters like learning rate and weight initialization, making hyperparameter tuning less critical.\n","\n","9. **Mitigation of Gradient Explosion/Vanishing:** Batch normalization helps in addressing the gradient explosion and vanishing problems, which can occur during backpropagation through deep networks.\n","\n","In summary, batch normalization has a positive impact on the training process and performance of neural networks by improving convergence speed, increasing stability, and enabling the use of higher learning rates. It addresses challenges related to vanishing/exploding gradients, making it an essential technique for training deep and complex architectures."],"metadata":{"id":"Phe-FehfQ8hH"},"id":"Phe-FehfQ8hH"},{"cell_type":"code","source":[],"metadata":{"id":"9Nmt1RtPQ749"},"id":"9Nmt1RtPQ749","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"3bcf2717","metadata":{"id":"3bcf2717"},"source":["<a id=\"3\"></a>\n"," # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p>"]},{"cell_type":"markdown","source":["### 1."],"metadata":{"id":"1KG0gY92SVer"},"id":"1KG0gY92SVer"},{"cell_type":"markdown","source":["Experimenting with different batch sizes can provide insights into how batch normalization and training dynamics are affected by the choice of batch size. The batch size determines the number of training samples used in each iteration for updating the model's weights. Here are some observations you might encounter:\n","\n","1. **Smaller Batch Sizes:**\n","   - Faster Convergence: Smaller batch sizes often lead to faster convergence since weight updates are more frequent.\n","   - More Noisy Updates: With smaller batches, each update is based on a smaller subset of data, which can lead to more noisy updates and less stable training.\n","   - Increased Exploration: Smaller batch sizes allow the model to explore the parameter space more diversely, potentially leading to better generalization.\n","   - More Frequent Parameter Updates: Smaller batches result in more frequent parameter updates, which can help the model quickly adapt to changes in the data.\n","\n","2. **Larger Batch Sizes:**\n","   - Slower Convergence: Larger batch sizes may lead to slower convergence due to fewer updates per epoch.\n","   - Smoother Updates: Larger batches provide smoother updates since they average out noise in gradients, which can lead to more stable training.\n","   - Less Exploration: Larger batches might result in less exploration of the parameter space, potentially leading to overfitting if the model gets stuck in local minima.\n","   - Reduced Memory Usage: Larger batch sizes use memory more efficiently since they require fewer updates per epoch.\n","\n","3. **Effect on Batch Normalization:**\n","   - Batch Normalization Behavior: Batch normalization behaves differently with different batch sizes. Smaller batch sizes can lead to noisier statistics in batch normalization layers.\n","   - Impact on Normalization: In larger batches, batch normalization statistics (mean and variance) might be more accurate, leading to more stable normalization.\n","\n","4. **Model Performance:**\n","   - Validation Performance: The impact of batch size on validation performance may vary. Smaller batches might lead to better validation performance initially due to more exploration, but larger batches can catch up over time.\n","   - Overfitting: Smaller batches might help reduce overfitting since they encourage exploration. However, larger batches can generalize better once they converge.\n","\n","5. **Computational Efficiency:**\n","   - Training Time: Smaller batch sizes may require more iterations to complete an epoch, resulting in longer training times.\n","   - Hardware Considerations: Larger batch sizes can take advantage of hardware parallelism, which might lead to faster training on certain hardware setups.\n","\n","6. **Ideal Batch Size:**\n","   - No One-Size-Fits-All: There's no universally ideal batch size. It depends on factors like dataset size, model architecture, hardware, and training goals.\n","   - Experimentation: It's important to experiment with different batch sizes and monitor training dynamics and validation performance to find the optimal balance.\n","\n","In summary, experimenting with different batch sizes can help you understand how batch normalization and training dynamics interact. Smaller batch sizes might result in faster convergence and more exploration, while larger batch sizes offer smoother updates and reduced memory usage. It's important to consider the trade-offs and choose a batch size that aligns with your specific goals and resources."],"metadata":{"id":"ozkEufq8SZLZ"},"id":"ozkEufq8SZLZ"},{"cell_type":"code","source":["from keras.optimizers import Adam\n","\n","def create_model():\n","            model = Sequential()\n","\n","            model.add(Dense(512, input_shape=(784,)))\n","            model.add(Activation('relu'))\n","            model.add(Dropout(0.2))\n","            model.add(BatchNormalization())\n","\n","            model.add(Dense(512))\n","            model.add(Activation('relu'))\n","            model.add(Dropout(0.2))\n","            model.add(BatchNormalization())\n","\n","            model.add(Dense(10))\n","            model.add(Activation('softmax'))\n","\n","            return model\n","\n","batch_dict = {}\n","# Experiment with different batch sizes\n","batch_sizes = [16, 32, 64, 128]\n","for batch_size in batch_sizes:\n","    print(f\"\\nBatch Size: {batch_size}\\n\")\n","\n","    model = create_model()\n","    optimizer = Adam(learning_rate=0.001)\n","\n","    model.compile(optimizer=optimizer,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    history = model.fit(X_train, Y_train, validation_data=(val_X, val_y),\n","                        batch_size=128, epochs=5,\n","                        verbose=1)\n","    score = model.evaluate(X_test, Y_test)\n","\n","    batch_dict[batch_size] = { \"model\" : model ,  \"history\" : history , \"score\" : score }\n","\n","\n","    print(\"\\nTraining Accuracy:\", history.history['accuracy'][-1])\n","    print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])\n","    print(\"-\" * 30)\n","    print('\\nTest score:', score[0])\n","    print('Test accuracy:', score[1])\n","    print(\"-\" * 150)\n","    print(\"-\" * 150)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhhwI8bkSVFK","executionInfo":{"status":"ok","timestamp":1693130954260,"user_tz":-330,"elapsed":307566,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"c5fbb6a5-51e9-4b6d-9ed4-8ebf27bd865c"},"id":"FhhwI8bkSVFK","execution_count":160,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Batch Size: 16\n","\n","Epoch 1/5\n","391/391 [==============================] - 10s 22ms/step - loss: 0.2443 - accuracy: 0.9250 - val_loss: 0.1159 - val_accuracy: 0.9650\n","Epoch 2/5\n","391/391 [==============================] - 9s 23ms/step - loss: 0.1080 - accuracy: 0.9659 - val_loss: 0.0914 - val_accuracy: 0.9719\n","Epoch 3/5\n","391/391 [==============================] - 9s 23ms/step - loss: 0.0798 - accuracy: 0.9740 - val_loss: 0.0862 - val_accuracy: 0.9733\n","Epoch 4/5\n","391/391 [==============================] - 8s 21ms/step - loss: 0.0686 - accuracy: 0.9780 - val_loss: 0.0921 - val_accuracy: 0.9740\n","Epoch 5/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.0551 - accuracy: 0.9814 - val_loss: 0.0748 - val_accuracy: 0.9767\n","313/313 [==============================] - 1s 4ms/step - loss: 0.0669 - accuracy: 0.9798\n","\n","Training Accuracy: 0.981440007686615\n","Validation Accuracy: 0.9767000079154968\n","------------------------------\n","\n","Test score: 0.06693384051322937\n","Test accuracy: 0.9797999858856201\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Batch Size: 32\n","\n","Epoch 1/5\n","391/391 [==============================] - 11s 24ms/step - loss: 0.2401 - accuracy: 0.9269 - val_loss: 0.1046 - val_accuracy: 0.9715\n","Epoch 2/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.1097 - accuracy: 0.9664 - val_loss: 0.1006 - val_accuracy: 0.9706\n","Epoch 3/5\n","391/391 [==============================] - 10s 25ms/step - loss: 0.0816 - accuracy: 0.9742 - val_loss: 0.0830 - val_accuracy: 0.9755\n","Epoch 4/5\n","391/391 [==============================] - 9s 22ms/step - loss: 0.0699 - accuracy: 0.9774 - val_loss: 0.0838 - val_accuracy: 0.9746\n","Epoch 5/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.0588 - accuracy: 0.9809 - val_loss: 0.0821 - val_accuracy: 0.9744\n","313/313 [==============================] - 1s 4ms/step - loss: 0.0747 - accuracy: 0.9754\n","\n","Training Accuracy: 0.9809200167655945\n","Validation Accuracy: 0.974399983882904\n","------------------------------\n","\n","Test score: 0.07469385117292404\n","Test accuracy: 0.9753999710083008\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Batch Size: 64\n","\n","Epoch 1/5\n","391/391 [==============================] - 12s 28ms/step - loss: 0.2462 - accuracy: 0.9250 - val_loss: 0.1141 - val_accuracy: 0.9640\n","Epoch 2/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.1111 - accuracy: 0.9652 - val_loss: 0.1001 - val_accuracy: 0.9694\n","Epoch 3/5\n","391/391 [==============================] - 8s 21ms/step - loss: 0.0806 - accuracy: 0.9741 - val_loss: 0.0805 - val_accuracy: 0.9758\n","Epoch 4/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.0659 - accuracy: 0.9792 - val_loss: 0.0830 - val_accuracy: 0.9764\n","Epoch 5/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.0571 - accuracy: 0.9812 - val_loss: 0.0750 - val_accuracy: 0.9794\n","313/313 [==============================] - 1s 5ms/step - loss: 0.0738 - accuracy: 0.9783\n","\n","Training Accuracy: 0.9811800122261047\n","Validation Accuracy: 0.9793999791145325\n","------------------------------\n","\n","Test score: 0.07379961758852005\n","Test accuracy: 0.9782999753952026\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Batch Size: 128\n","\n","Epoch 1/5\n","391/391 [==============================] - 13s 25ms/step - loss: 0.2438 - accuracy: 0.9251 - val_loss: 0.1155 - val_accuracy: 0.9657\n","Epoch 2/5\n","391/391 [==============================] - 10s 25ms/step - loss: 0.1106 - accuracy: 0.9654 - val_loss: 0.0901 - val_accuracy: 0.9747\n","Epoch 3/5\n","391/391 [==============================] - 8s 21ms/step - loss: 0.0841 - accuracy: 0.9733 - val_loss: 0.0825 - val_accuracy: 0.9759\n","Epoch 4/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.0648 - accuracy: 0.9793 - val_loss: 0.0815 - val_accuracy: 0.9761\n","Epoch 5/5\n","391/391 [==============================] - 9s 24ms/step - loss: 0.0554 - accuracy: 0.9820 - val_loss: 0.0748 - val_accuracy: 0.9786\n","313/313 [==============================] - 1s 4ms/step - loss: 0.0686 - accuracy: 0.9775\n","\n","Training Accuracy: 0.9819599986076355\n","Validation Accuracy: 0.978600025177002\n","------------------------------\n","\n","Test score: 0.06862480193376541\n","Test accuracy: 0.9775000214576721\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["batch_dict[128]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjKX33bFSVIC","executionInfo":{"status":"ok","timestamp":1693131020837,"user_tz":-330,"elapsed":724,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"91950cab-dc59-4d8a-f0cc-55c94805bb4f"},"id":"tjKX33bFSVIC","execution_count":163,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'model': <keras.engine.sequential.Sequential at 0x7a0a95f2d5d0>,\n"," 'history': <keras.callbacks.History at 0x7a0a95f70df0>,\n"," 'score': [0.06862480193376541, 0.9775000214576721]}"]},"metadata":{},"execution_count":163}]},{"cell_type":"code","execution_count":157,"id":"628078c9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"628078c9","executionInfo":{"status":"ok","timestamp":1693129839634,"user_tz":-330,"elapsed":562,"user":{"displayName":"Moh Sufiyan","userId":"05865655379155300368"}},"outputId":"35140af9-9c23-45bb-e361-6b310c94edc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Batch Size: 16\n","\n","       loss  accuracy  val_loss  val_accuracy\n","0  0.245828   0.92380  0.116240        0.9654\n","1  0.111564   0.96596  0.084893        0.9760\n","2  0.082379   0.97346  0.084482        0.9762\n","3  0.063741   0.98002  0.087895        0.9749\n","4  0.056763   0.98104  0.085908        0.9752\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Batch Size: 32\n","\n","       loss  accuracy  val_loss  val_accuracy\n","0  0.243131   0.92520  0.109269        0.9676\n","1  0.111127   0.96396  0.085120        0.9752\n","2  0.081985   0.97382  0.082246        0.9745\n","3  0.067011   0.97870  0.089028        0.9753\n","4  0.056949   0.98116  0.080026        0.9773\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Batch Size: 64\n","\n","       loss  accuracy  val_loss  val_accuracy\n","0  0.244691   0.92474  0.118062        0.9637\n","1  0.114848   0.96412  0.088300        0.9739\n","2  0.082964   0.97350  0.088418        0.9732\n","3  0.067452   0.97816  0.083888        0.9758\n","4  0.056536   0.98168  0.087043        0.9753\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","Batch Size: 128\n","\n","       loss  accuracy  val_loss  val_accuracy\n","0  0.242896   0.92632  0.123909        0.9628\n","1  0.109883   0.96628  0.083979        0.9749\n","2  0.080714   0.97384  0.086443        0.9758\n","3  0.067172   0.97798  0.082327        0.9768\n","4  0.057159   0.98174  0.077204        0.9802\n","------------------------------------------------------------------------------------------------------------------------------------------------------\n"]}],"source":["import pandas as pd\n","# print(\"batch_sizes\")\n","batch_sizes = [16, 32, 64, 128]\n","for batch_size in batch_sizes:\n","        print(f\"\\nBatch Size: {batch_size}\\n\")\n","        df = pd.DataFrame(batch_dict[batch_size][\"history\"].history)\n","        print(df)\n","        print(\"--\"*75)"]},{"cell_type":"markdown","source":["analysis of the effect of different batch sizes on the training dynamics and model performance:\n","\n","1. **Batch Size: 16**\n","   - Training Accuracy: 98.10%\n","   - Validation Accuracy: 97.52%\n","\n","2. **Batch Size: 32**\n","   - Training Accuracy: 98.12%\n","   - Validation Accuracy: 97.73%\n","\n","3. **Batch Size: 64**\n","   - Training Accuracy: 98.17%\n","   - Validation Accuracy: 97.53%\n","\n","4. **Batch Size: 128**\n","   - Training Accuracy: 98.17%\n","   - Validation Accuracy: 98.02%\n","\n","Observations:\n","- As the batch size increases, the training time per epoch generally decreases, as smaller batches require more iterations to process the entire dataset.\n","- Smaller batch sizes (16, 32) tend to show slightly better performance in terms of accuracy on the validation set compared to larger batch sizes (64, 128).\n","- Smaller batch sizes can lead to more frequent weight updates, potentially helping the model converge faster in the early epochs.\n","- Larger batch sizes can sometimes lead to smoother convergence due to the reduced noise in gradient estimates.\n","- There's a trade-off between computation speed and model convergence. Smaller batches lead to more frequent updates but might also introduce more noise in gradient estimation.\n","\n","In  this case, the batch size of 128 achieved the highest validation accuracy. However, different batch sizes could perform differently depending on the dataset, architecture, and optimization settings. It's recommended to perform these experiments on a variety of datasets to understand the general trend and choose a batch size that balances between training speed and convergence."],"metadata":{"id":"KiqZ2BpHcIwa"},"id":"KiqZ2BpHcIwa"},{"cell_type":"code","source":[],"metadata":{"id":"JpkwI889cITC"},"id":"JpkwI889cITC","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2."],"metadata":{"id":"6oGsZvBOdWz9"},"id":"6oGsZvBOdWz9"},{"cell_type":"markdown","source":["**Advantages of Batch Normalization:**\n","\n","1. **Faster Convergence:** Batch normalization helps the neural network converge faster. By reducing internal covariate shift, it ensures that the model doesn't have to wait for the entire dataset to propagate through the network before updating weights.\n","\n","2. **Stability during Training:** Batch normalization adds stability to the training process by reducing the impact of weight initialization and gradients. This allows the use of higher learning rates without risking divergence.\n","\n","3. **Regularization:** Batch normalization acts as a form of regularization, reducing the need for techniques like dropout. It adds a slight noise to the activations, which can help prevent overfitting.\n","\n","4. **Reduced Dependency on Initialization:** With batch normalization, the model is less sensitive to the choice of initial weights, making it easier to train deep networks.\n","\n","5. **Improved Generalization:** Batch normalization tends to generalize better to unseen data by making the learning process smoother and less sensitive to noise in the data.\n","\n","6. **Less Hyperparameter Tuning:** Neural networks with batch normalization often require less hyperparameter tuning compared to models without it.\n","\n","**Limitations of Batch Normalization:**\n","\n","1. **Batch Size Dependency:** Batch normalization performance can vary with batch size. It may not work well with very small batch sizes or extremely large batch sizes.\n","\n","2. **Test-time Variability:** During inference, batch normalization can introduce variability as each batch might have a different mean and variance. This can be mitigated by using techniques like running average statistics.\n","\n","3. **Not Suitable for Recurrent Networks:** Batch normalization is not as straightforward to apply to recurrent networks due to the sequential nature of the data.\n","\n","4. **Additional Computational Overhead:** While batch normalization improves convergence, it adds some computational overhead because of the extra computations required to normalize activations and scale them.\n","\n","5. **Dependency on Mini-Batch Statistics:** Batch normalization uses mini-batch statistics for normalization, which might not represent the entire dataset, leading to some variability.\n","\n","6. **Limited Effect on Gradient Vanishing/Exploding:** While batch normalization can mitigate gradient vanishing and exploding to some extent, it doesn't fully solve the problem.\n","\n","In summary, batch normalization is a powerful technique that has become a standard practice in neural network training. It has clear advantages in terms of faster convergence, regularization, and improved stability. However, like any technique, it has its limitations and should be used carefully based on the specifics of the problem and the architecture being used."],"metadata":{"id":"USHJElK-dg4V"},"id":"USHJElK-dg4V"},{"cell_type":"code","source":[],"metadata":{"id":"W6MrX5NHbE-m"},"id":"W6MrX5NHbE-m","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Lx_jFzH2bFBE"},"id":"Lx_jFzH2bFBE","execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Z_pCFoJibFDm"},"id":"Z_pCFoJibFDm","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"7c85e1f6","metadata":{"id":"7c85e1f6"},"source":["<a id=\"5\"></a>\n"," # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p>"]},{"cell_type":"code","execution_count":null,"id":"ca8ebe01","metadata":{"id":"ca8ebe01"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"281c330e","metadata":{"id":"281c330e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"96bd516a","metadata":{"id":"96bd516a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e92d8fc2","metadata":{"id":"e92d8fc2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"99067cf1","metadata":{"id":"99067cf1"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}