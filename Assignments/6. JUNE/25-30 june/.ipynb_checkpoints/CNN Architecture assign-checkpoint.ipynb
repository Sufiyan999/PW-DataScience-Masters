{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7d7b8c",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #000000 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">TOPIC: Understanding Pooling and Padding in CNN </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd9a78",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c30dc5e",
   "metadata": {},
   "source": [
    "Pooling, also known as subsampling or downsampling, is a fundamental operation in Convolutional Neural Networks (CNNs) that plays a crucial role in reducing the spatial dimensions of feature maps while retaining essential information. The main purpose of pooling in CNNs is to achieve several important objectives:\n",
    "\n",
    "1. **Dimensionality Reduction:** Pooling reduces the spatial dimensions (width and height) of the feature maps. This is particularly useful to decrease the computational complexity and memory requirements of subsequent layers in the network.\n",
    "\n",
    "2. **Translation Invariance:** Pooling helps the network become partially invariant to small translations or variations in the input image. This is especially beneficial for object recognition tasks, as the network can identify features regardless of their precise spatial location.\n",
    "\n",
    "3. **Feature Hierarchies:** Pooling contributes to building hierarchical representations of features. As the network progresses through multiple layers, each pooling operation captures higher-level features by summarizing the lower-level features within a local region.\n",
    "\n",
    "4. **Noise Reduction:** By summarizing local features, pooling can help reduce the impact of small variations or noise in the input image.\n",
    "\n",
    "5. **Parameter Efficiency:** Pooling reduces the number of parameters in subsequent layers, making the network more parameter-efficient and less prone to overfitting.\n",
    "\n",
    "There are two common types of pooling operations used in CNNs:\n",
    "\n",
    "1. **Max Pooling:** Max pooling selects the maximum value from a local region of the feature map. It retains the most prominent feature in that region, enhancing the network's ability to capture important features.\n",
    "\n",
    "2. **Average Pooling:** Average pooling calculates the average value within a local region. It helps to provide a more generalized view of the features and can be less sensitive to outliers.\n",
    "\n",
    "In summary, pooling operations in CNNs are essential for managing the computational complexity, improving the network's translation invariance, and building hierarchical representations of features. They enable the network to learn relevant and robust features from the input data, making CNNs effective for tasks such as image classification, object detection, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9b9f01",
   "metadata": {},
   "source": [
    "Pooling in Convolutional Neural Networks (CNNs) offers several benefits that contribute to the overall effectiveness and efficiency of the network's feature learning process:\n",
    "\n",
    "1. **Dimensionality Reduction:** Pooling reduces the spatial dimensions of feature maps, which in turn reduces the computational and memory requirements of subsequent layers. This is particularly important in deep networks where each layer's computation can be resource-intensive.\n",
    "\n",
    "2. **Translation Invariance:** Pooling helps the network become partially invariant to small translations or shifts in the input data. This means that even if the position of a feature in an image varies slightly, the network can still recognize it, leading to improved robustness and generalization.\n",
    "\n",
    "3. **Feature Generalization:** By summarizing local features, pooling provides a generalized representation of features. This can help suppress noise or small variations in the input image, allowing the network to focus on more important and consistent patterns.\n",
    "\n",
    "4. **Hierarchical Feature Learning:** Pooling contributes to building hierarchical representations of features. As the network progresses through layers, pooling captures increasingly complex and abstract features by summarizing the lower-level features learned earlier.\n",
    "\n",
    "5. **Reduced Overfitting:** Pooling reduces the spatial dimensions of the feature maps, which can help prevent overfitting by decreasing the number of model parameters. Fewer parameters mean the network is less likely to memorize noise in the training data.\n",
    "\n",
    "6. **Computationally Efficient:** Pooling reduces the amount of computation required in subsequent layers, as it operates on downsampled feature maps. This efficiency allows for faster training and inference times.\n",
    "\n",
    "7. **Memory Efficiency:** Smaller feature maps occupy less memory, making it feasible to train deeper networks with limited computational resources.\n",
    "\n",
    "8. **Locally Aggregated Information:** Pooling captures the most relevant information within local regions of the feature maps. This aggregation of information helps retain the most important features while discarding less relevant details.\n",
    "\n",
    "9. **Regularization:** Pooling acts as a form of regularization by preventing the network from overfitting to small variations in the input. This encourages the network to focus on more prominent and stable features.\n",
    "\n",
    "10. **Better Focus on Significant Features:** Max pooling, in particular, selects the most dominant feature within a local region. This enhances the network's ability to detect salient features that are critical for classification or detection tasks.\n",
    "\n",
    "In summary, pooling plays a vital role in managing computational complexity, enhancing translation invariance, and building hierarchical feature representations in CNNs. These benefits collectively contribute to the network's ability to learn meaningful features from input data and generalize well to unseen examples, making pooling an integral component of CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f179f37",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a488f0",
   "metadata": {},
   "source": [
    "Max pooling and min pooling are both types of pooling operations used in convolutional neural networks (CNNs) to downsample feature maps. However, they differ in their behavior and the type of information they retain:\n",
    "\n",
    "1. **Max Pooling:**\n",
    "   Max pooling is a pooling operation that selects the maximum value within each local region of the feature map. It's the most common form of pooling used in CNNs.\n",
    "   \n",
    "   **Behavior:**\n",
    "   - Max pooling retains the most prominent and dominant features within each local region.\n",
    "   - It enhances the network's ability to detect strong and salient features.\n",
    "   - It discards less important information and noise within the region.\n",
    "   \n",
    "   **Advantages:**\n",
    "   - Enhances robustness to small translations or shifts in the input data.\n",
    "   - Captures the most significant features, making the network focus on the most important information.\n",
    "   - Contributes to better invariance to variations in the input.\n",
    "   \n",
    "   **Limitations:**\n",
    "   - Ignores the relationship between different features within the region.\n",
    "   - Can lead to some loss of spatial information due to the selection of only the maximum value.\n",
    "\n",
    "2. **Min Pooling:**\n",
    "   Min pooling is less common compared to max pooling and involves selecting the minimum value within each local region of the feature map.\n",
    "   \n",
    "   **Behavior:**\n",
    "   - Min pooling retains the least prominent and smallest features within each local region.\n",
    "   - It discards strong features and focuses on the smallest values.\n",
    "   - It may be useful for certain applications, such as anomaly detection or identifying outliers.\n",
    "   \n",
    "   **Advantages:**\n",
    "   - May be useful in scenarios where identifying the smallest features or anomalies is important.\n",
    "   \n",
    "   **Limitations:**\n",
    "   - Less commonly used and not as extensively studied as max pooling.\n",
    "   - Can lead to a loss of important information if the smallest features are relevant to the task.\n",
    "   - Doesn't provide the same robustness to variations as max pooling.\n",
    "\n",
    "In general, max pooling is widely used due to its effectiveness in capturing strong and significant features while providing robustness to translations. Min pooling, on the other hand, is less common and typically used in specialized scenarios where identifying the smallest features is of interest. The choice between max pooling and min pooling depends on the specific requirements of the task at hand and the behavior of the features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92308623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a99159b",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c5ad4",
   "metadata": {},
   "source": [
    "Padding in Convolutional Neural Networks (CNNs) is a technique used to control the spatial dimensions of feature maps throughout the convolutional layers. It involves adding extra pixels (or values) around the borders of an input image or feature map before applying convolutional operations. Padding is particularly useful to control the size reduction of feature maps and to preserve spatial information at the borders of the input data.\n",
    "\n",
    "There are two main types of padding:\n",
    "\n",
    "1. **Valid Padding (No Padding):**\n",
    "   In valid padding, no extra pixels are added to the input before applying convolution. As a result, the spatial dimensions of the output feature map are smaller than the input. This is because the convolutional filter's receptive field must fit entirely within the input image, and the filter center cannot extend beyond the image boundaries.\n",
    "\n",
    "   Valid padding is commonly used when:\n",
    "   - The goal is to reduce the spatial dimensions of the feature map.\n",
    "   - The input image is large enough to maintain useful spatial information.\n",
    "\n",
    "2. **Same Padding (Zero Padding):**\n",
    "   In same padding, additional pixels (usually filled with zeros) are added around the borders of the input image or feature map before applying convolution. This padding ensures that the spatial dimensions of the output feature map remain the same as the input dimensions.\n",
    "\n",
    "   Same padding is useful when:\n",
    "   - You want to preserve the spatial dimensions and ensure that the output feature map has the same size as the input.\n",
    "   - You want to retain information near the edges of the input.\n",
    "\n",
    "Benefits of Padding:\n",
    "\n",
    "1. **Preserving Spatial Information:** Padding prevents the reduction of spatial dimensions after convolutional operations, which is important for maintaining fine-grained spatial information in the feature map.\n",
    "\n",
    "2. **Reducing Border Effects:** Convolutional operations at the edges of an input image may lose information because the receptive field of the filter extends beyond the image borders. Padding can mitigate this issue.\n",
    "\n",
    "3. **Alignment:** Padding ensures that the output feature map is aligned with the input in a way that facilitates stacking multiple layers.\n",
    "\n",
    "4. **Flexibility:** Padding allows you to control the trade-off between reducing spatial dimensions and preserving spatial details.\n",
    "\n",
    "However, it's important to note that padding increases the computational cost of convolutional operations and can introduce additional computational overhead.\n",
    "\n",
    "In summary, padding in CNNs is a technique used to control the spatial dimensions of feature maps by adding extra pixels around the borders of input data. It plays a crucial role in preserving spatial information and reducing border effects during convolutional operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b6c43c",
   "metadata": {},
   "source": [
    "The significance of padding in Convolutional Neural Networks (CNNs) lies in its ability to address several key challenges and enhance the effectiveness of feature extraction and network architecture. Here are the main reasons why padding is significant in CNNs:\n",
    "\n",
    "1. **Prevention of Information Loss:** Padding ensures that the edges and corners of the input data are sufficiently covered by the convolutional filters. Without padding, these border pixels are underutilized, leading to a loss of information. By adding padding, the network can extract features from the entire input, preserving information at the edges.\n",
    "\n",
    "2. **Maintaining Spatial Dimensions:** Padding is crucial for maintaining consistent spatial dimensions between the input and output feature maps. Without padding, the spatial dimensions of the output feature maps reduce with each convolutional layer due to the reduction in filter size. Maintaining spatial dimensions is important for preserving the spatial structure and hierarchical features of the input.\n",
    "\n",
    "3. **Symmetry and Translation Invariance:** Padding ensures that the convolutional filters can be centered around different parts of the input data. This property supports translation invariance, where the network can recognize the same pattern regardless of its position in the input image. Additionally, symmetry around the center pixel of the filter is maintained, enabling the network to learn rotationally invariant features.\n",
    "\n",
    "4. **Effective Feature Extraction:** Padding allows filters to capture features at various spatial resolutions. This enables the network to learn both low-level features, which may be present near the edges, and high-level features that represent complex patterns in the center of the input. Effective feature extraction is essential for the network's ability to learn and discriminate between different patterns and objects.\n",
    "\n",
    "5. **Architectural Consistency:** When designing deep CNN architectures, padding ensures that the spatial dimensions of the feature maps remain relatively consistent across layers. This consistency simplifies the design process and allows for better integration of various layers within the network.\n",
    "\n",
    "6. **Reducing Border Effects:** Without padding, the border pixels of the input are only involved in a fraction of convolutional operations, leading to border effects and distorted feature maps near the edges. Padding helps mitigate these effects by evenly utilizing all input pixels during convolution.\n",
    "\n",
    "7. **Facilitating Network Stacking:** Padding supports the stacking of multiple convolutional layers by maintaining consistent feature map dimensions. This enables the creation of deeper architectures that can capture more complex and abstract features.\n",
    "\n",
    "8. **Control over Stride:** Padding can also affect how the stride parameter (step size of the filter) impacts the output size. By padding appropriately, the stride can be used to downsample the feature map while preserving spatial information.\n",
    "\n",
    "In conclusion, padding in CNNs is significant because it addresses challenges related to information loss, spatial dimensions, symmetry, and effective feature extraction. It also contributes to the architectural consistency and robustness of the network, enabling the creation of more accurate and powerful models for tasks like image recognition, object detection, and segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7072e5",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f3415",
   "metadata": {},
   "source": [
    "Zero Padding and Valid Padding are two types of padding techniques used in Convolutional Neural Networks (CNNs) that have different effects on the output feature map size. Let's compare and contrast them in terms of their impact on the output feature map size:\n",
    "\n",
    "**1. Zero Padding:**\n",
    "- Zero Padding involves adding extra rows and columns of zeros around the input feature map before applying convolution.\n",
    "- It is used to ensure that the spatial dimensions of the output feature map remain the same as the input, especially when using larger convolutional filters or multiple layers.\n",
    "- Zero Padding helps maintain the centering of the filter over different parts of the input, preserving translation invariance and symmetry.\n",
    "- It effectively prevents the shrinking of the feature map dimensions.\n",
    "- The formula to calculate the output feature map size after convolution with zero padding:\n",
    "  Output Size = Input Size + 2 * Padding - Filter Size + 1\n",
    "\n",
    "**2. Valid Padding:**\n",
    "- Valid Padding, also known as \"no padding,\" does not add any extra rows or columns of zeros around the input feature map.\n",
    "- It results in a smaller output feature map size compared to the input.\n",
    "- Valid Padding is used when the goal is to reduce the spatial dimensions of the feature map, as it discards the outer edges of the input.\n",
    "- Since valid padding doesn't preserve spatial dimensions, it may lead to a loss of information near the edges of the input data.\n",
    "- The formula to calculate the output feature map size after convolution with valid padding:\n",
    "  Output Size = Input Size - Filter Size + 1\n",
    "\n",
    "**Comparison:**\n",
    "- Effect on Output Size: Zero Padding maintains the output feature map size, while Valid Padding reduces it.\n",
    "- Preservation of Information: Zero Padding preserves information at the edges of the input, while Valid Padding discards this information.\n",
    "- Translation Invariance: Zero Padding maintains symmetry and translation invariance due to the centered filter, while Valid Padding does not guarantee these properties.\n",
    "- Use Cases: Zero Padding is commonly used to maintain consistent spatial dimensions and to achieve better translation invariance. Valid Padding is useful when downsampling or reducing the feature map size is desired.\n",
    "- Computational Efficiency: Valid Padding involves fewer calculations, which can lead to slightly faster training and inference times.\n",
    "\n",
    "In summary, the choice between zero padding and valid padding depends on the specific goals of the CNN architecture and the desired behavior of the network's layers. Zero Padding is often used to maintain spatial dimensions and improve translation invariance, while Valid Padding is used for downsampling and reducing the feature map size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25338655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ce6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d43c6b0",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #000000 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\"> TOPIC: Exploring LeNet </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15a9d3f",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0c8b89",
   "metadata": {},
   "source": [
    "LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner in 1998. It was designed specifically for handwritten digit recognition and played a significant role in popularizing the use of CNNs for image classification tasks. LeNet-5 consists of several layers that perform convolution, pooling, and fully connected operations. Here's a brief overview of the LeNet-5 architecture:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - LeNet-5 accepts grayscale images of size 32x32 pixels as input.\n",
    "\n",
    "2. **Convolutional Layer 1:**\n",
    "   - The first convolutional layer applies six filters (also known as kernels or feature detectors) of size 5x5 to the input image.\n",
    "   - Each filter computes a dot product between its weights and a local receptive field in the input image, producing a feature map.\n",
    "   - The output feature maps have a size of 28x28 (32 - 5 + 1 = 28) due to the loss of border pixels.\n",
    "\n",
    "3. **Average Pooling Layer 1:**\n",
    "   - After convolution, LeNet-5 uses average pooling to downsample the feature maps.\n",
    "   - The pooling layer uses 2x2 non-overlapping windows and computes the average value within each window.\n",
    "   - This reduces the feature map size to 14x14.\n",
    "\n",
    "4. **Convolutional Layer 2:**\n",
    "   - The second convolutional layer applies 16 filters of size 5x5 to the output of the first pooling layer.\n",
    "   - The resulting feature maps have a size of 10x10.\n",
    "\n",
    "5. **Average Pooling Layer 2:**\n",
    "   - Similar to the first pooling layer, this layer further reduces the feature map size to 5x5.\n",
    "\n",
    "6. **Flatten Layer:**\n",
    "   - The output of the second pooling layer is flattened into a vector to be fed into fully connected layers.\n",
    "\n",
    "7. **Fully Connected Layer 1:**\n",
    "   - The flattened vector is connected to a fully connected layer with 120 neurons.\n",
    "   - Each neuron performs a weighted sum of its inputs followed by a non-linear activation function.\n",
    "\n",
    "8. **Fully Connected Layer 2:**\n",
    "   - The output of the first fully connected layer is connected to another fully connected layer with 84 neurons.\n",
    "\n",
    "9. **Output Layer:**\n",
    "   - The final fully connected layer consists of 10 neurons, each corresponding to one of the possible digit classes (0 to 9).\n",
    "   - The softmax activation function is applied to obtain class probabilities.\n",
    "\n",
    "LeNet-5 effectively demonstrates the power of convolutional neural networks for image classification tasks. While modern architectures have become more complex and deeper, LeNet-5's design principles of alternating convolution and pooling layers, followed by fully connected layers, laid the foundation for subsequent advancements in CNN architectures.\n",
    "\n",
    "It's important to note that while LeNet-5 was designed for small grayscale images, many contemporary CNN architectures are tailored for larger and more complex images with color channels and deeper networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31711f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0c208f8",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513dad4",
   "metadata": {},
   "source": [
    "LeNet-5, a pioneering convolutional neural network (CNN) architecture, consists of several key components that contribute to its effectiveness in image classification tasks. Here are the key components of LeNet-5:\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   LeNet-5 includes two convolutional layers. These layers apply a set of learnable filters (kernels) to the input image, capturing various features and patterns. The convolution operation involves sliding the filters over the input and computing dot products to generate feature maps.\n",
    "\n",
    "2. **Pooling Layers:**\n",
    "   The architecture utilizes average pooling layers to downsample the feature maps produced by the convolutional layers. Pooling helps reduce the spatial dimensions of the feature maps, making the network more robust to small spatial translations and variations in input.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   LeNet-5 includes two fully connected layers at the end of the network. These layers take flattened feature vectors from the previous layers and perform weighted sum operations followed by activation functions. The fully connected layers enable the network to make final predictions based on the extracted features.\n",
    "\n",
    "4. **Activation Functions:**\n",
    "   Throughout the network, non-linear activation functions are applied element-wise to the outputs of the convolutional and fully connected layers. LeNet-5 uses the sigmoid activation function for hidden layers and the softmax activation function in the output layer to convert raw scores into class probabilities.\n",
    "\n",
    "5. **Flatten Layer:**\n",
    "   Before entering the fully connected layers, the output of the last pooling layer is flattened into a 1D vector. This allows the network to process the spatial information as a sequence of features.\n",
    "\n",
    "6. **Input Image Size:**\n",
    "   LeNet-5 was designed to work with 32x32 pixel grayscale images. The input image dimensions are small compared to modern architectures due to the computational limitations of its time.\n",
    "\n",
    "7. **Local Receptive Fields:**\n",
    "   The convolutional layers use small receptive fields (filter sizes) to capture local patterns and features in the input image. This design choice helps the network focus on low-level features.\n",
    "\n",
    "8. **Weight Sharing:**\n",
    "   LeNet-5 employs weight sharing, meaning the same filter is applied across different spatial locations of the input. This reduces the number of parameters and enables the network to generalize better.\n",
    "\n",
    "9. **Gradient-Based Learning:**\n",
    "   The architecture is trained using backpropagation and gradient descent algorithms. The gradients of the loss with respect to the network's weights are computed and used to update the weights during training.\n",
    "\n",
    "10. **Softmax Output:**\n",
    "    The final layer of LeNet-5 uses the softmax activation function to produce class probabilities. The class with the highest probability is considered the network's prediction.\n",
    "\n",
    "11. **Training Data Augmentation:**\n",
    "    Although not a direct component of the architecture, data augmentation techniques like random cropping and horizontal flipping are often applied to the training data to increase the network's robustness and reduce overfitting.\n",
    "\n",
    "LeNet-5's components were groundbreaking at the time of its development and have since become foundational concepts in CNN design. While contemporary CNN architectures have evolved to address more complex tasks and larger datasets, the principles introduced by LeNet-5 continue to influence the design of modern networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c533d9",
   "metadata": {},
   "source": [
    " key components of LeNet-5 along with their respective purposes:\n",
    "\n",
    "1. **Convolutional Layers:**\n",
    "   Purpose: The convolutional layers apply filters (kernels) to the input image to detect various local features such as edges, corners, and textures. The convolution operation captures patterns in the input data and creates feature maps that highlight these patterns.\n",
    "\n",
    "2. **Pooling Layers:**\n",
    "   Purpose: Pooling layers perform downsampling by selecting the most important information from the feature maps. Average pooling computes the average value within a local region, while max pooling selects the maximum value. Pooling reduces the spatial dimensions of the feature maps, making the network more computationally efficient and robust to small spatial translations.\n",
    "\n",
    "3. **Fully Connected Layers:**\n",
    "   Purpose: The fully connected layers take the high-level features captured by the convolutional and pooling layers and combine them to make final predictions. These layers learn complex combinations of features and their relationships, allowing the network to differentiate between classes.\n",
    "\n",
    "4. **Activation Functions:**\n",
    "   Purpose: Activation functions introduce non-linearity to the network, enabling it to model complex relationships in the data. The sigmoid activation used in LeNet-5 helps introduce non-linearity to hidden layers, while the softmax activation in the output layer converts raw scores into class probabilities.\n",
    "\n",
    "5. **Flatten Layer:**\n",
    "   Purpose: The flatten layer transforms the output of the last pooling layer from a 2D grid of features to a 1D vector. This prepares the data for the fully connected layers, which require a flattened input.\n",
    "\n",
    "6. **Input Image Size:**\n",
    "   Purpose: LeNet-5 was designed to work with small 32x32 pixel grayscale images. This choice of input image size was influenced by the available computational resources at the time. It enabled the network to be trained efficiently on early computing hardware.\n",
    "\n",
    "7. **Local Receptive Fields:**\n",
    "   Purpose: Small receptive fields capture local patterns in the input image. By focusing on small regions at a time, the network becomes sensitive to basic features such as edges and textures. This hierarchical approach allows the network to gradually learn more complex features in higher layers.\n",
    "\n",
    "8. **Weight Sharing:**\n",
    "   Purpose: Weight sharing reduces the number of parameters in the network. By using the same set of weights across different spatial locations, the network is better able to generalize to variations in the input. This concept was introduced to manage the limited computational resources of the time.\n",
    "\n",
    "9. **Gradient-Based Learning:**\n",
    "   Purpose: LeNet-5 is trained using gradient-based optimization algorithms such as backpropagation and stochastic gradient descent. These algorithms calculate gradients of the loss with respect to the model's weights, allowing the weights to be updated iteratively in the direction that minimizes the loss.\n",
    "\n",
    "10. **Softmax Output:**\n",
    "    Purpose: The softmax activation function in the output layer converts the raw scores generated by the previous layers into class probabilities. The class with the highest probability is considered the network's prediction. This final layer facilitates the classification of input images.\n",
    "\n",
    "11. **Training Data Augmentation:**\n",
    "    Purpose: Data augmentation techniques such as random cropping and horizontal flipping help increase the diversity of the training dataset. This augmentation reduces overfitting and helps the network generalize better to unseen data.\n",
    "\n",
    "Each component of LeNet-5 contributes to its ability to extract relevant features from input images and make accurate predictions. These components, although simple by today's standards, laid the foundation for the development of more sophisticated and powerful CNN architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a43980",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a650f",
   "metadata": {},
   "source": [
    "LeNet-5, introduced by Yann LeCun and his colleagues in the early 1990s, was one of the pioneering convolutional neural network (CNN) architectures designed specifically for image classification tasks. Despite its simplicity compared to modern CNNs, LeNet-5 brought forward several advantages that contributed to its success and laid the groundwork for the advancement of deep learning in image classification:\n",
    "\n",
    "1. **Hierarchical Feature Learning:** LeNet-5 introduced the concept of hierarchical feature learning through multiple convolutional and pooling layers. The architecture's successive layers captured increasingly complex features, starting with edges and textures and progressing to more abstract features. This hierarchical approach enabled the network to learn meaningful representations of the input data.\n",
    "\n",
    "2. **Local Receptive Fields:** By using small receptive fields (convolutional filters), LeNet-5 captured local patterns in the input images. This approach made the network sensitive to specific features and helped it recognize basic structures like edges, corners, and textures. Local receptive fields allowed LeNet-5 to efficiently identify important local information.\n",
    "\n",
    "3. **Weight Sharing:** LeNet-5 introduced the concept of weight sharing, where the same set of weights was used across different spatial locations in the convolutional layers. This reduced the number of parameters in the network, making it easier to train and less prone to overfitting. Weight sharing also contributed to the network's ability to generalize well to variations in the input data.\n",
    "\n",
    "4. **Translation Invariance:** Because of the use of weight sharing and pooling layers, LeNet-5 exhibited translation invariance. This means that the network could recognize patterns and features in the input image regardless of their position. This property made the network more robust to small spatial translations and variations.\n",
    "\n",
    "5. **Efficient Architecture:** LeNet-5 was designed with computational efficiency in mind, making it feasible to train and deploy on the limited computing resources available at the time. Its small input size (32x32 pixels) and relatively modest number of layers and parameters allowed for faster training and inference compared to more complex architectures.\n",
    "\n",
    "6. **Early Success in Handwritten Digit Recognition:** LeNet-5's architecture was originally developed for recognizing handwritten digits in postal addresses, demonstrating its effectiveness in real-world applications. It achieved state-of-the-art results on the MNIST dataset and served as a proof of concept for the power of CNNs in image classification tasks.\n",
    "\n",
    "7. **Inspiration for Future Architectures:** LeNet-5 paved the way for further research and development of deep learning architectures for image classification. Its success inspired researchers to explore more complex and deeper CNN architectures, leading to breakthroughs in various computer vision tasks.\n",
    "\n",
    "8. **Impact on Convolutional Neural Networks:** LeNet-5 demonstrated the potential of convolutional neural networks and their ability to learn hierarchical features from image data. Its success played a crucial role in popularizing the use of CNNs in image classification and laid the foundation for the subsequent evolution of CNN architectures.\n",
    "\n",
    "In summary, LeNet-5's advantages in hierarchical feature learning, local receptive fields, weight sharing, translation invariance, and computational efficiency contributed to its effectiveness in image classification tasks. Its impact on the field of deep learning is undeniable, and it remains a significant milestone in the history of convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a87a4",
   "metadata": {},
   "source": [
    "While LeNet-5 was a pioneering architecture that contributed significantly to the field of deep learning, it also had some limitations, especially when considering the current state of image classification tasks and the advancements in neural network architectures. Here are some limitations of LeNet-5:\n",
    "\n",
    "1. **Limited Complexity:** LeNet-5 has a relatively shallow architecture compared to modern CNNs. It consists of only a few convolutional and pooling layers, which limits its ability to capture complex and high-level features in images. As a result, it may struggle to differentiate between objects with intricate details or textures.\n",
    "\n",
    "2. **Small Receptive Fields:** The small size of the receptive fields (5x5 pixels in the first layer) means that LeNet-5 can only capture relatively local features. This limitation prevents the network from effectively recognizing large, global patterns or objects that require a broader context.\n",
    "\n",
    "3. **Limited Input Size:** LeNet-5 was designed to work with small input images (32x32 pixels), which might not provide enough detail for recognizing fine-grained features in larger images. In contrast, modern CNNs often work with larger images, enabling them to capture more intricate details.\n",
    "\n",
    "4. **Lack of Non-Linearity:** LeNet-5 mainly uses the tanh activation function, which can suffer from the vanishing gradient problem, particularly in deeper networks. This can hinder the training process and limit the network's ability to learn complex relationships in the data.\n",
    "\n",
    "5. **Lack of Fully Connected Layers:** LeNet-5 doesn't include fully connected layers, which are common in modern architectures. Fully connected layers enable the network to learn global relationships and combine features learned across different spatial locations. This limitation could restrict the network's ability to perform well on more complex tasks.\n",
    "\n",
    "6. **Limited Performance on Complex Datasets:** While LeNet-5 performed well on relatively simple datasets like MNIST (handwritten digit recognition) and similar tasks, its simplicity might make it struggle with more complex and diverse datasets, where the features to be learned are more nuanced and varied.\n",
    "\n",
    "7. **Not Suitable for Large-Scale Networks:** LeNet-5's design is not well-suited for building very deep networks that are characteristic of contemporary architectures like ResNet and Inception. Its shallow architecture might limit its capacity to learn hierarchical features in large-scale tasks.\n",
    "\n",
    "8. **Limited Data Augmentation:** Data augmentation techniques, which help improve a model's generalization by artificially increasing the diversity of training data, were not as commonly used during the time of LeNet-5's development. This could impact the model's ability to handle variations in the input data.\n",
    "\n",
    "9. **Dependence on Hand-Crafted Features:** LeNet-5 still relied on some hand-crafted feature engineering, such as Canny edge detection, to preprocess the input data. Modern architectures tend to learn features directly from raw data, eliminating the need for manual preprocessing steps.\n",
    "\n",
    "In conclusion, while LeNet-5 was groundbreaking and effective for its time, its limitations become apparent when compared to more modern and complex CNN architectures. The field of deep learning has advanced considerably since the introduction of LeNet-5, leading to architectures that address many of these limitations and perform exceptionally well on a wide range of image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696dfc31",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cc9b2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 37s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3), (10000, 32, 32, 3), (50000, 10), (10000, 10))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "x_train.shape , x_test.shape , y_train.shape ,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544860d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 6)         456       \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 14, 14, 6)        0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 5, 5, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the Model Architecture\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='tanh', input_shape = (32,32,3)))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size = (5,5), padding = 'valid', activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68caccdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 6)         456       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 6)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62,006\n",
      "Trainable params: 62,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the LeNet-5 model\n",
    "def build_lenet5(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Layer 1: Convolutional Layer with 6 filters of size 5x5\n",
    "    model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    # Layer 2: Convolutional Layer with 16 filters of size 5x5\n",
    "    model.add(Conv2D(16, kernel_size=(5, 5), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    # Flatten the feature maps\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Layer 3: Fully Connected Layer with 120 units\n",
    "    model.add(Dense(120, activation='relu'))\n",
    "    \n",
    "    # Layer 4: Fully Connected Layer with 84 units\n",
    "    model.add(Dense(84, activation='relu'))\n",
    "    \n",
    "    # Layer 5: Fully Connected Layer with output units equal to the number of classes\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (32, 32, 3)  \n",
    "num_classes = 10  \n",
    "\n",
    "model = build_lenet5(input_shape, num_classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a46e1ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from  keras.callbacks import ModelCheckpoint\n",
    "chckpoint = ModelCheckpoint(\"model_weights.h5\" , save_weights_only= True, save_best_only=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1e6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "391/391 [==============================] - 133s 339ms/step - loss: 1.7252 - accuracy: 0.3735 - val_loss: 1.5029 - val_accuracy: 0.4535\n",
      "Epoch 2/30\n",
      "391/391 [==============================] - 109s 277ms/step - loss: 1.4399 - accuracy: 0.4844 - val_loss: 1.4181 - val_accuracy: 0.4984\n",
      "Epoch 3/30\n",
      "391/391 [==============================] - 85s 217ms/step - loss: 1.3159 - accuracy: 0.5334 - val_loss: 1.3238 - val_accuracy: 0.5302\n",
      "Epoch 4/30\n",
      "391/391 [==============================] - 81s 206ms/step - loss: 1.2391 - accuracy: 0.5606 - val_loss: 1.2364 - val_accuracy: 0.5622\n",
      "Epoch 5/30\n",
      "185/391 [=============>................] - ETA: 57s - loss: 1.1855 - accuracy: 0.5808"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.metrics.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=30, verbose=1, validation_data=(x_test, y_test) , callbacks=[chckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a249a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4a95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test)\n",
    "\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d525588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Accuracy:\", history.history['accuracy'][-1])\n",
    "print(\"Validation Accuracy:\", history.history['val_accuracy'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf860d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sgd_history.history).tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = sgd_model.predict(X_test)\n",
    "predicted_classes = np_utils.to_categorical(np.argmax(predicted_classes ,axis =-1), nb_classes)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990b06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_indices = np.nonzero(predicted_classes == y_test)\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)\n",
    "\n",
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[1][:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(np.argmax(predicted_classes[correct] ,axis =-1), np.argmax(y_test[correct],axis =-1)))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[0][:9]):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(np.argmax(predicted_classes[incorrect] ,axis =-1), np.argmax(y_test[incorrect],axis =-1)))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def  plot():\n",
    "        # Plot training loss and validation loss\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training and Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Plot training accuracy and validation accuracy\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb63820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca923bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db4ddc31",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #000000 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\"> TOPIC: Analyzing AlexNet </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c2966",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cff830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d900300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8394003e",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836eb856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d893fe11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "941b5b3e",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fad61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5404dd7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97b940ba",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a> \n",
    " ## <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29fbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880b09d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03cba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c84d921",
   "metadata": {},
   "source": [
    "<a id=\"14\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13907086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e233fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6dd27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e16c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7416162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29975ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253286e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299e3038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633ebb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
