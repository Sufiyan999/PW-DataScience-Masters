{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9585f8",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca3694",
   "metadata": {},
   "source": [
    "Analysis of Variance (ANOVA) is a statistical method used to compare means between multiple groups. To use ANOVA effectively and draw valid conclusions, several assumptions need to be met. These assumptions ensure that the results obtained from the ANOVA analysis are accurate and reliable. The key assumptions required to use ANOVA are as follows:\n",
    "\n",
    "1. **Independence of Observations:** The observations within each group and between groups should be independent of each other. This means that the value of one observation should not be influenced by the value of another observation, neither within nor between groups.\n",
    "\n",
    "2. **Normality:** The data within each group should follow a normal distribution. This assumption is important because ANOVA relies on the normal distribution to make inferences about the population means. You can check normality using methods like histograms, Q-Q plots, or statistical tests like the Shapiro-Wilk test.\n",
    "\n",
    "3. **Homogeneity of Variances (Homoscedasticity):** The variances of the data within each group should be roughly equal across all groups. In other words, the variability in the data should be similar among different groups. You can assess homogeneity of variances using techniques like Levene's test or Bartlett's test.\n",
    "\n",
    "4. **Mutual Independence:** The observations across different groups should be mutually independent. This means that the behavior of one group should not affect the behavior of another group. Violating this assumption can lead to confounding effects and unreliable results.\n",
    "\n",
    "If these assumptions are not met, the results of the ANOVA analysis may be biased, and the conclusions drawn may not accurately represent the underlying population. It's important to assess these assumptions before performing ANOVA and, if necessary, take appropriate steps to address any violations.\n",
    "\n",
    "When the assumptions of ANOVA are met, it is a powerful tool for comparing means among multiple groups. However, if the assumptions are significantly violated, alternative methods or transformations of the data might be needed to ensure the validity of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237b561",
   "metadata": {},
   "source": [
    " examples of violations of the assumptions required for ANOVA and how they could impact the validity of the results:\n",
    "\n",
    "1. **Independence of Observations:**\n",
    "   - Example: In a study of the effect of a new teaching method on student performance, a teacher uses the new method for one group and the traditional method for another. However, the teacher provides additional help to the group using the new method, which may lead to dependencies between the groups.\n",
    "   - Impact: The dependency between groups can introduce bias and confounding effects, making it difficult to isolate the true effect of the teaching method.\n",
    "\n",
    "2. **Normality:**\n",
    "   - Example: In a study comparing reaction times of different age groups, the data within each group show a skewed distribution and do not resemble a normal distribution.\n",
    "   - Impact: Violation of normality can affect the validity of the p-values and confidence intervals, leading to incorrect conclusions about the differences between group means.\n",
    "\n",
    "3. **Homogeneity of Variances (Homoscedasticity):**\n",
    "   - Example: In a research study comparing the effectiveness of two medications on reducing blood pressure, one group has a higher variance in blood pressure measurements than the other.\n",
    "   - Impact: Violation of homogeneity of variances can result in unequal representation of the groups' variances in the ANOVA, leading to incorrect conclusions about the significance of group differences.\n",
    "\n",
    "4. **Mutual Independence:**\n",
    "   - Example: In a study examining the effects of exercise on weight loss, individuals within the same household are enrolled in different exercise programs. Their weight loss outcomes might be influenced by shared household behaviors.\n",
    "   - Impact: Mutual independence violation can introduce confounding variables that affect the observed differences between groups, potentially leading to spurious findings.\n",
    "\n",
    "It's important to assess these assumptions before conducting ANOVA and take appropriate actions if violations are detected. In some cases, transformations of the data, non-parametric methods, or additional statistical techniques may be used to address violations and ensure the validity of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd3159",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad204b5",
   "metadata": {},
   "source": [
    "There are three main types of Analysis of Variance (ANOVA) techniques, each designed to address specific types of experimental designs and research questions. The three types of ANOVA are:\n",
    "\n",
    "1. **One-Way ANOVA:**\n",
    "   - Used when you have one categorical independent variable (factor) with more than two levels (groups).\n",
    "   - Research Question: Is there a significant difference in the means of the dependent variable among the different levels of the independent variable?\n",
    "   - Example: Comparing the performance of students in three different teaching methods (A, B, and C) to determine if there is a significant difference in test scores.\n",
    "\n",
    "2. **Two-Way ANOVA:**\n",
    "   - Used when you have two categorical independent variables (factors) and want to study their main effects and interaction effect.\n",
    "   - Research Question: Are there significant differences in the means of the dependent variable based on the main effects of both independent variables and their interaction?\n",
    "   - Example: Analyzing the effects of both gender (male/female) and treatment type (A/B) on a measure of anxiety levels.\n",
    "\n",
    "3. **Repeated Measures ANOVA (or Within-Subjects ANOVA):**\n",
    "   - Used when the same participants are measured under different conditions, leading to correlated data points.\n",
    "   - Research Question: Is there a significant difference in means across different conditions or treatments while accounting for within-subject variability?\n",
    "   - Example: Investigating changes in participants' heart rate measured at different time points before and after receiving different doses of a drug.\n",
    "\n",
    "Each type of ANOVA serves a specific purpose based on the research design and the number of independent variables involved. It's important to choose the appropriate type of ANOVA based on your research question and experimental setup to ensure that you obtain accurate and meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12446026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1bf48ed",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc9b44e",
   "metadata": {},
   "source": [
    "The partitioning of variance in ANOVA refers to the process of decomposing the total variability observed in a dataset into different components that can be attributed to various sources. ANOVA is used to analyze the differences in means between groups and to determine the contributions of different factors to the total variability in the data.\n",
    "\n",
    "In a general sense, the variance of a dataset can be divided into three main components in the context of ANOVA:\n",
    "\n",
    "1. **Between-Groups Variance (Treatment Variance):**\n",
    "   - This component represents the variability of the group means from the overall mean.\n",
    "   - It reflects the differences among the group means that are attributed to the treatment or factor being studied.\n",
    "   - A large between-groups variance suggests that the groups are significantly different from each other due to the effect of the independent variable.\n",
    "\n",
    "2. **Within-Groups Variance (Error Variance):**\n",
    "   - This component accounts for the variability within each group or treatment.\n",
    "   - It represents the random variability or measurement error that cannot be attributed to the independent variable.\n",
    "   - A smaller within-groups variance indicates that the observations within each group are relatively consistent, implying that the effects of the independent variable are significant.\n",
    "\n",
    "3. **Total Variance:**\n",
    "   - This is the overall variability in the data, combining both the between-groups variance and the within-groups variance.\n",
    "   - It represents the total variability observed in the dataset across all groups and conditions.\n",
    "\n",
    "The partitioning of variance is fundamental in ANOVA because it allows researchers to assess the proportion of total variability that can be explained by the differences between groups (treatment effect) and the proportion that is due to random variability (error). By comparing the explained variance to the total variance, ANOVA helps determine whether the differences among group means are statistically significant.\n",
    "\n",
    "In summary, ANOVA partitioning allows us to quantitatively assess the impact of different factors on the variation in the data, helping us understand the sources of variability and the significance of the observed differences among groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e394cd",
   "metadata": {},
   "source": [
    "Understanding the concept of partitioning of variance in ANOVA is crucial for several reasons:\n",
    "\n",
    "1. **Interpretation of Results:** ANOVA results provide insights into the sources of variation in the data. By understanding how the total variance is partitioned into between-groups and within-groups components, researchers can interpret the significance and impact of different factors or treatments on the outcome variable.\n",
    "\n",
    "2. **Identifying Significant Effects:** Partitioning of variance helps identify whether the observed differences among groups are statistically significant. It allows researchers to determine whether the variation attributed to the treatment effect is larger than the random variation within each group.\n",
    "\n",
    "3. **Experimental Design:** When designing experiments, understanding partitioning of variance helps researchers allocate resources effectively. It allows for better decision-making in terms of sample size determination and experimental conditions that contribute the most to the observed variation.\n",
    "\n",
    "4. **Improving Model Fit:** In situations where ANOVA is used as part of a larger statistical model, understanding the partitioning of variance helps researchers assess the fit of the model to the data. This can guide model selection and refinement.\n",
    "\n",
    "5. **Comparing Factors:** When multiple factors are involved, understanding how each factor contributes to the variation helps in making comparisons. Researchers can determine which factors have a larger influence on the outcome variable and focus on the most relevant ones.\n",
    "\n",
    "6. **Generalization of Results:** A clear understanding of variance partitioning enhances the ability to generalize findings to larger populations. It provides insight into whether the observed effects are consistent and reliable beyond the sample.\n",
    "\n",
    "7. **Enhanced Communication:** Researchers can effectively communicate their findings to others, including colleagues and stakeholders, when they can explain how the variance in the data is distributed and what factors contribute to observed differences.\n",
    "\n",
    "8. **Decision-Making:** Businesses, organizations, and policymakers often base decisions on the outcomes of ANOVA analyses. Understanding partitioning of variance ensures that these decisions are well-informed and based on a solid statistical foundation.\n",
    "\n",
    "In essence, the concept of partitioning of variance is the foundation of ANOVA, allowing researchers to assess relationships, significance, and the impact of treatments or factors on observed differences. This understanding ensures that the conclusions drawn from ANOVA analyses are accurate, valid, and can guide subsequent actions or decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1946ba",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6dbf9",
   "metadata": {},
   "source": [
    "The Total Sum of Squares (SST) is a measure of the total variability observed in the data. It quantifies how much the individual observations deviate from the overall mean. SST is an essential component in the analysis of variance (ANOVA) framework, as it provides a basis for assessing the variability that can be attributed to treatment effects and random variability.\n",
    "\n",
    "To calculate the Total Sum of Squares (SST), follow these steps:\n",
    "\n",
    "1. Calculate the overall mean (grand mean) of all observations.\n",
    "\n",
    "2. For each observation, calculate the squared difference between its value and the overall mean.\n",
    "\n",
    "3. Sum up the squared differences obtained in step 2 for all observations.\n",
    "\n",
    "The formula for calculating SST is:\n",
    "\n",
    "\\[ SST = ∑(X_i - μ)^2 \\]\n",
    "\n",
    "Where:\n",
    "- \\(n\\) is the total number of observations.\n",
    "- \\(X_i\\) represents the value of the \\(i\\)th observation.\n",
    "- \\(μ\\) is the overall mean of all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c5d65d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Mean: 20.0\n",
      "Squared Differences: [100.0, 25.0, 0.0, 25.0, 100.0]\n",
      "Total Sum of Squares (SST): 250.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = np.array([10, 15, 20, 25, 30])\n",
    "\n",
    "# Calculate the overall mean\n",
    "overall_mean = np.mean(data)\n",
    "\n",
    "# Calculate the squared differences from the overall mean\n",
    "squared_diff = [(x - overall_mean)**2 for x in data]\n",
    "\n",
    "# Calculate the Total Sum of Squares (SST)\n",
    "sst = np.sum(squared_diff)\n",
    "\n",
    "print(\"Overall Mean:\", overall_mean)\n",
    "print(\"Squared Differences:\", squared_diff)\n",
    "print(\"Total Sum of Squares (SST):\", sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576327d2",
   "metadata": {},
   "source": [
    "In this example, the code calculates the SST for a sample dataset. The squared differences between each observation and the overall mean are summed to obtain the SST. The SST quantifies the total variability in the data without considering any grouping or factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd12d301",
   "metadata": {},
   "source": [
    "**Explained Sum of Squares (SSE):**\n",
    "\n",
    "SSE measures the variability in the data that can be explained by the differences between the group means. In other words, SSE represents the sum of squared deviations of each group mean from the overall mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3681e9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Sum of Squares (SSE): 50.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data for three groups\n",
    "group1 = np.array([25, 30, 35, 40, 45])\n",
    "group2 = np.array([20, 25, 30, 35, 40])\n",
    "group3 = np.array([15, 20, 25, 30, 35])\n",
    "\n",
    "# Calculate overall mean\n",
    "overall_mean = np.mean(np.concatenate((group1, group2, group3)))\n",
    "\n",
    "# Calculate group means\n",
    "mean_group1 = np.mean(group1)\n",
    "mean_group2 = np.mean(group2)\n",
    "mean_group3 = np.mean(group3)\n",
    "\n",
    "# Calculate Explained Sum of Squares (SSE)\n",
    "sse = np.sum((mean_group1 - overall_mean)**2) + np.sum((mean_group2 - overall_mean)**2) + np.sum((mean_group3 - overall_mean)**2)\n",
    "\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096b3cd",
   "metadata": {},
   "source": [
    "**Residual Sum of Squares (SSR):**\n",
    "    \n",
    "SSR measures the unexplained variability in the data, often referred to as the \"error.\" It quantifies the sum of squared differences between individual data points and their respective group means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b327d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Sum of Squares (SSR): 750.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate Residual Sum of Squares (SSR)\n",
    "ssr_group1 = np.sum((group1 - mean_group1)**2)\n",
    "ssr_group2 = np.sum((group2 - mean_group2)**2)\n",
    "ssr_group3 = np.sum((group3 - mean_group3)**2)\n",
    "ssr = ssr_group1 + ssr_group2 + ssr_group3\n",
    "\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f795957",
   "metadata": {},
   "source": [
    "In the context of ANOVA, we're interested in comparing the explained variability (SSE) to the unexplained variability (SSR) to determine whether the differences in group means are significant. This is done by calculating the F-statistic and evaluating its associated p-value. If the p-value is sufficiently small, it suggests that the differences between the group means are statistically significant, indicating that the independent variable (factor) likely has an effect on the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2e7653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Sum of Squares (SSE): 50.0\n",
      "Residual Sum of Squares (SSR): 750.0\n",
      "F-Statistic: 0.4\n",
      "P-Value: 0.6789341568946838\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Sample data for three groups\n",
    "group1 = np.array([25, 30, 35, 40, 45])\n",
    "group2 = np.array([20, 25, 30, 35, 40])\n",
    "group3 = np.array([15, 20, 25, 30, 35])\n",
    "\n",
    "# Overall mean of all observations\n",
    "overall_mean = np.mean(np.concatenate((group1, group2, group3)))\n",
    "\n",
    "# Calculate group means\n",
    "mean_group1 = np.mean(group1)\n",
    "mean_group2 = np.mean(group2)\n",
    "mean_group3 = np.mean(group3)\n",
    "\n",
    "# Calculate the Explained Sum of Squares (SSE)\n",
    "sse = np.sum((mean_group1 - overall_mean)**2) + np.sum((mean_group2 - overall_mean)**2) + np.sum((mean_group3 - overall_mean)**2)\n",
    "\n",
    "# Calculate the Residual Sum of Squares (SSR)\n",
    "ssr_group1 = np.sum((group1 - mean_group1)**2)\n",
    "ssr_group2 = np.sum((group2 - mean_group2)**2)\n",
    "ssr_group3 = np.sum((group3 - mean_group3)**2)\n",
    "ssr = ssr_group1 + ssr_group2 + ssr_group3\n",
    "\n",
    "# Degrees of freedom\n",
    "df_between = 2  # Number of groups minus 1\n",
    "df_within = 12  # Total number of observations minus the number of groups\n",
    "df_total = 14   # Total number of observations minus 1\n",
    "\n",
    "# Calculate the Mean Squares\n",
    "ms_between = sse / df_between\n",
    "ms_within = ssr / df_within\n",
    "\n",
    "# Calculate the F-statistic\n",
    "f_statistic = ms_between / ms_within\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = 1 - stats.f.cdf(f_statistic, df_between, df_within)\n",
    "\n",
    "print(\"Explained Sum of Squares (SSE):\", sse)\n",
    "print(\"Residual Sum of Squares (SSR):\", ssr)\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"P-Value:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334e8cbf",
   "metadata": {},
   "source": [
    "In this example, we calculate the Explained Sum of Squares (SSE) by summing the squared differences between each group mean and the overall mean. The Residual Sum of Squares (SSR) is calculated by summing the squared differences between individual data points and their respective group mean. These two components are essential for calculating the F-statistic and assessing the significance of the group means' differences in a one-way ANOVA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd0813",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3cbf4",
   "metadata": {},
   "source": [
    "In a two-way ANOVA, you can calculate the main effects and interaction effects to understand the impact of two categorical independent variables (factors) on a continuous dependent variable. The main effects represent the individual effects of each factor, while the interaction effect represents how the combination of factors affects the dependent variable. Here's how you can calculate main effects and interaction effects using Python:\n",
    "\n",
    "consider an example with two factors: Factor A (with levels A1 and A2) and Factor B (with levels B1 and B2). We'll use the following hypothetical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf30bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   sum_sq   df         F    PR(>F)\n",
      "Factor_A           36.125  1.0  2.979381  0.159410\n",
      "Factor_B           28.125  1.0  2.319588  0.202420\n",
      "Factor_A:Factor_B   0.125  1.0  0.010309  0.924012\n",
      "Residual           48.500  4.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Create a dataframe with hypothetical data\n",
    "data = pd.DataFrame({\n",
    "    'Factor_A': ['A1', 'A1', 'A2', 'A2', 'A1', 'A1', 'A2', 'A2'],\n",
    "    'Factor_B': ['B1', 'B2', 'B1', 'B2', 'B1', 'B2', 'B1', 'B2'],\n",
    "    'Dependent_Variable': [10, 15, 12, 18, 14, 16, 20, 22]\n",
    "})\n",
    "\n",
    "# Fit the two-way ANOVA model\n",
    "formula = 'Dependent_Variable ~ Factor_A + Factor_B + Factor_A:Factor_B'\n",
    "model = ols(formula, data=data).fit()\n",
    "\n",
    "# Perform ANOVA analysis\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3306f428",
   "metadata": {},
   "source": [
    "In this example, we're using the `statsmodels` library to perform the two-way ANOVA. The formula specifies the model including the main effects of both factors (Factor_A and Factor_B) and their interaction effect (Factor_A:Factor_B). The ANOVA table provides information about the main effects and interaction effects.\n",
    "\n",
    "In the ANOVA table, you can find the sum of squares, degrees of freedom, mean squares, F-statistic, and p-values for the main effects of Factor A and Factor B, as well as for their interaction effect (Factor_A:Factor_B). The p-values indicate the significance of each effect. If the p-value is below the chosen significance level (e.g., 0.05), you can conclude that the corresponding effect is statistically significant.\n",
    "\n",
    "Interpreting the ANOVA results will give you insights into whether the main effects and interaction effects are contributing significantly to the variability in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9a4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06dca56f",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 6 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87477f16",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic and its associated p-value are used to determine whether there are significant differences between the means of the groups. Let's interpret the F-statistic of 5.23 and the p-value of 0.02 in your scenario:\n",
    "\n",
    "1. **F-Statistic (5.23):**\n",
    "   - The F-statistic is a measure of the ratio of the variability between group means to the variability within the groups.\n",
    "   - A larger F-statistic suggests that the differences between the group means are relatively large compared to the random variability within each group.\n",
    "\n",
    "2. **p-value (0.02):**\n",
    "   - The p-value is a measure of the probability of observing the F-statistic or a more extreme value under the assumption that there are no real differences between the group means (null hypothesis is true).\n",
    "   - A smaller p-value indicates stronger evidence against the null hypothesis and suggests that the observed differences between the groups are unlikely to have occurred due to random chance.\n",
    "\n",
    "Interpretation:\n",
    "Given that the p-value is 0.02, which is less than the commonly used significance level of 0.05, we can conclude the following:\n",
    "\n",
    "- The null hypothesis, which states that there are no significant differences between the group means, is rejected.\n",
    "- There is evidence to suggest that at least one group mean is different from the others.\n",
    "\n",
    "However, the specific differences between which groups are causing the significance cannot be determined solely from the ANOVA result. If you find a significant result in ANOVA, further post hoc tests (such as Tukey's HSD or Bonferroni correction) can be conducted to identify which specific group means are significantly different from each other.\n",
    "\n",
    "In summary, with an F-statistic of 5.23 and a p-value of 0.02, you can conclude that there are significant differences between the group means in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "883f9e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The p-value is less than the significance level.\n",
      "We reject the null hypothesis.\n",
      "There are significant differences between the group means.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Given F-statistic and p-value\n",
    "f_statistic = 5.23\n",
    "p_value = 0.02\n",
    "significance_level = 0.05\n",
    "\n",
    "# Check if the p-value is less than the significance level\n",
    "if p_value < significance_level:\n",
    "    print(\"The p-value is less than the significance level.\")\n",
    "    print(\"We reject the null hypothesis.\")\n",
    "    print(\"There are significant differences between the group means.\" + \"\\n\"*2)\n",
    "else:\n",
    "    print(\"The p-value is not less than the significance level.\")\n",
    "    print(\"We fail to reject the null hypothesis.\")\n",
    "    print(\"There is no significant evidence of differences between the group means.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a35d95",
   "metadata": {},
   "source": [
    "The interpretation of the F-statistic and p-value from the results of a one-way ANOVA involves assessing whether there are significant differences between the group means. Let's interpret these results:\n",
    "\n",
    "**F-Statistic (5.23):**\n",
    "The F-statistic is a measure of the ratio of the variability between group means to the variability within the groups. In your case, the F-statistic is 5.23.\n",
    "\n",
    "**p-value (0.02):**\n",
    "The p-value is a measure of the probability of observing the F-statistic or a more extreme value under the assumption that there are no real differences between the group means (null hypothesis is true). In your case, the p-value is 0.02.\n",
    "\n",
    "**Interpretation:**\n",
    "Given the F-statistic of 5.23 and the p-value of 0.02, here's how you can interpret the results:\n",
    "\n",
    "1. **p-value Interpretation:**\n",
    "   - The p-value (0.02) is less than the commonly used significance level (e.g., 0.05).\n",
    "   - This indicates that the probability of observing such a large F-statistic (or even larger) under the assumption of no differences between group means is relatively low.\n",
    "\n",
    "2. **Rejection of the Null Hypothesis:**\n",
    "   - With a p-value less than 0.05, you have evidence to reject the null hypothesis.\n",
    "   - The null hypothesis states that there are no significant differences between the group means.\n",
    "\n",
    "3. **Significant Differences:**\n",
    "   - The p-value being below 0.05 suggests that there are significant differences between the group means.\n",
    "   - The differences between at least some of the groups are likely to be real and not due to random chance.\n",
    "\n",
    "In summary, with an F-statistic of 5.23 and a p-value of 0.02, you can conclude that there are significant differences between the group means in your data. The results indicate that the factors being studied have an impact on the dependent variable, and the null hypothesis of no significant differences is rejected. Further analysis or post hoc tests can be conducted to explore the specific group differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc9868e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f492676",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 7 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a272e2",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA requires careful consideration, as missing data can introduce bias and affect the validity of the results. Here are a few approaches to handle missing data in a repeated measures ANOVA:\n",
    "\n",
    "1. **Listwise Deletion:**\n",
    "   Delete cases (subjects) with missing data on any variable used in the analysis. This approach is straightforward but can lead to reduced sample size and potentially biased results if missing data are not random.\n",
    "\n",
    "2. **Pairwise Deletion:**\n",
    "   Conduct the analysis using available data for each pairwise comparison. This approach retains more cases than listwise deletion but can lead to different sample sizes for different comparisons.\n",
    "\n",
    "3. **Imputation:**\n",
    "   Impute missing values using statistical techniques before conducting the analysis. Common imputation methods include mean imputation, median imputation, regression imputation, and multiple imputation. Imputation aims to preserve sample size while reducing the impact of missing data.\n",
    "\n",
    "4. **Mixed-Effects Models:**\n",
    "   Utilize mixed-effects models (e.g., linear mixed-effects models) that can handle missing data through estimation techniques like maximum likelihood. These models consider both within-subject variability and between-subject variability.\n",
    "\n",
    "5. **Multivariate Analysis of Variance (MANOVA):**\n",
    "   If missing data are multivariate and occur across different variables in the repeated measures design, consider using MANOVA, which allows you to analyze multiple dependent variables simultaneously.\n",
    "\n",
    "6. **Sensitivity Analysis:**\n",
    "   Conduct sensitivity analyses to examine the impact of missing data on results. Evaluate whether different handling methods (e.g., deletion, imputation) lead to different conclusions.\n",
    "\n",
    "7. **Missing Data Patterns:**\n",
    "   Examine the patterns of missing data. If the missingness is not random, it could indicate systematic issues that might require addressing before analysis.\n",
    "\n",
    "Before choosing a method, consider the nature and pattern of missing data, the potential biases introduced by each approach, and the assumptions of the analysis method you intend to use. It's advisable to consult with statisticians or researchers experienced in handling missing data to make informed decisions that minimize bias and ensure the validity of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e053afa",
   "metadata": {},
   "source": [
    "Using different methods to handle missing data can have various potential consequences, affecting the validity, reliability, and interpretation of your results. Here are some potential consequences associated with different methods:\n",
    "\n",
    "1. **Listwise Deletion:**\n",
    "   - Consequence: Reduced sample size, leading to loss of statistical power and potentially biased estimates, especially if the missing data are not missing completely at random.\n",
    "   - Impact: May result in less precise estimates, larger standard errors, and potentially misleading statistical significance.\n",
    "\n",
    "2. **Pairwise Deletion:**\n",
    "   - Consequence: Unequal sample sizes for different comparisons, leading to unequal statistical power across comparisons.\n",
    "   - Impact: Inaccurate and inconsistent parameter estimates, possibly affecting the overall interpretation of the analysis.\n",
    "\n",
    "3. **Imputation:**\n",
    "   - Consequence: Introducing imputed values that might not accurately represent the true values, particularly if the imputation method is not appropriate for the data.\n",
    "   - Impact: Can affect parameter estimates, standard errors, and significance tests. May lead to overestimation of statistical significance if imputed values are biased.\n",
    "\n",
    "4. **Mixed-Effects Models:**\n",
    "   - Consequence: Relying on complex statistical modeling techniques that assume specific patterns of missingness and adherence to model assumptions.\n",
    "   - Impact: Validity of the results depends on accurate modeling of the missing data mechanism. Can lead to biased estimates if the missing data mechanism is not well understood.\n",
    "\n",
    "5. **Multivariate Analysis of Variance (MANOVA):**\n",
    "   - Consequence: Accounting for missing data in multivariate analyses can be complex and may require careful consideration of relationships between variables.\n",
    "   - Impact: Failure to properly account for missing data patterns can lead to biased parameter estimates and unreliable conclusions.\n",
    "\n",
    "6. **Sensitivity Analysis:**\n",
    "   - Consequence: Time-consuming and may require expertise in both the analysis method and missing data techniques.\n",
    "   - Impact: Provides insights into the robustness of results across different handling methods, helping to gauge the potential impact of missing data on conclusions.\n",
    "\n",
    "7. **Missing Data Patterns:**\n",
    "   - Consequence: Ignoring systematic patterns of missing data can lead to biased results, as the missingness might be related to the variables under study.\n",
    "   - Impact: Ignoring missing data patterns can lead to invalid statistical inferences and incorrect conclusions.\n",
    "\n",
    "It's crucial to carefully consider the trade-offs of each approach, understand the assumptions underlying the chosen method, and transparently report the method used in your analysis. Selecting the appropriate method depends on factors such as the nature of the data, the extent and pattern of missingness, the research question, and the assumptions of the analysis method. Consulting with experts in missing data analysis and adhering to best practices will help minimize potential biases and ensure the reliability of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0a7b33",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 8 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f32319",
   "metadata": {},
   "source": [
    "\n",
    "Post-hoc tests are used to determine which specific group means are significantly different from each other after obtaining a significant result in an analysis of variance (ANOVA).ome common post-hoc tests that are commonly used : \n",
    "\n",
    "\n",
    "1. **Tukey's Honestly Significant Difference (Tukey's HSD):**\n",
    "   - **When to Use:** Tukey's HSD is suitable when you have conducted an ANOVA and want to perform all pairwise comparisons among the group means.\n",
    "   - **Why Use:** Tukey's HSD controls the experiment-wise error rate, which is the probability of making at least one type I error across all pairwise comparisons. It is one of the most conservative methods and ensures that the family-wise error rate is maintained at the desired level (often 0.05).\n",
    "\n",
    "2. **Bonferroni Correction:**\n",
    "   - **When to Use:** The Bonferroni correction is used when you want to control the overall family-wise error rate while performing multiple pairwise comparisons.\n",
    "   - **Why Use:** This method is more conservative than some other post-hoc tests. It divides the desired significance level (alpha) by the number of comparisons, ensuring that the probability of making at least one type I error remains low across all comparisons. It's suitable when you need a stringent control over the family-wise error rate.\n",
    "\n",
    "3. **Dunn's Test:**\n",
    "   - **When to Use:** Dunn's test is appropriate when the assumptions of parametric tests (like ANOVA) are violated, such as when the data is not normally distributed or homoscedastic.\n",
    "   - **Why Use:** This non-parametric test uses rank-based procedures, making it robust to violations of normality and equal variances assumptions. It's especially useful for data that may not meet the assumptions of parametric tests.\n",
    "\n",
    "4. **Scheffe's Method:**\n",
    "   - **When to Use:** Scheffe's method is used when you have specific hypotheses about the comparisons you want to make, rather than conducting all possible pairwise comparisons.\n",
    "   - **Why Use:** This test is more conservative and provides wider confidence intervals than other post-hoc tests, offering protection against making any false positives across comparisons. It's suitable when you have specific a priori hypotheses and want to maintain a certain level of control over the family-wise error rate.\n",
    "\n",
    "5. **Fisher's Least Significant Difference (LSD):**\n",
    "   - **When to Use:** Fisher's LSD is used when you want to perform pairwise comparisons among group means while controlling the experiment-wise error rate.\n",
    "   - **Why Use:** LSD is less conservative than some other methods, making it suitable when you have a limited number of comparisons to make. It provides a balance between controlling the family-wise error rate and maintaining statistical power.\n",
    "\n",
    "6. **Games-Howell Test:**\n",
    "   - **When to Use:** The Games-Howell test is used when group variances are not equal and you want to perform pairwise comparisons.\n",
    "   - **Why Use:** Unlike Tukey's HSD or Bonferroni correction, the Games-Howell test doesn't assume equal variances among groups. It provides an alternative for making pairwise comparisons when homogeneity of variances is not met.\n",
    "\n",
    "7. **Holm-Bonferroni Method:**\n",
    "   - **When to Use:** The Holm-Bonferroni method is used when you want to control the family-wise error rate in a more flexible manner, potentially providing more statistical power.\n",
    "   - **Why Use:** This method adjusts the significance level for each comparison in a stepwise manner, offering a balance between controlling the family-wise error rate and maintaining power. It's suitable when you have a mix of strong and weak hypotheses.\n",
    "\n",
    "8. **Benjamini-Hochberg Procedure (False Discovery Rate Control):**\n",
    "   - **When to Use:** The Benjamini-Hochberg procedure is used when you are conducting multiple comparisons and want to control the false discovery rate (expected proportion of false rejections) instead of the family-wise error rate.\n",
    "   - **Why Use:** This procedure is more lenient than methods controlling the family-wise error rate, allowing you to detect more potential differences among groups while still controlling the rate of false discoveries. It's useful when exploring a large number of comparisons.\n",
    "\n",
    "In summary, the choice of post-hoc test depends on the assumptions of your data, the research question, the level of control over error rates desired, and whether you have specific hypotheses to test. Each post-hoc test has its strengths and limitations, so it's important to select the most appropriate method based on the context of your study.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1cd3a",
   "metadata": {},
   "source": [
    "example where a post-hoc test would be necessary to understand the differences between group means after conducting an ANOVA.\n",
    "\n",
    "**Scenario: Comparing Exam Scores in Different Teaching Methods**\n",
    "\n",
    "Imagine a study conducted to compare the effectiveness of three different teaching methods (A, B, and C) on student exam scores in a mathematics course. The researchers want to determine if there are any statistically significant differences in the mean exam scores among the three teaching methods.\n",
    "\n",
    "They collect exam scores from three groups of students: Group A taught using traditional lectures, Group B taught using interactive online modules, and Group C taught using small-group discussions. The ANOVA is conducted to assess whether there are overall differences in mean exam scores among the teaching methods.\n",
    "\n",
    "**Results of ANOVA:**\n",
    "After conducting the ANOVA, the researchers obtain a significant p-value of 0.025. This indicates that there is at least one statistically significant difference in mean exam scores among the teaching methods.\n",
    "\n",
    "**The Need for Post-Hoc Tests:**\n",
    "While the ANOVA tells us that there is a significant difference somewhere among the groups, it doesn't specify which specific groups are different. This is where post-hoc tests come into play. The researchers need to perform post-hoc tests to pinpoint exactly which pairs of teaching methods have significantly different mean exam scores.\n",
    "\n",
    "**Post-Hoc Test Selection:**\n",
    "In this scenario, the researchers could choose to use Tukey's Honestly Significant Difference (Tukey's HSD) post-hoc test. Tukey's HSD will allow them to compare all possible pairs of teaching methods and identify where the significant differences lie.\n",
    "\n",
    "**Interpreting Post-Hoc Test Results:**\n",
    "Upon conducting Tukey's HSD, the researchers find that the mean exam scores for Group A (traditional lectures) and Group B (interactive online modules) are not significantly different from each other (p > 0.05). However, the mean exam scores for Group A and Group C (small-group discussions) are significantly different (p < 0.05), as are the scores for Group B and Group C.\n",
    "\n",
    "**Conclusion:**\n",
    "In this example, the post-hoc test helped the researchers understand that while there is a significant difference in mean exam scores among the teaching methods, the significant differences lie specifically between the small-group discussions method and the other two methods. This information is crucial for making informed decisions about the effectiveness of different teaching approaches.\n",
    "\n",
    "Overall, the use of post-hoc tests in this scenario allowed the researchers to gain a deeper understanding of the nuances within the significant result obtained from the ANOVA, helping them draw more accurate conclusions about the impact of teaching methods on exam scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8760929a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53fbd8fb",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 9 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54af6ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-Statistic: 158.44186806009148\n",
      "p-value: 2.0737282786500992e-37\n",
      "There is a significant difference in mean weight loss among the diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulated weight loss data for three diets: A, B, and C\n",
    "diet_a = np.array([2.5, 3.0, 4.0, 3.5, 2.0, 2.5, 3.5, 4.0, 3.0, 2.0,\n",
    "                   2.5, 3.0, 3.5, 2.5, 3.0, 4.5, 3.0, 2.0, 2.5, 3.0,\n",
    "                   3.5, 2.0, 3.0, 2.5, 3.5, 3.0, 4.0, 2.0, 3.5, 2.5,\n",
    "                   3.0, 2.5, 3.0, 2.0, 3.5, 3.0, 4.0, 3.5, 2.0, 2.5,\n",
    "                   3.0, 4.5, 3.0, 2.0, 2.5, 3.0, 3.5, 2.5, 3.0, 4.0])\n",
    "\n",
    "diet_b = np.array([1.5, 2.0, 2.5, 1.0, 1.5, 2.0, 1.5, 2.5, 3.0, 2.5,\n",
    "                   1.5, 2.0, 2.5, 1.0, 2.0, 2.5, 1.5, 2.0, 2.5, 1.0,\n",
    "                   1.5, 2.0, 1.5, 2.5, 3.0, 2.5, 1.5, 2.0, 2.5, 1.0,\n",
    "                   2.5, 1.5, 2.0, 2.5, 1.0, 1.5, 2.0, 1.5, 2.5, 3.0,\n",
    "                   2.5, 1.5, 2.0, 2.5, 1.0, 1.5, 2.0, 1.5, 2.5, 3.0])\n",
    "\n",
    "diet_c = np.array([0.5, 1.0, 1.5, 1.0, 0.5, 1.0, 1.5, 1.0, 0.5, 1.0,\n",
    "                   0.5, 1.0, 1.5, 1.0, 1.0, 1.5, 1.0, 0.5, 1.0, 1.5,\n",
    "                   1.0, 0.5, 1.0, 1.5, 1.0, 0.5, 1.0, 1.5, 1.0, 0.5,\n",
    "                   1.0, 1.5, 1.0, 0.5, 1.0, 1.5, 1.0, 0.5, 1.0, 1.5,\n",
    "                   1.0, 0.5, 1.0, 1.5, 1.0, 0.5, 1.0, 1.5, 1.0, 0.5])\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_a, diet_b, diet_c)\n",
    "\n",
    "# Print results\n",
    "print(\"F-Statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a significant difference in mean weight loss among the diets.\")\n",
    "else:\n",
    "    print(\"There is no significant difference in mean weight loss among the diets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb20aa",
   "metadata": {},
   "source": [
    "interpretation of the results of the one-way ANOVA :\n",
    "**Step 1: Understanding the Analysis**\n",
    "The one-way ANOVA is used to compare the mean weight loss of three different diets: A, B, and C. The analysis aims to determine whether there are any significant differences in the mean weight loss among these diets.\n",
    "\n",
    "**Step 2: F-Statistic and p-value**\n",
    "After performing the one-way ANOVA analysis using Python's `scipy.stats.f_oneway` function, we obtain the following results:\n",
    "- F-Statistic: The calculated value of the F-statistic.\n",
    "- p-value: The calculated p-value associated with the F-statistic.\n",
    "\n",
    "**Step 3: Interpretation of F-Statistic**\n",
    "The F-statistic is a measure of the variability between the group means relative to the variability within the groups. In our analysis, it quantifies whether the observed differences in mean weight loss among the diets are larger than what we would expect due to random chance.\n",
    "\n",
    "**Step 4: Interpretation of p-value**\n",
    "The p-value is a measure of the evidence against the null hypothesis. It tells us the probability of observing such extreme results (or more extreme) if the null hypothesis were true. In our case, the null hypothesis states that there are no significant differences in mean weight loss among the diets.\n",
    "\n",
    "**Step 5: Significance Level**\n",
    "Before interpreting the p-value, we need to establish a significance level (often denoted as alpha), which represents the threshold for considering a result statistically significant. A common choice is alpha = 0.05, meaning we are willing to accept a 5% chance of making a Type I error (rejecting a true null hypothesis).\n",
    "\n",
    "**Step 6: Interpreting the p-value**\n",
    "Comparing the calculated p-value to the significance level:\n",
    "- If p-value < alpha: This means that the probability of observing the observed differences (or more extreme) under the null hypothesis is less than 5%. In other words, the result is statistically significant.\n",
    "- If p-value >= alpha: This means that the probability of observing the observed differences (or more extreme) under the null hypothesis is greater than or equal to 5%. The result is not statistically significant.\n",
    "\n",
    "**Step 7: Making a Conclusion**\n",
    "Based on the p-value and the chosen significance level:\n",
    "- If p-value < alpha: We conclude that there is enough evidence to reject the null hypothesis. This implies that there are statistically significant differences in mean weight loss among the diets.\n",
    "- If p-value >= alpha: We do not have enough evidence to reject the null hypothesis. This implies that there are no statistically significant differences in mean weight loss among the diets.\n",
    "\n",
    "**Step 8: Conclusion**\n",
    "In our case, the calculated p-value is compared to the significance level (e.g., alpha = 0.05). If the p-value is less than 0.05, we can conclude that there are statistically significant differences in mean weight loss among the diets. If the p-value is greater than or equal to 0.05, we cannot conclude that the diets have significantly different mean weight losses.\n",
    "\n",
    "**Step 9: Further Analysis (Post-Hoc Tests)**\n",
    "If the ANOVA result is significant, it indicates that there are differences among the groups, but it doesn't tell us which specific pairs of groups are different. To determine which diets have significantly different mean weight losses, post-hoc tests (like Tukey's HSD or others) can be performed.\n",
    "\n",
    "Remember that interpreting statistical results requires a careful consideration of the context, research question, chosen significance level, and other relevant factors. It's essential to avoid drawing conclusions solely based on p-values without considering the broader context of the study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51ee68",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 10 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "057d218f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>df</th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>mean_sq</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Software</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.651000</td>\n",
       "      <td>0.825500</td>\n",
       "      <td>0.027720</td>\n",
       "      <td>0.972674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experience</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.160167</td>\n",
       "      <td>0.160167</td>\n",
       "      <td>0.005378</td>\n",
       "      <td>0.941808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Software:Experience</th>\n",
       "      <td>2.0</td>\n",
       "      <td>151.650333</td>\n",
       "      <td>75.825167</td>\n",
       "      <td>2.546223</td>\n",
       "      <td>0.087754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>54.0</td>\n",
       "      <td>1608.091000</td>\n",
       "      <td>29.779463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       df       sum_sq    mean_sq         F    PR(>F)\n",
       "Software              2.0     1.651000   0.825500  0.027720  0.972674\n",
       "Experience            1.0     0.160167   0.160167  0.005378  0.941808\n",
       "Software:Experience   2.0   151.650333  75.825167  2.546223  0.087754\n",
       "Residual             54.0  1608.091000  29.779463       NaN       NaN"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Simulated data for task completion time\n",
    "data = {\n",
    "    'Software': ['A', 'B', 'C'] * 20,\n",
    "    'Experience': ['Novice', 'Experienced'] * 30,\n",
    "    'Time':  [25.2, 24.8, 26.3, 32.1, 32.8, 31.9, 18.5, 17.9, 19.3,\n",
    "             29.7, 30.4, 30.1, 23.6, 22.9, 23.5, 29.2, 30.8, 29.5,\n",
    "             20.4, 21.1, 20.8, 31.6, 31.2, 32.5, 19.8, 18.2, 19.6,\n",
    "             26.7, 26.4, 27.9, 15.1, 16.8, 15.5, 24.7, 23.1, 24.5,\n",
    "             33.3, 34.5, 33.8, 21.9, 21.3, 20.6, 28.4, 29.7, 28.9,\n",
    "             22.6, 22.3, 23.9, 30.2, 31.6, 30.9 , 25.3, 32.3,14.9 ,27.7 ,16.3 ,26.7 , 19.9 ,29.2 , 30.9 ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit the ANOVA model\n",
    "model = ols('Time ~ Software * Experience', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Print ANOVA results\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee16619f",
   "metadata": {},
   "source": [
    " **interpretation of  the results of the two-way ANOVA step by step:**\n",
    "\n",
    "**Step 1: Understanding the Analysis**\n",
    "The two-way ANOVA is used to determine if there are any significant differences in the average time it takes to complete a task based on two factors: software programs (A, B, C) and employee experience levels (novice vs. experienced). The analysis aims to investigate whether there are main effects of software, main effects of experience, and an interaction effect between software and experience.\n",
    "\n",
    "**Step 2: ANOVA Table**\n",
    "After performing the two-way ANOVA analysis using the `anova_lm` function from `statsmodels`, we obtain an ANOVA table that breaks down the sources of variability in the data and provides F-statistics and p-values for different effects.\n",
    "\n",
    "**Step 3: Source of Variability**\n",
    "The ANOVA table has rows corresponding to different sources of variability:\n",
    "- `Software`\n",
    "- `Experience`\n",
    "- `Software:Experience` (Interaction)\n",
    "\n",
    "**Step 4: Main Effects**\n",
    "For each source of variability, the ANOVA table provides information about the main effects of that factor. This includes the F-statistic and the associated p-value. The main effects indicate whether there are significant differences in task completion time due to that factor alone.\n",
    "\n",
    "**Step 5: Interaction Effect**\n",
    "The interaction effect between software and experience represents whether the effect of one factor on task completion time depends on the level of the other factor. The ANOVA table provides the F-statistic and p-value for the interaction effect.\n",
    "\n",
    "**Step 6: Significance Level**\n",
    "Before interpreting the p-values, establish a significance level (e.g., alpha = 0.05), which represents the threshold for considering results statistically significant.\n",
    "\n",
    "**Step 7: Interpreting Main Effects**\n",
    "For each main effect:\n",
    "- If the p-value < alpha: Conclude that there is enough evidence to reject the null hypothesis for that effect. In other words, there is a significant difference in task completion time based on that factor.\n",
    "- If the p-value >= alpha: Conclude that there is not enough evidence to reject the null hypothesis for that effect. There is no significant difference in task completion time based on that factor.\n",
    "\n",
    "**Step 8: Interpreting Interaction Effect**\n",
    "For the interaction effect:\n",
    "- If the p-value < alpha: Conclude that there is enough evidence to reject the null hypothesis for the interaction effect. This suggests that the effect of software on task completion time depends on the level of experience (and vice versa).\n",
    "- If the p-value >= alpha: Conclude that there is not enough evidence to reject the null hypothesis for the interaction effect. The interaction effect is not significant.\n",
    "\n",
    "**Step 9: Conclusions**\n",
    "Based on the p-values and significance levels:\n",
    "- If any main effect's p-value < alpha: Conclude that there is a significant difference in task completion time based on that factor.\n",
    "- If the interaction effect's p-value < alpha: Conclude that there is a significant interaction between software and experience on task completion time.\n",
    "\n",
    "**Step 10: Next Steps**\n",
    "If significant effects are found, further analysis (post-hoc tests) might be conducted to determine which specific groups are different from each other. Post-hoc tests help identify which software pairs and experience level pairs contribute to the significant differences.\n",
    "\n",
    "Remember that interpreting ANOVA results involves careful consideration of the context, research question, chosen significance level, and other relevant factors. It's essential to understand the nature of the data and the factors being studied to draw meaningful conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab97e079",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 11 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39c1f4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Sample T-Test:\n",
      "T-Statistic: -4.754695943505282\n",
      "P-Value: 3.819135262679469e-06\n",
      "\n",
      "Post-Hoc (Tukey's HSD) Test:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05   \n",
      "========================================================\n",
      " group1    group2    meandiff p-adj lower  upper  reject\n",
      "--------------------------------------------------------\n",
      "Control Experimental   6.2615   0.0 3.6645 8.8585   True\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Simulated data for test scores\n",
    "np.random.seed(42)\n",
    "control_scores = np.random.normal(75, 10, size=100)\n",
    "experimental_scores = np.random.normal(80, 10, size=100)\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_scores, experimental_scores)\n",
    "\n",
    "# Print t-test results\n",
    "print(\"Two-Sample T-Test:\")\n",
    "print(\"T-Statistic:\", t_statistic)\n",
    "print(\"P-Value:\", p_value)\n",
    "\n",
    "# Perform post-hoc (Tukey's HSD) test\n",
    "data = pd.DataFrame({'Scores': np.concatenate((control_scores, experimental_scores)),\n",
    "                     'Group': ['Control'] * 100 + ['Experimental'] * 100})\n",
    "\n",
    "tukey_results = pairwise_tukeyhsd(data['Scores'], data['Group'])\n",
    "\n",
    "# Print post-hoc test results\n",
    "print(\"\\nPost-Hoc (Tukey's HSD) Test:\")\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192e4c1",
   "metadata": {},
   "source": [
    "interpret the results of the two-sample t-test and the post-hoc test step by step:\n",
    "\n",
    "**Step 1: Understanding the Analysis**\n",
    "The analysis aims to determine whether there are any significant differences in test scores between two groups of students: the control group (traditional teaching method) and the experimental group (new teaching method). The t-test will help us compare the means of the two groups. If the results are significant, the post-hoc test (Tukey's HSD) will identify which specific groups have significantly different means.\n",
    "\n",
    "**Step 2: Two-Sample T-Test**\n",
    "The two-sample t-test is used to compare the means of two independent groups. It calculates a t-statistic and a p-value to assess whether the observed differences in means are statistically significant.\n",
    "\n",
    "**Step 3: T-Statistic**\n",
    "The t-statistic measures the size of the difference between the means of the two groups relative to the variability within the groups. A larger absolute t-statistic indicates a larger difference between the group means.\n",
    "\n",
    "**Step 4: P-Value**\n",
    "The p-value associated with the t-test quantifies the evidence against the null hypothesis. It tells us the probability of observing such extreme differences (or more extreme) in means if the null hypothesis (no difference between groups) were true.\n",
    "\n",
    "**Step 5: Significance Level**\n",
    "Before interpreting the p-value, establish a significance level (e.g., alpha = 0.05), which represents the threshold for considering results statistically significant.\n",
    "\n",
    "**Step 6: Interpreting the P-Value**\n",
    "Compare the calculated p-value to the significance level:\n",
    "- If p-value < alpha: Conclude that there is enough evidence to reject the null hypothesis. In other words, there is a significant difference in test scores between the two groups.\n",
    "- If p-value >= alpha: Conclude that there is not enough evidence to reject the null hypothesis. There is no significant difference in test scores between the two groups.\n",
    "\n",
    "**Step 7: Post-Hoc (Tukey's HSD) Test**\n",
    "If the t-test is significant, indicating that there are differences in means, the post-hoc test (Tukey's HSD) is used to determine which specific groups have significantly different means.\n",
    "\n",
    "**Step 8: Post-Hoc Test Results**\n",
    "The post-hoc test results show pairwise comparisons between groups and whether the differences in means are statistically significant.\n",
    "\n",
    "**Step 9: Interpreting Post-Hoc Test Results**\n",
    "For each pair of groups:\n",
    "- If the p-value is less than the chosen significance level (e.g., alpha = 0.05), conclude that there is enough evidence to reject the null hypothesis. This indicates a significant difference in means between those groups.\n",
    "- If the p-value is greater than or equal to alpha, conclude that there is not enough evidence to reject the null hypothesis. There is no significant difference in means between those groups.\n",
    "\n",
    "**Step 10: Conclusion**\n",
    "Based on the t-test results:\n",
    "- If the t-test is significant (p-value < alpha), conclude that there is a significant difference in test scores between the control and experimental groups.\n",
    "\n",
    "Based on the post-hoc test results:\n",
    "- If there are significant pairwise differences, the post-hoc test will identify which specific groups differ significantly from each other.\n",
    "\n",
    "**Step 11: Consider the Context**\n",
    "Interpreting statistical results requires considering the context, research question, chosen significance level, and other relevant factors. It's important to understand the nature of the data and the factors being studied to draw meaningful conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714116e",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 12 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559ebf91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd7185e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated Measures ANOVA:\n",
      "                sum_sq    df         F    PR(>F)\n",
      "Store     2.224098e+05   2.0  3.085689  0.050734\n",
      "Residual  3.135386e+06  87.0       NaN       NaN\n",
      "\n",
      "Post-Hoc (Tukey's HSD) Test:\n",
      "  Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n",
      "=======================================================\n",
      "group1 group2 meandiff p-adj    lower    upper   reject\n",
      "-------------------------------------------------------\n",
      "     A      B 115.8201 0.0527   -1.0582 232.6984  False\n",
      "     A      C   90.464  0.161  -26.4143 207.3423  False\n",
      "     B      C -25.3561 0.8631 -142.2344  91.5222  False\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Simulated data for daily sales\n",
    "np.random.seed(42)\n",
    "store_a_sales = np.random.normal(1000, 200, size=30)\n",
    "store_b_sales = np.random.normal(1100, 180, size=30)\n",
    "store_c_sales = np.random.normal(1050, 220, size=30)\n",
    "\n",
    "# Combine sales data into a single DataFrame\n",
    "data = pd.DataFrame({'Store': ['A'] * 30 + ['B'] * 30 + ['C'] * 30,\n",
    "                     'Sales': np.concatenate((store_a_sales, store_b_sales, store_c_sales))})\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "model = sm.formula.ols('Sales ~ Store', data=data).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# Print ANOVA results\n",
    "print(\"Repeated Measures ANOVA:\")\n",
    "print(anova_table)\n",
    "\n",
    "# Perform post-hoc (Tukey's HSD) test\n",
    "tukey_results = pairwise_tukeyhsd(data['Sales'], data['Store'])\n",
    "\n",
    "# Print post-hoc test results\n",
    "print(\"\\nPost-Hoc (Tukey's HSD) Test:\")\n",
    "print(tukey_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126233ca",
   "metadata": {},
   "source": [
    "interpretation the results of the repeated measures ANOVA and the post-hoc test (Tukey's HSD) step by step:\n",
    "\n",
    "**Step 1: Understanding the Analysis**\n",
    "The analysis aims to determine whether there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. The repeated measures ANOVA will help us compare the means of sales across these three stores. If the results are significant, the post-hoc test will identify which specific stores have significantly different sales.\n",
    "\n",
    "**Step 2: Repeated Measures ANOVA**\n",
    "The repeated measures ANOVA is used to analyze the effects of a categorical independent variable (store) on a continuous dependent variable (sales). It accounts for the repeated measurements taken on the same subjects (days). The ANOVA table breaks down the sources of variability and provides F-statistics and p-values for different effects.\n",
    "\n",
    "**Step 3: Source of Variability**\n",
    "The ANOVA table has a row corresponding to the source of variability (`Store`). This represents the main effect of the store.\n",
    "\n",
    "**Step 4: Main Effect**\n",
    "For the source of variability (`Store`), the ANOVA table provides information about the main effect. This includes the F-statistic and the associated p-value. The main effect indicates whether there is a significant difference in sales based on the store.\n",
    "\n",
    "**Step 5: Significance Level**\n",
    "Before interpreting the p-value, establish a significance level (e.g., alpha = 0.05), which represents the threshold for considering results statistically significant.\n",
    "\n",
    "**Step 6: Interpreting Main Effect**\n",
    "Compare the calculated p-value to the significance level:\n",
    "- If p-value < alpha: Conclude that there is enough evidence to reject the null hypothesis. In other words, there is a significant difference in average daily sales between at least two stores.\n",
    "- If p-value >= alpha: Conclude that there is not enough evidence to reject the null hypothesis. There is no significant difference in average daily sales between the stores.\n",
    "\n",
    "**Step 7: Post-Hoc (Tukey's HSD) Test**\n",
    "If the ANOVA is significant, indicating that there are differences in means, the post-hoc test (Tukey's HSD) is used to determine which specific stores have significantly different sales.\n",
    "\n",
    "**Step 8: Post-Hoc Test Results**\n",
    "The post-hoc test results show pairwise comparisons between stores and whether the differences in sales are statistically significant.\n",
    "\n",
    "**Step 9: Interpreting Post-Hoc Test Results**\n",
    "For each pair of stores:\n",
    "- If the p-value is less than the chosen significance level (e.g., alpha = 0.05), conclude that there is enough evidence to reject the null hypothesis. This indicates a significant difference in average daily sales between those stores.\n",
    "- If the p-value is greater than or equal to alpha, conclude that there is not enough evidence to reject the null hypothesis. There is no significant difference in average daily sales between those stores.\n",
    "\n",
    "**Step 10: Conclusion**\n",
    "Based on the ANOVA results:\n",
    "- If the ANOVA is significant (p-value < alpha), conclude that there is a significant difference in average daily sales among at least some of the stores.\n",
    "\n",
    "Based on the post-hoc test results:\n",
    "- Identify which specific stores have significantly different sales.\n",
    "\n",
    "**Step 11: Consider the Context**\n",
    "Interpreting statistical results requires considering the context, research question, chosen significance level, and other relevant factors. It's essential to understand the nature of the data and the factors being studied to draw meaningful conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0326377d",
   "metadata": {},
   "source": [
    "<a id=\"14\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdea39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b331ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e660f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3928f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c970a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import scipy.stats as stats\n",
    "\n",
    "# # Generate sample data for three groups\n",
    "# group1 = np.random.normal(loc=20, scale=5, size=30)  # Group 1 with mean 20 and std deviation 5\n",
    "# group2 = np.random.normal(loc=25, scale=5, size=30)  # Group 2 with mean 25 and std deviation 5\n",
    "# group3 = np.random.normal(loc=30, scale=5, size=30)  # Group 3 with mean 30 and std deviation 5\n",
    "\n",
    "# # Perform one-way ANOVA\n",
    "# f_statistic, p_value = stats.f_oneway(group1, group2, group3)\n",
    "\n",
    "# # Significance level\n",
    "# significance_level = 0.05\n",
    "\n",
    "# # Interpret the results\n",
    "# print(\"F-Statistic:\", f_statistic)\n",
    "# print(\"P-Value:\", p_value)\n",
    "\n",
    "# if p_value < significance_level:\n",
    "#     print(\"The p-value is less than the significance level.\")\n",
    "#     print(\"We reject the null hypothesis.\")\n",
    "#     print(\"There are significant differences between the group means.\")\n",
    "# else:\n",
    "#     print(\"The p-value is not less than the significance level.\")\n",
    "#     print(\"We fail to reject the null hypothesis.\")\n",
    "#     print(\"There is no significant evidence of differences between the group means.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33edb2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d =  [25.2, 24.8, 26.3, 32.1, 32.8, 31.9, 18.5, 17.9, 19.3,\n",
    "             29.7, 30.4, 30.1, 23.6, 22.9, 23.5, 29.2, 30.8, 29.5,\n",
    "             20.4, 21.1, 20.8, 31.6, 31.2, 32.5, 19.8, 18.2, 19.6,\n",
    "             26.7, 26.4, 27.9, 15.1, 16.8, 15.5, 24.7, 23.1, 24.5,\n",
    "             33.3, 34.5, 33.8, 21.9, 21.3, 20.6, 28.4, 29.7, 28.9,\n",
    "             22.6, 22.3, 23.9, 30.2, 31.6, 30.9 , 25.3, 32.3,14.9 ,27.7 ,16.3 ,26.7 , 19.9 ,29.2 , 30.9 ]\n",
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d28f08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
