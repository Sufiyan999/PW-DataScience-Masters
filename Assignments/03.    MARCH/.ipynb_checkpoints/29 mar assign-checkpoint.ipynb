{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701c2f7e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73970347",
   "metadata": {},
   "source": [
    "Lasso Regression, short for \"Least Absolute Shrinkage and Selection Operator\" Regression, is a type of linear regression technique that incorporates L1 regularization. It is used for both prediction and feature selection in high-dimensional datasets. Lasso Regression adds a penalty term to the linear regression cost function, which encourages the model to shrink the coefficients of less important features to zero. This can lead to sparse models where only the most relevant features are retained.\n",
    "\n",
    "The key difference between Lasso Regression and ordinary linear regression is the addition of the regularization term, which is a multiple of the absolute values of the coefficients (\\( \\beta \\)) of the features:\n",
    "\n",
    "Cost Function = Sum of Squared Residuals + λ * ∑ ∣β∣  \\)\n",
    "\n",
    "Here:\n",
    "- Sum of Squared Residuals is the same as in ordinary linear regression.\n",
    "- \\( λ \\) is the regularization parameter that controls the strength of the penalty. Higher \\( λ \\) values result in more aggressive coefficient shrinkage.\n",
    "\n",
    "Advantages of Lasso Regression:\n",
    "1. **Feature Selection:** Lasso Regression inherently performs feature selection by pushing the coefficients of irrelevant features to zero. This makes it particularly useful when dealing with high-dimensional datasets.\n",
    "2. **Regularization:** Lasso effectively prevents overfitting by reducing the complexity of the model.\n",
    "3. **Interpretable Models:** Lasso can lead to simpler and more interpretable models with fewer non-zero coefficients.\n",
    "\n",
    "Limitations of Lasso Regression:\n",
    "1. **Selects One Variable from a Group:** In the presence of highly correlated variables, Lasso tends to arbitrarily choose one variable from the group and shrink the coefficients of the rest to zero. This can lead to instability in feature selection.\n",
    "2. **Bias in Coefficient Estimates:** Lasso can introduce bias in coefficient estimates when the true underlying relationship is not sparse. This might affect prediction accuracy.\n",
    "3. **Impact of Scaling:** Lasso is sensitive to the scaling of features. Features with larger magnitudes might dominate the regularization process.\n",
    "\n",
    "Applications of Lasso Regression:\n",
    "1. **Feature Selection:** Lasso is commonly used to select important features from high-dimensional datasets, such as gene expression data or text analysis.\n",
    "2. **Predictive Modeling:** Lasso can be used for prediction tasks when dealing with datasets containing many features.\n",
    "3. **Signal Processing:** Lasso has applications in signal denoising and compressive sensing.\n",
    "\n",
    "Lasso Regression is a valuable tool for addressing multicollinearity and improving model generalization in situations where there are many features and a subset of them are likely to be irrelevant or redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e912706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b18d336",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1ccc90",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression in feature selection is its inherent ability to automatically identify and select the most relevant features from a high-dimensional dataset. This advantage arises from the L1 regularization term in the Lasso regression's cost function.\n",
    "\n",
    "Lasso Regression introduces a penalty term that encourages the coefficients of less important features to become exactly zero. As a result, the model naturally performs feature selection by effectively removing irrelevant or redundant features from the model. This is particularly beneficial in situations where the dataset has more features than observations, which can lead to overfitting in traditional linear regression.\n",
    "\n",
    "Key benefits of using Lasso Regression for feature selection:\n",
    "\n",
    "1. **Automatic Selection:** Lasso Regression automates the process of feature selection without requiring manual intervention or domain expertise to determine which features are important.\n",
    "\n",
    "2. **Sparse Models:** Lasso produces models with sparse coefficients, where only a subset of features have non-zero coefficients. This leads to simpler and more interpretable models, reducing complexity and improving model generalization.\n",
    "\n",
    "3. **Reduces Overfitting:** By shrinking the coefficients of less important features to zero, Lasso reduces the risk of overfitting, especially in scenarios where the number of features is much larger than the number of observations.\n",
    "\n",
    "4. **Improved Generalization:** The selected features are those that are most relevant to the outcome variable, leading to improved model performance on unseen data.\n",
    "\n",
    "5. **Computational Efficiency:** Sparse models resulting from Lasso are computationally efficient to train and deploy, making them suitable for large-scale datasets.\n",
    "\n",
    "6. **Handles Correlation:** Lasso can handle correlated features by selecting one feature from a group of correlated features and setting the rest to zero. This helps mitigate multicollinearity issues.\n",
    "\n",
    "Overall, the main advantage of using Lasso Regression in feature selection is its ability to simplify models by automatically identifying and retaining only the most informative features, leading to better model performance and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd75ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "147f599a",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c137989",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting coefficients in a traditional linear regression model, but with an additional consideration due to the regularization effect of the Lasso penalty.\n",
    "\n",
    "In Lasso Regression, the L1 regularization term encourages some coefficients to be exactly zero, effectively performing feature selection. This means that the model may exclude certain features from the final equation. Here's how to interpret the coefficients:\n",
    "\n",
    "1. **Non-Zero Coefficients:** For features with non-zero coefficients, the interpretation remains the same as in linear regression. A one-unit change in the predictor variable results in a change of the coefficient's value in the response variable.\n",
    "\n",
    "2. **Zero Coefficients:** For features with zero coefficients, these features have been effectively excluded from the model due to Lasso's feature selection property. This suggests that these features are considered less relevant by the model in predicting the target variable.\n",
    "\n",
    "3. **Magnitude of Coefficients:** The magnitude of the non-zero coefficients indicates the strength of the relationship between a feature and the target variable. A larger coefficient magnitude implies a stronger influence on the target variable.\n",
    "\n",
    "4. **Sign of Coefficients:** The sign of the coefficients indicates the direction of the relationship between the predictor variable and the response variable. A positive coefficient implies a positive correlation, while a negative coefficient implies a negative correlation.\n",
    "\n",
    "5. **Magnitude of Regularization Parameter (Lambda):** The value of the regularization parameter (lambda) influences the extent of shrinkage of the coefficients. A larger lambda value leads to more aggressive shrinkage and more coefficients being driven to zero.\n",
    "\n",
    "It's important to note that interpreting Lasso coefficients requires considering both the individual coefficient values and the presence or absence of certain features in the model due to feature selection. Additionally, interpreting the coefficients within the context of the problem domain and other relevant factors is crucial for a meaningful interpretation.\n",
    "\n",
    "Visual aids such as coefficient plots or partial dependency plots can help in visualizing the impact of the coefficients on the model's predictions. Additionally, cross-validation techniques can be used to fine-tune the regularization parameter and obtain more stable coefficient estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac48f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f018f710",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e6bdf",
   "metadata": {},
   "source": [
    "In Lasso Regression, the primary tuning parameter that can be adjusted is the regularization parameter, often denoted as \"alpha\" (α). The alpha parameter controls the strength of the L1 regularization penalty applied to the model. The higher the value of alpha, the more aggressive the regularization, leading to more coefficients being driven towards zero.\n",
    "\n",
    "Here's how the tuning parameter alpha affects Lasso Regression:\n",
    "\n",
    "- **Small Alpha (Closer to 0):** With a small alpha, the regularization effect is weak, and Lasso Regression behaves more like ordinary linear regression. This can lead to overfitting, especially when dealing with a large number of features.\n",
    "\n",
    "- **Large Alpha (Closer to Infinity):** As alpha increases, the regularization effect becomes stronger. Many coefficients are driven towards zero, leading to sparsity in the model. This is particularly useful for feature selection, as it helps to identify and retain only the most important features.\n",
    "\n",
    "In practice, you can use techniques like cross-validation to find the optimal value of alpha that balances between model complexity and fitting the data well. Libraries like scikit-learn in Python provide functions like `LassoCV` that perform cross-validation to automatically select the optimal alpha value based on the data.\n",
    "\n",
    "Additionally, some implementations of Lasso Regression also allow you to adjust the maximum number of iterations and the convergence tolerance for the optimization algorithm used to solve the optimization problem. These parameters may need to be adjusted in specific cases where convergence issues are encountered.\n",
    "\n",
    "Keep in mind that the choice of alpha depends on the specific problem, the amount of data available, and the nature of the features. Cross-validation helps you choose an alpha value that generalizes well to unseen data and avoids overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6897e",
   "metadata": {},
   "source": [
    "The tuning parameters in Lasso Regression, specifically the regularization parameter (alpha), can significantly affect the model's performance and behavior. Let's explore how different values of alpha can impact the model:\n",
    "\n",
    "1. **Small Alpha (Closer to 0):**\n",
    "   - Effect: When alpha is very small, the regularization effect is weak, and the model behaves similarly to ordinary linear regression.\n",
    "   - Impact on Coefficients: Lasso tends to retain all or most of the features, as the penalty for shrinking coefficients to zero is minimal. This can lead to overfitting, especially if there are many irrelevant features in the dataset.\n",
    "   - Performance: The model might perform well on the training data but could generalize poorly to new, unseen data.\n",
    "\n",
    "2. **Medium Alpha:**\n",
    "   - Effect: As alpha increases, the regularization effect becomes stronger, encouraging the model to shrink some coefficients towards zero.\n",
    "   - Impact on Coefficients: Some less important features may have their coefficients reduced to zero, effectively performing feature selection. The model becomes simpler and more interpretable.\n",
    "   - Performance: The model's generalization performance on unseen data might improve compared to the case of very small alpha.\n",
    "\n",
    "3. **Large Alpha (Closer to Infinity):**\n",
    "   - Effect: A very large alpha enforces strong regularization, causing many coefficients to be driven exactly to zero.\n",
    "   - Impact on Coefficients: Most features will be excluded from the model, resulting in a sparse model with only a few significant features.\n",
    "   - Performance: The model becomes highly interpretable and avoids overfitting, but it might sacrifice some predictive power by excluding potentially useful features.\n",
    "\n",
    "Choosing the right value of alpha is crucial to achieving a balance between model complexity and generalization performance. Cross-validation is commonly used to search for the optimal alpha value that minimizes the prediction error on unseen data. Libraries like scikit-learn provide functions that perform cross-validation to automatically select the optimal alpha for Lasso Regression.\n",
    "\n",
    "It's important to note that the appropriate value of alpha depends on the specific dataset, the number of features, and the underlying relationships. Different data sets may require different levels of regularization to strike the right balance between bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad496456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94cccafc",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0327a2",
   "metadata": {},
   "source": [
    "Lasso Regression is primarily designed for linear regression problems, which means it assumes a linear relationship between the independent variables and the dependent variable. It is particularly effective when dealing with high-dimensional datasets and performing feature selection by shrinking less important coefficients to zero.\n",
    "\n",
    "However, Lasso Regression itself does not handle non-linear relationships between variables. If the relationship between variables is non-linear, using Lasso Regression directly may not yield accurate results. In such cases, you may consider using techniques like Polynomial Regression or non-linear regression models (e.g., Support Vector Regression, Decision Trees, Random Forests, etc.) that are specifically designed to capture non-linear relationships.\n",
    "\n",
    "If you want to apply Lasso-like regularization to non-linear regression problems, you can consider the following approaches:\n",
    "\n",
    "1. **Feature Engineering:** Transform your original features into higher-order terms or use other non-linear transformations to introduce non-linearities. Then, apply Lasso Regression to the transformed features.\n",
    "\n",
    "2. **Kernel Tricks:** Utilize kernel functions, such as the Radial Basis Function (RBF) kernel, to map the original features into a higher-dimensional space where they might exhibit linear relationships. You can then apply Lasso Regression in the transformed space.\n",
    "\n",
    "3. **Regularized Non-linear Models:** Some algorithms combine non-linear transformations with regularization techniques. For example, Elastic Net combines Lasso and Ridge regularization and can be extended to handle non-linearities to some extent.\n",
    "\n",
    "4. **Ensemble Methods:** Combine multiple non-linear models or transformations using ensemble techniques like Stacking or Gradient Boosting to capture non-linear relationships while also controlling for feature importance.\n",
    "\n",
    "In summary, while Lasso Regression itself is not designed for non-linear regression problems, you can use various strategies to incorporate Lasso-like regularization in combination with non-linear transformations to address non-linear relationships in your data. However, for complex non-linear relationships, it's often more appropriate to use models explicitly designed for non-linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34690d16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da2068ee",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 6 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0819639",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both regularization techniques used in linear regression to mitigate issues like multicollinearity and overfitting. While they share similarities, they have distinct differences in how they achieve regularization and handle the coefficients of the regression model.\n",
    "\n",
    "1. **Regularization Term:**\n",
    "   - Ridge Regression adds a regularization term to the linear regression cost function, which is the sum of squared magnitudes of the coefficients multiplied by a regularization parameter (lambda or alpha). The term added is \\(  α * ∑ β_i^2 \\), where \\( β_i \\) represents the coefficients.\n",
    "   - Lasso Regression also adds a regularization term to the cost function, but it uses the sum of the absolute values of the coefficients multiplied by the regularization parameter. The term added is \\( α * ∑ | β_i | \\).\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - Ridge Regression tends to shrink all coefficients towards zero while still keeping them non-zero, which means it doesn't perform explicit feature selection. It's useful when all features are potentially relevant and you want to reduce their impact but not exclude any completely.\n",
    "   - Lasso Regression, on the other hand, can drive some coefficients exactly to zero. This makes it a natural feature selection technique, effectively eliminating less important variables from the model. This makes Lasso useful when you suspect only a subset of the features are relevant.\n",
    "\n",
    "3. **Impact on Coefficients:**\n",
    "   - Ridge Regression's regularization term adds a penalty to the square of the coefficients, leading to a tendency for all coefficients to decrease, but not necessarily become zero.\n",
    "   - Lasso Regression's regularization term has a tendency to directly force some coefficients to become exactly zero, effectively removing those features from the model.\n",
    "\n",
    "4. **Solution Behavior:**\n",
    "   - Ridge Regression's solution is more stable when dealing with multicollinearity, as it doesn't force coefficients to be exactly zero, reducing the impact of collinear features.\n",
    "   - Lasso Regression is sensitive to multicollinearity and might arbitrarily choose one of the correlated features and zero out the others.\n",
    "\n",
    "In summary, while Ridge Regression and Lasso Regression are both regularization techniques, they differ in the types of regularization they apply, their impact on feature selection, and how they handle the coefficients of the model. The choice between them depends on the specific characteristics of the dataset, the nature of the problem, and the goals of the analysis. In some cases, a combination of both techniques (Elastic Net) is used to take advantage of their strengths while mitigating their weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d30989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95cc02ad",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 7 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73f7f62",
   "metadata": {},
   "source": [
    "Lasso Regression can handle multicollinearity in input features to some extent, but it approaches it differently compared to Ridge Regression.\n",
    "\n",
    "Multicollinearity occurs when two or more independent variables in a regression model are highly correlated with each other. This can lead to instability in coefficient estimates and difficulties in interpreting the impact of individual variables. Both Ridge and Lasso Regression are regularization techniques that can help mitigate the impact of multicollinearity, but they do so in different ways.\n",
    "\n",
    "In Lasso Regression:\n",
    "- Lasso has the ability to force some of the coefficients to become exactly zero. This means that it can effectively perform feature selection by excluding some of the correlated features from the model.\n",
    "- When multicollinearity is present, Lasso may choose one of the correlated features and zero out the coefficients of the others. This can help in reducing the impact of collinear features on the model.\n",
    "\n",
    "However, it's important to note that Lasso's sensitivity to multicollinearity can also lead to some challenges:\n",
    "- If multiple features are highly correlated and equally important, Lasso might arbitrarily choose one over the others, causing instability in the model.\n",
    "- When features are strongly correlated, the Lasso may not necessarily keep the most meaningful one; it could be affected by the specific dataset and noise.\n",
    "\n",
    "If multicollinearity is a major concern and feature selection is not the primary goal, Ridge Regression might be a more suitable choice. Ridge Regression can shrink the coefficients of correlated features, but it doesn't force them to be exactly zero. This helps maintain the stability of the model while reducing the multicollinearity effect.\n",
    "\n",
    "In summary, Lasso Regression can handle multicollinearity to some extent by performing feature selection and zeroing out some coefficients. However, the choice between Ridge and Lasso should be based on the specific characteristics of the dataset, the nature of the problem, and the goals of the analysis. Regularization techniques can be effective in dealing with multicollinearity, but they should be used judiciously based on the context of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5b99f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccb6c588",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 8 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b8d6b",
   "metadata": {},
   "source": [
    "Choosing the optimal value of the regularization parameter (lambda) in Lasso Regression is crucial for achieving the best balance between bias and variance. The choice of lambda determines the amount of regularization applied to the model, and finding the right value involves a trade-off between model complexity and fitting the data. There are a few common methods to select the optimal lambda value:\n",
    "\n",
    "1. Cross-Validation:\n",
    "   Cross-validation is a widely used technique to select the best lambda value. In k-fold cross-validation, the dataset is divided into k subsets. The model is trained on k-1 subsets and validated on the remaining subset for each iteration. Different lambda values are tested, and the one that results in the best cross-validation performance (e.g., lowest mean squared error or highest R-squared) is selected.\n",
    "\n",
    "2. Lasso Path and Coordinate Descent:\n",
    "   The Lasso Path is a plot of the magnitude of the coefficients against different values of lambda. It shows how each coefficient changes as lambda varies. Coordinate Descent is an iterative optimization algorithm used to fit Lasso models for various values of lambda. By analyzing the Lasso Path or monitoring the behavior of the coefficients during Coordinate Descent, you can identify the point where some coefficients become exactly zero (feature selection) and choose a lambda value accordingly.\n",
    "\n",
    "3. Information Criterion:\n",
    "   Information criteria such as AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) can help in choosing the optimal lambda value. These criteria balance the goodness of fit with the complexity of the model. Lower AIC or BIC values indicate a better model fit. You can iterate over different lambda values and select the one with the lowest AIC or BIC.\n",
    "\n",
    "4. Regularization Path Plot:\n",
    "   Similar to the Lasso Path, the Regularization Path Plot shows the behavior of coefficients as lambda changes. This plot can help you visualize the effect of regularization on each coefficient and assist in selecting an appropriate lambda value.\n",
    "\n",
    "5. Cross-Validation Grid Search:\n",
    "   A more exhaustive approach involves performing a grid search over a predefined range of lambda values and using cross-validation to evaluate model performance for each lambda. This helps in finding the lambda value that results in the best generalization performance.\n",
    "\n",
    "The choice of method depends on the size of the dataset, computational resources, and the specific goals of your analysis. Cross-validation is a robust approach that is commonly used, but exploring the Lasso Path and other techniques can provide additional insights into how the model responds to different levels of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36761cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2836a41",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9831a406",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f0d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cf1fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025c9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010f21a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
