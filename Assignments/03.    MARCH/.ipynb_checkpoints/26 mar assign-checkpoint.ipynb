{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8340689",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaf8a55",
   "metadata": {},
   "source": [
    "Simple linear regression and multiple linear regression are both techniques used in predictive modeling, particularly in the field of statistics and machine learning. They are used to establish relationships between one or more independent variables and a dependent variable. However, they differ in terms of the number of independent variables they consider and the complexity of the relationships they model.\n",
    "\n",
    "1. **Simple Linear Regression:**\n",
    "   - In simple linear regression, there is only one independent variable (predictor variable) that is used to predict a single dependent variable (response variable).\n",
    "   - The relationship between the independent variable and the dependent variable is assumed to be linear. The goal is to find the best-fit line (regression line) that minimizes the sum of squared differences between the observed data points and the predicted values on the line.\n",
    "   - The equation of the regression line is: `y = b0 + b1 * x`, where `y` is the dependent variable, `x` is the independent variable, `b0` is the intercept, and `b1` is the slope of the line.\n",
    "\n",
    "2. **Multiple Linear Regression:**\n",
    "   - In multiple linear regression, there are multiple independent variables that are used to predict a single dependent variable.\n",
    "   - It considers a scenario where the relationship between the dependent variable and the independent variables is linear, but it accounts for the influence of more than one predictor variable.\n",
    "   - The equation of the regression line becomes: `y = b0 + b1 * x1 + b2 * x2 + ... + bn * xn`, where `x1`, `x2`, ..., `xn` are the independent variables, and `b0`, `b1`, `b2`, ..., `bn` are the coefficients that need to be estimated.\n",
    "\n",
    "In summary, the key differences between simple linear regression and multiple linear regression are the number of independent variables and the complexity of the model. Simple linear regression deals with a single independent variable, while multiple linear regression deals with multiple independent variables. The goal of both techniques is to find the best-fitting model that can make accurate predictions based on the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae86785",
   "metadata": {},
   "source": [
    "\n",
    "**Example 1: Simple Linear Regression**\n",
    "Suppose we want to predict a student's final exam score (`y`) based on the number of hours they studied (`x`). We have the following data:\n",
    "\n",
    "| Hours Studied (x) | Final Exam Score (y) |\n",
    "|-------------------|----------------------|\n",
    "| 3                 | 65                   |\n",
    "| 5                 | 75                   |\n",
    "| 7                 | 85                   |\n",
    "| 4                 | 70                   |\n",
    "| 6                 | 80                   |\n",
    "\n",
    "We want to fit a simple linear regression model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985e37d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data\n",
    "hours_studied = np.array([3, 5, 7, 4, 6]).reshape(-1, 1)\n",
    "exam_scores = np.array([65, 75, 83, 70, 80])\n",
    "\n",
    "# Simple Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(hours_studied, exam_scores)\n",
    "\n",
    "# Prediction\n",
    "predicted_scores = regressor.predict(hours_studied)\n",
    "\n",
    "# Plotting\n",
    "plt.scatter(hours_studied, exam_scores, color='blue', label='Actual Scores')\n",
    "plt.plot(hours_studied, predicted_scores, color='red', label='Regression Line')\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Final Exam Score')\n",
    "plt.title('Simple Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c5fc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9924953095684802"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "regressor.score(hours_studied, exam_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da9797",
   "metadata": {},
   "source": [
    "\n",
    "**Example 2: Multiple Linear Regression**\n",
    "Now let's consider a scenario with multiple independent variables. Suppose we want to predict a car's gas mileage (`mpg`) based on its horsepower (`hp`) and weight (`wt`). We have the following data:\n",
    "\n",
    "| Horsepower (hp) | Weight (wt) | Gas Mileage (mpg) |\n",
    "|-----------------|-------------|-------------------|\n",
    "| 100             | 2100        | 30                |\n",
    "| 150             | 2300        | 25                |\n",
    "| 120             | 2200        | 28                |\n",
    "| 170             | 2500        | 20                |\n",
    "| 200             | 2700        | 18                |\n",
    "\n",
    "We want to fit a multiple linear regression model to this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a42abbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient (Horsepower): -0.05555555555555557\n",
      "Coefficient (Weight): -0.011944444444444447\n",
      "Intercept: 60.611111111111114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Data\n",
    "horsepower = np.array([100, 150, 120, 170, 200]).reshape(-1, 1)\n",
    "weight = np.array([2100, 2300, 2200, 2500, 2700]).reshape(-1, 1)\n",
    "gas_mileage = np.array([30, 25, 28, 20, 18])\n",
    "\n",
    "# Multiple Linear Regression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(np.hstack((horsepower, weight)), gas_mileage)\n",
    "\n",
    "# Coefficients and intercept\n",
    "coeff_hp = regressor.coef_[0]\n",
    "coeff_wt = regressor.coef_[1]\n",
    "intercept = regressor.intercept_\n",
    "\n",
    "print(\"Coefficient (Horsepower):\", coeff_hp)\n",
    "print(\"Coefficient (Weight):\", coeff_wt)\n",
    "print(\"Intercept:\", intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0c8532",
   "metadata": {},
   "source": [
    "In this example, we're using `horsepower` and `weight` as the independent variables to predict `gas_mileage` using multiple linear regression. The coefficients represent the impact of each variable on the gas mileage, and the intercept represents the gas mileage when both variables are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e78d12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44f2e940",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33438d9e",
   "metadata": {},
   "source": [
    "Linear regression makes several key assumptions that need to be satisfied for the model to provide reliable results. These assumptions are important to ensure the validity and interpretability of the regression analysis. Here are the main assumptions of linear regression:\n",
    "\n",
    "1. **Linearity:** The relationship between the independent variables and the dependent variable is linear. This means that changes in the independent variables lead to a proportional change in the dependent variable.\n",
    "\n",
    "2. **Independence of Residuals:** The residuals (the differences between actual and predicted values) should be independent of each other. This assumption ensures that there is no pattern or correlation in the residuals, indicating that the model is capturing all the relevant information.\n",
    "\n",
    "3. **Homoscedasticity (Equal Variance):** The residuals should have constant variance across all levels of the independent variables. In other words, the spread of residuals should be consistent along the range of predicted values.\n",
    "\n",
    "4. **Normality of Residuals:** The residuals should follow a normal distribution. This assumption is important for conducting hypothesis tests and constructing confidence intervals.\n",
    "\n",
    "5. **No Multicollinearity:** There should be little to no multicollinearity among the independent variables. Multicollinearity occurs when independent variables are highly correlated with each other, making it difficult to isolate their individual effects on the dependent variable.\n",
    "\n",
    "6. **No Endogeneity:** There should be no endogeneity, which means that the error term should not be correlated with any of the independent variables. Endogeneity can lead to biased coefficient estimates.\n",
    "\n",
    "7. **No Autocorrelation:** The residuals should not exhibit autocorrelation, which means that the errors are not correlated with each other. This assumption is particularly important when dealing with time-series data.\n",
    "\n",
    "Violations of these assumptions can lead to biased or inefficient parameter estimates, incorrect hypothesis testing, and inaccurate predictions. It's essential to assess these assumptions when building a linear regression model and take appropriate corrective actions if any assumptions are violated. Various diagnostic tools and techniques, such as residual plots, normality tests, and variance inflation factors (VIFs), can help assess these assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563d5239",
   "metadata": {},
   "source": [
    "We can use various diagnostic techniques and visualizations to check whether the assumptions of linear regression hold in a given dataset. Here's how you can assess each assumption:\n",
    "\n",
    "1. **Linearity:** Plot a scatter plot of the dependent variable against each independent variable. If the points roughly follow a linear pattern, the linearity assumption is likely satisfied. You can also use residual plots to check for linearity.\n",
    "\n",
    "2. **Independence of Residuals:** Create a plot of residuals against the predicted values. If the residuals exhibit no clear pattern and are randomly scattered around zero, the assumption is met.\n",
    "\n",
    "3. **Homoscedasticity (Equal Variance):** Plot the residuals against the predicted values. If the spread of residuals remains roughly constant as the predicted values change, the assumption is likely satisfied. You can also use statistical tests like the Breusch-Pagan or White tests to formally assess heteroscedasticity.\n",
    "\n",
    "4. **Normality of Residuals:** Create a histogram or a Q-Q plot of the residuals. If the residuals roughly follow a normal distribution, the assumption is met. Additionally, you can perform normality tests such as the Shapiro-Wilk test or Anderson-Darling test.\n",
    "\n",
    "5. **No Multicollinearity:** Calculate the variance inflation factor (VIF) for each independent variable. VIF values above 10 indicate the presence of multicollinearity. You can also examine correlation matrices to identify high correlations between independent variables.\n",
    "\n",
    "6. **No Endogeneity:** This assumption can be challenging to test directly. Domain knowledge and theory can guide you in selecting appropriate variables and techniques to address potential endogeneity issues.\n",
    "\n",
    "7. **No Autocorrelation:** For time-series data, plot the residuals against time to identify any patterns. You can also use statistical tests like the Durbin-Watson test or the Ljung-Box test to assess autocorrelation.\n",
    "\n",
    "Additionally, residual plots, Cook's distance, leverage plots, and influence plots can help you identify influential data points that might be affecting the assumptions.\n",
    "\n",
    "It's important to note that no dataset will perfectly meet all assumptions. Some level of deviation is often acceptable, but if significant violations are detected, you might need to consider transformations, data cleaning, or using more advanced modeling techniques to address the issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ec5499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0c9a000",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08dda3",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept have specific interpretations:\n",
    "\n",
    "1. **Intercept (β₀):** The intercept represents the predicted value of the dependent variable (Y) when all independent variables (X) are equal to zero. However, this interpretation might not always make sense in the context of your data. It's important to consider whether a zero value for all independent variables is plausible and meaningful.\n",
    "\n",
    "2. **Slope (β₁, β₂, ...):** The slope represents the change in the dependent variable (Y) associated with a one-unit change in the corresponding independent variable (X), while holding all other variables constant. In other words, it quantifies the impact of a change in the independent variable on the dependent variable.\n",
    "\n",
    "Here's an example to illustrate:\n",
    "\n",
    "Suppose you have a linear regression model with one independent variable (X) predicting a dependent variable (Y):\n",
    "\n",
    "Y = β₀ + β₁ * X\n",
    "\n",
    "- Intercept (β₀): If the intercept is 10, it means that when X = 0, the predicted value of Y is 10. However, this might not have a practical interpretation depending on the context of the data.\n",
    "\n",
    "- Slope (β₁): If the slope is 2, it means that for every one-unit increase in X, the predicted value of Y increases by 2 units, assuming all other factors remain constant.\n",
    "\n",
    "For instance, if you're predicting the price of a car based on its mileage (X), a slope of 2 means that, on average, for every one-mile increase in mileage, the price of the car is expected to increase by $2, while keeping other factors (like make, model, year, etc.) constant.\n",
    "\n",
    "It's important to note that the interpretation of the slope can change based on the unit of measurement of the variables. Make sure to consider the units and the context of the problem to provide meaningful interpretations of the slope and intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def09481",
   "metadata": {},
   "source": [
    "consider a real-world scenario where we want to predict the sales (Y) of a product based on its advertising expenditure (X) in a linear regression model. Here, the intercept and slope will have specific interpretations.\n",
    "\n",
    "Suppose we have the following linear regression equation:\n",
    "\n",
    "Sales = β₀ + β₁ * Advertising\n",
    "\n",
    "- Intercept (β₀): If the intercept is 50 units, it means that when the advertising expenditure is zero (X = 0), the predicted sales is 50 units. This might represent the baseline sales that occur even without any advertising.\n",
    "\n",
    "- Slope (β₁): If the slope is 10, it means that for every one-unit increase in advertising expenditure (X), the predicted sales increase by 10 units, assuming all other factors remain constant. This implies that advertising has a positive impact on sales, and for every increase in advertising, we expect a proportional increase in sales.\n",
    "\n",
    "For instance, if the advertising expenditure for a product is $1000 (X = 1000), using the given equation:\n",
    "\n",
    "Sales = 50 + 10 * 1000 = 1050 units\n",
    "\n",
    "This means that with an advertising expenditure of $1000, we predict the sales to be 1050 units.\n",
    "\n",
    "Similarly, if the advertising expenditure increases to $2000 (X = 2000), the predicted sales would be:\n",
    "\n",
    "Sales = 50 + 10 * 2000 = 2050 units\n",
    "\n",
    "In this example, the slope of 10 indicates that each additional dollar spent on advertising is associated with an increase of 10 units in sales.\n",
    "\n",
    "Remember that interpretations should be made within the context of the problem and the units of measurement for the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b16d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept (β₀): 51.11075538723614\n",
      "Slope (β₁): 9.96846751070102\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10  # Advertising expenditure\n",
    "y = 50 + 10 * X + np.random.randn(100, 1) * 5  # Simulated sales with noise\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print intercept and slope\n",
    "intercept = model.intercept_[0]\n",
    "slope = model.coef_[0][0]\n",
    "print(f\"Intercept (β₀): {intercept}\")\n",
    "print(f\"Slope (β₁): {slope}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d30596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ff357ff",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b6c18",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm used to minimize the cost or loss function of a machine learning model. It is commonly used in training machine learning algorithms, especially for models like linear regression, neural networks, and support vector machines. The goal of gradient descent is to find the optimal values of the model's parameters that minimize the difference between predicted and actual outcomes.\n",
    "\n",
    "Here's a step-by-step explanation of how gradient descent works:\n",
    "\n",
    "1. **Initialize Parameters**: Start with initial parameter values. These parameters define the model's behavior and need to be optimized to minimize the cost function.\n",
    "\n",
    "2. **Calculate Loss**: Evaluate the cost or loss function using the current parameter values. The loss function quantifies how well the model's predictions match the actual data.\n",
    "\n",
    "3. **Calculate Gradients**: Compute the gradients of the loss function with respect to each parameter. The gradient represents the direction and magnitude of the steepest increase of the function at a specific point.\n",
    "\n",
    "4. **Update Parameters**: Adjust the parameters by moving in the opposite direction of the gradients. This step involves subtracting a fraction of the gradient from each parameter. The fraction is determined by the learning rate, which controls the step size.\n",
    "\n",
    "5. **Repeat**: Steps 2 to 4 are repeated iteratively until a stopping criterion is met, such as reaching a certain number of iterations or achieving a satisfactory level of convergence.\n",
    "\n",
    "Gradient descent comes in different variants, including:\n",
    "\n",
    "- **Batch Gradient Descent**: Computes gradients using the entire dataset. It can be slow for large datasets but provides a stable convergence path.\n",
    "  \n",
    "- **Stochastic Gradient Descent (SGD)**: Computes gradients using only one randomly chosen data point at each iteration. It is faster but more noisy compared to batch gradient descent.\n",
    "\n",
    "- **Mini-Batch Gradient Descent**: A compromise between batch and stochastic gradient descent, where gradients are computed using a small subset (mini-batch) of the dataset.\n",
    "\n",
    "- **Adaptive Methods (e.g., Adam, RMSProp)**: These algorithms dynamically adjust the learning rate based on historical gradient information to improve convergence.\n",
    "\n",
    "The gradient descent process aims to iteratively update the model parameters, gradually moving them toward the values that minimize the loss function. As the optimization progresses, the model's predictions improve, and the loss function decreases. The learning rate and the choice of optimization algorithm are crucial factors that can influence the convergence speed and stability of gradient descent.\n",
    "\n",
    "In summary, gradient descent is a fundamental optimization technique used to train machine learning models by iteratively adjusting their parameters to minimize the cost or loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61237c2a",
   "metadata": {},
   "source": [
    "Gradient descent is a fundamental optimization technique used in machine learning to train models and update their parameters in order to minimize the loss function. Here's how gradient descent is used in various machine learning algorithms:\n",
    "\n",
    "1. **Linear Regression**: In linear regression, gradient descent adjusts the coefficients (slope and intercept) of the linear equation to find the line that best fits the data. The goal is to minimize the mean squared error (MSE) between predicted and actual values.\n",
    "\n",
    "2. **Logistic Regression**: In logistic regression, gradient descent is used to find the optimal weights for the features, allowing the algorithm to classify data points into two or more classes.\n",
    "\n",
    "3. **Neural Networks**: Gradient descent is a fundamental component of training neural networks. Backpropagation, which calculates the gradients of the loss function with respect to each parameter, uses gradient descent to update weights and biases in each layer.\n",
    "\n",
    "4. **Support Vector Machines (SVM)**: In SVM, gradient descent helps determine the optimal hyperplane that maximizes the margin between classes, while also minimizing classification errors.\n",
    "\n",
    "5. **Principal Component Analysis (PCA)**: Gradient descent is used to find the principal components that capture the most variance in a dataset while reducing its dimensionality.\n",
    "\n",
    "6. **Natural Language Processing (NLP)**: In NLP tasks, such as training word embeddings or optimizing language models, gradient descent is used to update word vectors or neural network parameters.\n",
    "\n",
    "7. **Recommender Systems**: Gradient descent is employed to learn user and item embeddings in collaborative filtering-based recommender systems.\n",
    "\n",
    "8. **Optimization Algorithms**: Many optimization algorithms, such as AdaGrad, RMSProp, and Adam, build on gradient descent by adapting learning rates or incorporating momentum to improve convergence efficiency.\n",
    "\n",
    "The general process of using gradient descent in machine learning involves the following steps:\n",
    "\n",
    "1. Choose a suitable loss function that quantifies the model's performance.\n",
    "\n",
    "2. Initialize the model's parameters (weights and biases) randomly or using specific strategies.\n",
    "\n",
    "3. Compute the gradients of the loss function with respect to each parameter using backpropagation or other methods.\n",
    "\n",
    "4. Update the parameters by subtracting a fraction of the gradients, scaled by a learning rate.\n",
    "\n",
    "5. Repeat the update process iteratively until convergence, which is typically determined by a predefined number of iterations or a threshold for the change in parameters.\n",
    "\n",
    "6. Evaluate the trained model on validation or test data to ensure it generalizes well to unseen examples.\n",
    "\n",
    "Gradient descent helps models learn from data and adapt their parameters to fit patterns in the data, leading to improved performance on new data. The choice of learning rate, optimization algorithm, and other hyperparameters can influence the speed and stability of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6de7e7",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170f16d0",
   "metadata": {},
   "source": [
    "Multiple linear regression is an extension of simple linear regression that allows for the prediction of a target variable using multiple predictor variables. In multiple linear regression, the goal is to find a linear relationship between the predictor variables and the target variable by fitting a hyperplane to the data. The model equation for multiple linear regression can be represented as:\n",
    "\n",
    "\\[ y = β_0 + β₁ * x_1 + β₂ * x_2 + ... + βp * x_p + ϵ \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the target variable (dependent variable) to be predicted.\n",
    "- \\( x_1, x_2, ..., x_p \\) are the predictor variables (independent variables).\n",
    "- \\( β0 \\) is the intercept or bias term.\n",
    "- \\( β₁, β₂, ...  \\) are the coefficients corresponding to the predictor variables.\n",
    "- \\( p \\) is the number of predictor variables.\n",
    "- \\( ϵ \\) is the error term, representing the difference between the actual and predicted values.\n",
    "\n",
    "The primary steps in building a multiple linear regression model are as follows:\n",
    "\n",
    "1. **Data Collection**: Collect a dataset containing the target variable and multiple predictor variables.\n",
    "\n",
    "2. **Data Preprocessing**: Clean and preprocess the data, handling missing values, outliers, and categorical variables.\n",
    "\n",
    "3. **Feature Selection/Engineering**: Select relevant predictor variables and engineer new features if needed.\n",
    "\n",
    "4. **Model Training**: Use the dataset to estimate the coefficients \\( β0, β₁, β₂, ... ,  βp \\) that best fit the data. This is often done using optimization techniques like least squares.\n",
    "\n",
    "5. **Model Evaluation**: Evaluate the model's performance using various metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), R-squared (coefficient of determination), and others.\n",
    "\n",
    "6. **Model Interpretation**: Interpret the coefficients \\( β0, β₁, β₂, ... ,  βp \\) to understand the strength and direction of the relationships between predictor variables and the target variable.\n",
    "\n",
    "7. **Prediction**: Use the trained model to make predictions on new, unseen data.\n",
    "\n",
    "Multiple linear regression assumes the following assumptions:\n",
    "- Linearity: The relationship between the predictor variables and the target variable is linear.\n",
    "- Independence: Residuals (errors) are independent of each other.\n",
    "- Homoscedasticity: Residuals have constant variance across all levels of the predictor variables.\n",
    "- Normality: Residuals are normally distributed.\n",
    "- No Multicollinearity: Predictor variables are not highly correlated with each other.\n",
    "\n",
    "Multiple linear regression is widely used in various fields, such as economics, finance, social sciences, and machine learning, to model relationships between multiple variables and make predictions or understand the impact of different predictors on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ae1aa",
   "metadata": {},
   "source": [
    "Simple linear regression and multiple linear regression differ primarily in the number of predictor variables used to predict the target variable. Here's a comparison of the two:\n",
    "\n",
    "1. **Number of Predictor Variables:**\n",
    "   - **Simple Linear Regression:** In simple linear regression, there is only one predictor variable used to predict the target variable.\n",
    "   - **Multiple Linear Regression:** In multiple linear regression, there are two or more predictor variables used to predict the target variable.\n",
    "\n",
    "2. **Model Equation:**\n",
    "   - **Simple Linear Regression:** The model equation for simple linear regression is \\( y = β0 + β₁ * x + \\epsilon \\), where \\( x \\) is the predictor variable.\n",
    "   - **Multiple Linear Regression:** The model equation for multiple linear regression includes multiple predictor variables: \\[ y = β_0 + β₁ * x_1 + β₂ * x_2 + ... + βp * x_p + ϵ \\].\n",
    "\n",
    "3. **Complexity:**\n",
    "   - **Simple Linear Regression:** The model is simpler since it involves only one predictor variable.\n",
    "   - **Multiple Linear Regression:** The model is more complex due to the involvement of multiple predictor variables.\n",
    "\n",
    "4. **Interpretation:**\n",
    "   - **Simple Linear Regression:** The slope coefficient (\\( β₁ \\)) represents the change in the target variable for a one-unit change in the predictor variable.\n",
    "   - **Multiple Linear Regression:** The interpretation of coefficients becomes more intricate since each coefficient (\\( β₁, β₂, ...  \\)) represents the change in the target variable while holding other predictors constant.\n",
    "\n",
    "5. **Use Cases:**\n",
    "   - **Simple Linear Regression:** Used when there is a single predictor variable and a linear relationship between it and the target variable.\n",
    "   - **Multiple Linear Regression:** Used when there are multiple predictor variables, and you want to model the joint effect of these variables on the target variable.\n",
    "\n",
    "6. **Assumptions:**\n",
    "   - The assumptions of linearity, independence, homoscedasticity, normality, and no multicollinearity apply to both types of regression.\n",
    "\n",
    "7. **Visualizations:**\n",
    "   - **Simple Linear Regression:** Often visualized with a scatter plot and a fitted regression line.\n",
    "   - **Multiple Linear Regression:** More complex to visualize due to the multiple dimensions, but scatter plots and residual plots can still provide insights.\n",
    "\n",
    "Both types of regression models aim to model relationships between variables and make predictions. The choice between simple and multiple linear regression depends on the complexity of the relationships among variables and the research question you're trying to answer. If there's only one predictor variable with a clear linear relationship, simple linear regression might suffice. However, if multiple factors influence the target variable, multiple linear regression is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47540cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88e96030",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 6 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f36c0",
   "metadata": {},
   "source": [
    "Multicollinearity is a phenomenon that occurs in multiple linear regression when two or more predictor variables in the model are highly correlated with each other. In other words, there is a strong linear relationship between two or more independent variables. This can pose challenges and complications in the regression analysis, leading to unstable coefficient estimates and difficulties in interpreting the model results. Here's a more detailed explanation of multicollinearity:\n",
    "\n",
    "1. **Correlation Among Predictor Variables:** Multicollinearity arises when there is a high correlation between two or more predictor variables. This means that changes in one predictor variable are associated with changes in another predictor variable.\n",
    "\n",
    "2. **Impact on Coefficient Estimates:** Multicollinearity makes it difficult to isolate the individual effects of the correlated predictors on the target variable. As a result, the coefficient estimates of the correlated predictors can become unstable and have large standard errors.\n",
    "\n",
    "3. **Interpretation of Coefficients:** When multicollinearity is present, it becomes challenging to interpret the coefficients accurately. It becomes unclear which variable is truly influencing the target variable since their effects are intertwined.\n",
    "\n",
    "4. **Inflated Standard Errors:** High multicollinearity results in inflated standard errors for the coefficient estimates of correlated variables. This makes it difficult to determine whether a coefficient is statistically significant or not.\n",
    "\n",
    "5. **Reduced Model Reliability:** Multicollinearity can lead to misleading model results, as the relationships between variables become difficult to discern. The model may perform well in terms of statistical measures, but the coefficients may not accurately reflect the underlying relationships.\n",
    "\n",
    "6. **Solutions to Multicollinearity:**\n",
    "   - Removing one of the correlated predictors: If two predictors are highly correlated, removing one can help mitigate multicollinearity.\n",
    "   - Feature selection: Choose only one of the correlated predictors based on domain knowledge or feature importance.\n",
    "   - Ridge regression: Regularization techniques like ridge regression can help stabilize coefficient estimates and mitigate multicollinearity.\n",
    "   - Collect more data: Increasing the sample size can sometimes alleviate multicollinearity.\n",
    "\n",
    "7. **Detecting Multicollinearity:**\n",
    "   - Correlation matrix: Examine the correlation matrix of predictor variables to identify strong correlations.\n",
    "   - Variance Inflation Factor (VIF): Calculate the VIF for each predictor to quantify the degree of multicollinearity.\n",
    "\n",
    "Dealing with multicollinearity is important to ensure the reliability and interpretability of the multiple linear regression model. High multicollinearity can undermine the validity of model results and lead to incorrect conclusions about the relationships between variables. Therefore, it's crucial to address multicollinearity through appropriate techniques to build a more robust and accurate regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ed46d",
   "metadata": {},
   "source": [
    "Detecting Multicollinearity:\n",
    "1. **Correlation Matrix:** Calculate the correlation matrix among predictor variables. High correlations (close to 1 or -1) indicate potential multicollinearity.\n",
    "2. **Variance Inflation Factor (VIF):** Calculate the VIF for each predictor variable. VIF measures how much the variance of a coefficient is inflated due to multicollinearity. High VIF values (typically above 5 or 10) suggest multicollinearity.\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "1. **Feature Selection:** If two or more predictors are highly correlated, consider keeping only one of them in the model based on domain knowledge or feature importance.\n",
    "2. **Combining Correlated Variables:** Create a new predictor variable that combines information from correlated predictors, reducing multicollinearity.\n",
    "3. **Ridge Regression:** Use regularization techniques like ridge regression. Ridge regression adds a penalty term to the coefficients, forcing them to be smaller and mitigating multicollinearity's impact.\n",
    "4. **Collect More Data:** Increasing the sample size can sometimes help alleviate multicollinearity issues.\n",
    "5. **PCA (Principal Component Analysis):** PCA can transform correlated predictors into uncorrelated principal components, reducing multicollinearity.\n",
    "6. **Interaction Terms:** Introduce interaction terms between correlated predictors to account for their joint effect.\n",
    "\n",
    "It's important to note that while addressing multicollinearity, you should strive for models that are both statistically reliable and interpretable. Careful consideration of the underlying relationships between variables, domain knowledge, and the specific goals of the analysis is essential to decide how to detect and handle multicollinearity effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8827a736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "460548b8",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 7 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce7a64",
   "metadata": {},
   "source": [
    "Polynomial regression is a type of regression analysis that extends the simple linear regression model by introducing polynomial terms of higher degrees into the equation. In essence, it allows for curved relationships between the predictor and the target variable. The polynomial regression model can capture non-linear patterns that simple linear regression cannot.\n",
    "\n",
    "The polynomial regression model is defined as follows:\n",
    "\n",
    "\\[ y = β_0+  β₁ * x + β₂ * x^2 + β_3 * x^3 + ... + β_n * x^n +  ϵ \\]\n",
    "\n",
    "Where:\n",
    "- \\( y \\) is the target (dependent) variable.\n",
    "- \\( x \\) is the predictor (independent) variable.\n",
    "- \\( β_0, β₁ ,  β₂, ... ,  β_n \\) are the coefficients.\n",
    "- \\(  ϵ  \\) represents the error term.\n",
    "\n",
    "Key points to note about polynomial regression:\n",
    "1. The model can capture various degrees of curvature by including polynomial terms up to degree \\( n \\).\n",
    "2. Choosing the appropriate degree of the polynomial is crucial. A higher degree doesn't necessarily lead to a better fit and can result in overfitting.\n",
    "3. Polynomial regression is still a form of linear regression because the relationship between the coefficients and the target variable remains linear.\n",
    "4. Polynomial regression can be sensitive to outliers, especially in higher-degree polynomials.\n",
    "5. Regularization techniques like ridge or lasso regression can be applied to manage overfitting in polynomial regression as well.\n",
    "\n",
    "In practice, polynomial regression can be useful when the relationship between variables isn't linear and when higher-degree polynomial terms help capture the underlying patterns in the data. However, care should be taken to avoid overfitting and to choose an appropriate degree of the polynomial based on the data and the problem's context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd008dcc",
   "metadata": {},
   "source": [
    "Polynomial regression and linear regression are both regression techniques used to model relationships between variables, but they differ in terms of the type of relationships they can capture and the equations they use to represent those relationships.\n",
    "\n",
    "1. **Type of Relationships:**\n",
    "   - **Linear Regression:** Linear regression models assume a linear relationship between the predictor variable(s) and the target variable. It models a straight-line relationship and works well when the data points tend to align in a relatively straight line.\n",
    "   - **Polynomial Regression:** Polynomial regression can capture non-linear relationships between the predictor variable(s) and the target variable by introducing polynomial terms of higher degrees. This allows it to model curves and bends in the relationship that linear regression cannot capture.\n",
    "\n",
    "2. **Equations:**\n",
    "   - **Linear Regression:** The equation for a simple linear regression is \\( y = β_0 +  β₁ * x + ϵ \\), where \\(  β_0 \\) is the intercept, \\(  β₁ \\) is the coefficient for the predictor variable \\( x \\), and \\( ϵ  \\) represents the error term.\n",
    "   - **Polynomial Regression:** The equation for polynomial regression includes additional terms for higher degrees of the predictor variable, such as \\( x^2 \\), \\( x^3 \\), and so on. The equation becomes \\( y =  β_0 + β₁ * x + β₂ * x^2 + ...  + ϵ  \\), with coefficients for each polynomial term.\n",
    "\n",
    "3. **Fitting Complexity:**\n",
    "   - **Linear Regression:** Linear regression is relatively simpler to interpret and implement. It involves finding the best-fitting straight line that minimizes the sum of squared differences between the predicted and actual values.\n",
    "   - **Polynomial Regression:** Polynomial regression involves finding the coefficients for polynomial terms, and the complexity increases as the degree of the polynomial increases. Higher-degree polynomials can lead to overfitting if not chosen carefully.\n",
    "\n",
    "4. **Model Interpretability:**\n",
    "   - **Linear Regression:** Linear regression models are easier to interpret because the relationship is represented by a straight line with clear coefficients.\n",
    "   - **Polynomial Regression:** Interpretability can be more challenging with polynomial regression, especially with higher-degree polynomials, as the relationships become more complex.\n",
    "\n",
    "5. **Data Points:**\n",
    "   - **Linear Regression:** Linear regression works well when data points exhibit a relatively straight-line pattern.\n",
    "   - **Polynomial Regression:** Polynomial regression is suitable when data points show non-linear patterns, such as curves, bends, or parabolic shapes.\n",
    "\n",
    "In summary, polynomial regression is an extension of linear regression that allows for modeling non-linear relationships by introducing polynomial terms. It is a more flexible approach when dealing with data that doesn't follow a straight-line trend. However, choosing the right degree of the polynomial is crucial to avoid overfitting and to ensure the model's generalization to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99b82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82029f83",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 8 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f31b1",
   "metadata": {},
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "1. **Flexibility in Modeling:** Polynomial regression can capture more complex relationships in the data, including curves and bends, which linear regression cannot represent.\n",
    "\n",
    "2. **Better Fit:** When the relationship between variables is non-linear, polynomial regression can provide a better fit to the data compared to linear regression.\n",
    "\n",
    "3. **Higher Order Interactions:** Polynomial regression can capture interactions between predictor variables that may affect the target variable.\n",
    "\n",
    "4. **Non-Constant Variance:** Polynomial regression can handle situations where the variance of the residuals is not constant across all levels of the predictor variable(s).\n",
    "\n",
    "5. **Increased Predictive Power:** In cases where the data exhibits a non-linear trend, using polynomial regression can lead to better predictive performance.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "1. **Overfitting:** Higher-degree polynomial models can easily overfit the training data, leading to poor generalization to new, unseen data. Selecting the appropriate degree of the polynomial is crucial to avoid overfitting.\n",
    "\n",
    "2. **Complexity:** As the degree of the polynomial increases, the model becomes more complex and harder to interpret. This complexity can make it challenging to identify the true relationships within the data.\n",
    "\n",
    "3. **Extrapolation Uncertainty:** Polynomial regression models are sensitive to extrapolation, meaning that predictions made outside the range of the training data may be unreliable.\n",
    "\n",
    "4. **Ill-Conditioning:** When predictor variables are highly correlated, polynomial regression can lead to multicollinearity, which affects the stability and interpretability of the model.\n",
    "\n",
    "5. **Data Requirement:** Polynomial regression requires a sufficient amount of data, especially for higher-degree polynomials, to accurately estimate the model parameters.\n",
    "\n",
    "6. **Bias-Variance Tradeoff:** While polynomial regression can fit complex patterns, it can also introduce more variance into the model, leading to a potential tradeoff between bias and variance.\n",
    "\n",
    "7. **Higher Computational Cost:** Fitting higher-degree polynomial models can be computationally expensive, particularly if the dataset is large.\n",
    "\n",
    "In summary, polynomial regression is a useful technique for capturing non-linear relationships in the data, but it comes with the tradeoffs of potential overfitting, complexity, and the need to carefully select the degree of the polynomial. It's important to assess the benefits and drawbacks of polynomial regression in the context of the specific dataset and the goals of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba7cf9",
   "metadata": {},
   "source": [
    "Polynomial regression is preferred in situations where the relationship between the predictor variables and the target variable is not linear and exhibits curvature or non-linearity. Here are some scenarios where polynomial regression could be a suitable choice:\n",
    "\n",
    "1. **Curved Relationships:** When the scatter plot of the data suggests a non-linear pattern, such as a curve or bend, polynomial regression can capture these complex relationships.\n",
    "\n",
    "2. **Domain Knowledge:** If domain knowledge or theoretical reasoning indicates that the relationship between variables should follow a specific polynomial shape, using polynomial regression can help model this relationship accurately.\n",
    "\n",
    "3. **Underlying Mechanisms:** In some cases, the true relationship between variables may be more accurately represented by a polynomial function due to underlying physical or biological mechanisms.\n",
    "\n",
    "4. **Limited Data Transformation:** When linear transformations of predictor variables cannot capture the desired relationship, polynomial regression provides a way to incorporate higher-order terms.\n",
    "\n",
    "5. **Interactions and Thresholds:** Polynomial regression can capture interactions between variables that result in non-linear effects. It can also model threshold effects where the relationship changes sharply at specific values.\n",
    "\n",
    "6. **Feature Engineering:** If linear regression doesn't provide a good fit, engineers often use polynomial features as a way to enhance the model's representation power.\n",
    "\n",
    "7. **Time Series Analysis:** Polynomial regression can be useful for modeling trends in time series data, especially when the trend is non-linear.\n",
    "\n",
    "8. **Economic Data:** Economic data often exhibits non-linear relationships due to diminishing returns or saturation effects. Polynomial regression can model such relationships.\n",
    "\n",
    "However, it's important to consider the potential drawbacks of polynomial regression, such as overfitting, complexity, and the need to select an appropriate degree for the polynomial. Careful analysis, model validation, and cross-validation techniques should be employed to ensure that the chosen polynomial degree is suitable for the data and prevents overfitting. Additionally, other advanced techniques like regularization can be used to mitigate overfitting issues in polynomial regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74497134",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcce72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83949216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d4d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9d1485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9624e98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
