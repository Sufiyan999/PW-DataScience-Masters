{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aef7e073",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533b3e9",
   "metadata": {},
   "source": [
    "Ridge Regression, also known as L2 regularization, is a linear regression technique used to mitigate multicollinearity (high correlation between predictor variables) and overfitting in regression models. It adds a penalty term to the linear regression cost function that discourages large coefficients for the predictor variables. The penalty term is proportional to the square of the magnitude of the coefficients.\n",
    "\n",
    "In Ridge Regression, the goal is to find the coefficients that not only fit the data well but also have smaller magnitudes. This helps in reducing the impact of irrelevant features and controlling the complexity of the model. The Ridge Regression cost function can be represented as follows:\n",
    "\n",
    "Cost Function = RSS (Residual Sum of Squares) + α * Σ(β²)\n",
    "\n",
    "Where:\n",
    "- RSS is the Residual Sum of Squares, which measures the difference between predicted and actual values.\n",
    "- α (alpha) is the regularization parameter, a non-negative hyperparameter that controls the strength of the regularization. A higher α leads to stronger regularization.\n",
    "\n",
    "The Ridge Regression algorithm minimizes this cost function to find the optimal coefficients. The regularization term penalizes large coefficients, which means that the algorithm will favor smaller coefficient values, even if it results in slightly increased errors in the predictions. This helps prevent overfitting by discouraging the model from fitting noise in the data.\n",
    "\n",
    "Ridge Regression is particularly useful when dealing with multicollinearity, which can cause instability in the coefficients and make them sensitive to small changes in the data. By adding the penalty term, Ridge Regression ensures that the coefficients are less likely to change dramatically, leading to more stable and interpretable results.\n",
    "\n",
    "The optimal value of α can be determined using techniques like cross-validation, grid search, or analytically by finding the value that minimizes the cross-validated error. Ridge Regression is suitable for cases where there is a suspicion of multicollinearity or when you want to control model complexity and prevent overfitting while retaining all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8fd84",
   "metadata": {},
   "source": [
    "Ridge Regression differs from Ordinary Least Squares (OLS) Regression primarily in the way it handles the coefficients and addresses the issues of multicollinearity and overfitting:\n",
    "\n",
    "1. **Penalty Term:**\n",
    "   - OLS: Ordinary Least Squares aims to minimize the residual sum of squares (RSS) without any penalty on the magnitude of coefficients. It seeks to find the coefficients that fit the training data the best.\n",
    "   - Ridge Regression: Ridge Regression adds a penalty term to the RSS, which is proportional to the sum of the squares of the coefficients (except the intercept). The penalty term is controlled by the regularization parameter α (alpha). This encourages the coefficients to be smaller and less sensitive to variations in the data.\n",
    "\n",
    "2. **Purpose:**\n",
    "   - OLS: OLS aims to find the coefficients that minimize the difference between predicted and actual values, making the model fit the training data as closely as possible.\n",
    "   - Ridge Regression: Ridge Regression aims to achieve a balance between fitting the data well and maintaining smaller coefficient values. It aims to prevent overfitting by reducing the impact of high-variance features.\n",
    "\n",
    "3. **Multicollinearity:**\n",
    "   - OLS: OLS can be sensitive to multicollinearity, which is the high correlation between predictor variables. In the presence of multicollinearity, OLS may lead to unstable or unreliable coefficient estimates.\n",
    "   - Ridge Regression: Ridge Regression is designed to handle multicollinearity effectively by shrinking the coefficients of correlated predictors. It ensures that the coefficients are more stable and less sensitive to variations in the data, reducing the risk of overfitting.\n",
    "\n",
    "4. **Coefficients:**\n",
    "   - OLS: OLS coefficients can take any value that best fits the data, even if this results in large coefficient magnitudes.\n",
    "   - Ridge Regression: Ridge coefficients are constrained to be smaller due to the penalty term. This regularization helps in avoiding very large coefficients and potentially prevents overfitting.\n",
    "\n",
    "5. **Bias-Variance Trade-off:**\n",
    "   - OLS: OLS may suffer from overfitting when the model captures noise in the training data, leading to high variance.\n",
    "   - Ridge Regression: Ridge Regression provides a trade-off between bias and variance. It reduces the variance by shrinking coefficients, which can lead to slightly increased bias. This trade-off helps in obtaining a more generalizable model.\n",
    "\n",
    "In summary, Ridge Regression adds a regularization term to the cost function that encourages smaller coefficient values and provides better stability against multicollinearity and overfitting compared to Ordinary Least Squares. It's a useful technique when dealing with high-dimensional data, multicollinearity, or when a balance between fitting the data and controlling model complexity is desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d78414",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff341958",
   "metadata": {},
   "source": [
    "Ridge Regression is an extension of Ordinary Least Squares (OLS) Regression, so many of the assumptions of OLS also apply to Ridge Regression. However, Ridge Regression does introduce a regularization term that can affect the interpretation of the assumptions. Here are the assumptions of Ridge Regression:\n",
    "\n",
    "1. **Linearity:** The relationship between the predictors and the response variable is assumed to be linear.\n",
    "\n",
    "2. **Independence:** The observations should be independent of each other. This assumption is essential to avoid bias in coefficient estimates.\n",
    "\n",
    "3. **Homoscedasticity:** The residuals (the differences between observed and predicted values) should have constant variance across all levels of the predictors. Ridge Regression does not directly address this assumption; it mainly focuses on reducing multicollinearity and overfitting.\n",
    "\n",
    "4. **Normality of Residuals:** The residuals should follow a normal distribution. Ridge Regression doesn't significantly affect this assumption, as it primarily impacts the coefficients.\n",
    "\n",
    "5. **No Multicollinearity:** Ridge Regression was specifically developed to address the multicollinearity assumption. It helps stabilize coefficient estimates when multicollinearity is present among predictor variables.\n",
    "\n",
    "6. **No Endogeneity:** There should be no correlation between the error term and the predictors. Ridge Regression doesn't directly impact this assumption.\n",
    "\n",
    "While Ridge Regression helps with multicollinearity and reduces overfitting, it doesn't guarantee that all the assumptions of linear regression are met. Therefore, it's important to assess the assumptions relevant to Ridge Regression and interpret its results in conjunction with the overall context of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de71dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6eedb8a",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1263609e",
   "metadata": {},
   "source": [
    "In Ridge Regression, the tuning parameter (lambda or alpha) controls the strength of regularization applied to the model. It determines the balance between fitting the data well and keeping the coefficients small to prevent overfitting. The goal is to find the optimal value of lambda that results in the best model performance.\n",
    "\n",
    "There are several methods to select the value of the tuning parameter in Ridge Regression:\n",
    "\n",
    "1. **Grid Search:** This is a common method where you specify a range of lambda values and then evaluate the model's performance (e.g., using cross-validation) for each lambda value. The lambda that gives the best performance (e.g., lowest mean squared error or highest R-squared) is selected.\n",
    "\n",
    "2. **Cross-Validation:** Techniques like k-fold cross-validation can be used to evaluate the model's performance with different lambda values. By testing the model on multiple subsets of the data, you can get a better estimate of how well it generalizes. The lambda value that leads to the best average performance across all folds is chosen.\n",
    "\n",
    "3. **Regularization Path:** You can also examine the regularization path by plotting the coefficients against different lambda values. This can help you visualize how the coefficients change as lambda varies, and you can choose a value that achieves the desired level of shrinkage.\n",
    "\n",
    "4. **Information Criterion:** Criteria like the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) can be used to balance the goodness of fit with the complexity of the model. Lower values indicate better trade-offs between fit and complexity.\n",
    "\n",
    "5. **Domain Knowledge:** Sometimes, domain knowledge or prior information about the problem can guide the choice of lambda. If you know certain variables should have small coefficients, you might choose a lambda that encourages this.\n",
    "\n",
    "6. **Automated Algorithms:** Some libraries and software packages provide algorithms that automatically select the best lambda value based on certain criteria. For example, scikit-learn's RidgeCV in Python performs cross-validation to find the optimal lambda.\n",
    "\n",
    "It's important to note that the optimal lambda value can depend on the specific dataset and problem at hand. It's recommended to try different methods and evaluate the model's performance using appropriate metrics to ensure that the chosen lambda value generalizes well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ad0863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d464c7",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45daef3",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for feature selection indirectly by shrinking the coefficients of less important features toward zero. While Ridge Regression doesn't exactly eliminate features from the model like some other feature selection methods, it can effectively reduce the impact of less relevant features on the model's predictions.\n",
    "\n",
    "Here's how Ridge Regression can be used for feature selection:\n",
    "\n",
    "1. **Coefficient Shrinkage:** In Ridge Regression, the regularization term adds a penalty to the size of the coefficients. As the regularization parameter (lambda or alpha) increases, the magnitude of the coefficients decreases. This means that features with smaller coefficients are effectively downweighted in the model.\n",
    "\n",
    "2. **Automatic Feature Weighting:** Features with large coefficients are weighted more heavily in the model's predictions, while those with smaller coefficients contribute less. As a result, less important features may have coefficients that are shrunk close to zero, effectively reducing their impact on the predictions.\n",
    "\n",
    "3. **Implicit Feature Selection:** As the regularization parameter increases, Ridge Regression tends to encourage simpler models with fewer large coefficients. This implicitly leads to a form of feature selection where only the most important features retain significant coefficients, while others are suppressed.\n",
    "\n",
    "However, Ridge Regression doesn't entirely eliminate features from the model, unlike some explicit feature selection methods. It's important to note that Ridge Regression doesn't perform feature selection in the sense of setting coefficients to exactly zero, as is the case with Lasso Regression. Therefore, while Ridge Regression can help mitigate multicollinearity and prevent overfitting, it may not be the best choice if your main goal is aggressive feature selection.\n",
    "\n",
    "If your primary aim is feature selection, you might consider Lasso Regression, which performs both regularization and feature selection by driving some coefficients to exactly zero. Alternatively, Elastic Net Regression combines Ridge and Lasso regularization, offering a trade-off between the two techniques and providing a way to perform feature selection while also handling multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0f24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf4f8a12",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306cce1",
   "metadata": {},
   "source": [
    "Ridge Regression is particularly well-suited for addressing multicollinearity in linear regression models. Multicollinearity occurs when independent variables (features) in a regression model are highly correlated with each other. This can lead to unstable and unreliable coefficient estimates, making it difficult to interpret the individual impact of each feature on the target variable.\n",
    "\n",
    "Here's how Ridge Regression performs in the presence of multicollinearity:\n",
    "\n",
    "1. **Reduced Sensitivity to Multicollinearity:** Ridge Regression adds a regularization term to the loss function, which includes the sum of squared coefficients. As the regularization parameter (lambda or alpha) increases, the magnitude of the coefficients gets reduced. This reduction in coefficient values helps mitigate the impact of multicollinearity, as it discourages the model from relying too heavily on any one correlated feature.\n",
    "\n",
    "2. **Balanced Coefficient Shrinking:** Ridge Regression doesn't force coefficients to exactly zero, but it shrinks them toward zero. This means that highly correlated features will have similar, but not necessarily identical, coefficients. As a result, Ridge Regression assigns a more balanced influence to correlated features, which can help in reducing the instability caused by multicollinearity.\n",
    "\n",
    "3. **Stabilized Predictions:** Ridge Regression can help stabilize predictions in the presence of multicollinearity. The regularization encourages the model to use all correlated features to some extent, rather than overemphasizing one of them. This results in more stable and reliable predictions across different datasets.\n",
    "\n",
    "4. **Tuned Regularization Parameter:** The choice of the regularization parameter (lambda) is crucial. Cross-validation can be used to determine the optimal value of lambda that balances the reduction of multicollinearity and model performance. Cross-validation helps ensure that the model generalizes well to unseen data.\n",
    "\n",
    "Overall, Ridge Regression is a valuable tool for addressing multicollinearity and creating more robust linear regression models. It strikes a balance between stabilizing coefficient estimates and maintaining predictive performance, making it a useful approach when multicollinearity is a concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a075d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65b37c44",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 6 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62224e4f",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can handle both categorical and continuous independent variables. However, some considerations need to be taken into account when dealing with categorical variables in Ridge Regression.\n",
    "\n",
    "1. **Encoding Categorical Variables:** Categorical variables need to be properly encoded before using them in Ridge Regression. Many machine learning libraries, such as scikit-learn in Python, require categorical variables to be converted into numerical values using techniques like one-hot encoding or label encoding. One-hot encoding is preferred for Ridge Regression, as it avoids assigning ordinal relationships to categorical variables.\n",
    "\n",
    "2. **Regularization Impact:** Ridge Regression applies regularization to all coefficients, including those associated with categorical variables. While Ridge Regression helps prevent overfitting, it might not be as effective for categorical variables with a large number of levels (categories). In such cases, Lasso Regression might be more appropriate, as it can potentially drive some categorical coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "3. **Interaction Effects:** When one-hot encoding is used, interaction terms between categorical variables and continuous variables can be introduced. Ridge Regression can help manage multicollinearity between these terms.\n",
    "\n",
    "4. **Scaling:** It's important to scale continuous variables before applying Ridge Regression to ensure that all variables are on a similar scale. This ensures that the regularization term equally impacts all features.\n",
    "\n",
    "In summary, Ridge Regression can handle both categorical and continuous independent variables, but proper preprocessing steps such as encoding categorical variables and scaling continuous variables are essential for its effective application. Additionally, consider the nature of the categorical variables and the overall model objectives when deciding between Ridge and other regression techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d44592e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cfc52a0",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 7 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667cc4c",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of Ridge Regression is slightly different from interpreting coefficients in ordinary least squares (OLS) regression due to the presence of regularization. In Ridge Regression, the coefficients are adjusted to minimize the sum of squared errors while also considering a regularization term.\n",
    "\n",
    "The formula for Ridge Regression is:\n",
    "\n",
    "\\[ Loss = ∑  ( y_i  -  Ŷ_i )^2 + λ * ∑ β_j^2 \\]\n",
    "\n",
    "Here, \\( y_i \\) is the actual response, \\( Ŷ_i \\) is the predicted response, \\( λ \\) is the regularization parameter, \\( p \\) is the number of features, and \\( \\ β_j \\) are the coefficients.\n",
    "\n",
    "Interpreting the coefficients:\n",
    "\n",
    "1. **Sign of Coefficients:** The sign of coefficients (\\(  β_j \\)) indicates whether the relationship with the target variable is positive or negative. Positive coefficients imply that an increase in the predictor variable is associated with an increase in the response, and vice versa.\n",
    "\n",
    "2. **Magnitude of Coefficients:** The magnitude of coefficients reflects the strength of the relationship between a predictor and the response. However, unlike in OLS regression, the coefficients' magnitudes can be smaller due to the regularization term.\n",
    "\n",
    "3. **Shrinkage:** Ridge Regression applies a \"shrinkage\" effect on the coefficients, which means that they are pushed closer to zero. This is evident in the formula, where the regularization term \\( λ * ∑ {j=1}^{p}  β_j^2 \\) penalizes large coefficients.\n",
    "\n",
    "4. **Trade-off with Bias:** Ridge Regression introduces a bias in order to reduce variance and prevent overfitting. This means that the model may not fit the training data perfectly, but it generalizes better to new data.\n",
    "\n",
    "5. **Feature Importance:** In Ridge Regression, coefficients with smaller magnitudes contribute less to the model's prediction, while those with larger magnitudes have a more significant impact. However, Ridge Regression typically does not set coefficients exactly to zero, so all features are retained in the model to some extent.\n",
    "\n",
    "In conclusion, interpreting Ridge Regression coefficients involves considering the signs, magnitudes, and the effect of regularization on the coefficients. Keep in mind that Ridge Regression aims to balance bias and variance, and feature importance is determined based on the relative magnitudes of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5b6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb22beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e3b742",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 8 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697867bf",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be used for time-series data analysis, but its application in this context may require careful consideration and adaptation to suit the characteristics of time-series data. Ridge Regression can be particularly useful when dealing with multicollinearity or when there are more features than observations in the time-series dataset.\n",
    "\n",
    "Here's how Ridge Regression can be used for time-series data analysis:\n",
    "\n",
    "1. **Feature Engineering:** Just like in other regression tasks, you need to identify relevant features for prediction. In time-series data, these features can include lagged values of the target variable and relevant external factors.\n",
    "\n",
    "2. **Regularization Parameter (Lambda):** The choice of the regularization parameter (\\( \\lambda \\)) is crucial. Cross-validation techniques, such as k-fold cross-validation, can help you select an optimal \\( \\lambda \\) value that balances bias and variance. This process involves training Ridge Regression models with different \\( \\lambda \\) values and evaluating their performance on validation data.\n",
    "\n",
    "3. **Lagged Variables:** In time-series data, lagged variables (past values of the target variable) are often included as predictors. The regularization term in Ridge Regression can help mitigate the multicollinearity that may arise when including lagged variables.\n",
    "\n",
    "4. **Feature Scaling:** Standardizing or scaling the features is important, especially when working with Ridge Regression. It ensures that all features are on a comparable scale, which helps the regularization term treat all features fairly.\n",
    "\n",
    "5. **Model Evaluation:** Evaluate the performance of your Ridge Regression model using appropriate metrics for time-series data, such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and potentially specialized metrics like Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "6. **Time-Series Cross-Validation:** When evaluating the performance of your Ridge Regression model, it's important to use time-series cross-validation techniques that respect the temporal ordering of the data. Techniques like rolling-window cross-validation or walk-forward validation can be used.\n",
    "\n",
    "7. **Grid Search for Hyperparameter Tuning:** Along with selecting the optimal \\( \\lambda \\) value, you may want to tune other hyperparameters of the model, such as the order of lagged variables or the rolling window size for cross-validation.\n",
    "\n",
    "8. **Consider Other Methods:** While Ridge Regression can help mitigate multicollinearity and overfitting in time-series data, other techniques like autoregressive integrated moving average (ARIMA) or seasonal decomposition of time series (STL) might be more suitable for capturing time-specific patterns.\n",
    "\n",
    "Keep in mind that while Ridge Regression can be adapted for time-series data, the choice of the modeling approach should depend on the characteristics of your data, the underlying patterns, and the specific goals of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b61ec36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f996f754",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006c2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b100e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4369c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd134cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8136ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18e378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
