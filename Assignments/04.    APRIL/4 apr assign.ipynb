{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f62300c",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d37179",
   "metadata": {},
   "source": [
    "A decision tree classifier is a supervised machine learning algorithm used for classification tasks. It makes predictions by recursively partitioning the input data into subsets based on the values of different features, eventually assigning a class label to each subset. It's called a \"tree\" because the structure of the algorithm resembles a tree-like diagram.\n",
    "\n",
    "Here's how the decision tree classifier algorithm works:\n",
    "\n",
    "1. **Tree Construction:**\n",
    "   - The algorithm starts with the entire dataset at the root node.\n",
    "   - It selects a feature and a corresponding threshold that best separates the data into different classes. This selection is based on various criteria such as Gini impurity, entropy, or information gain.\n",
    "   - The dataset is then split into two or more subsets based on the chosen feature and threshold.\n",
    "   - The above step is repeated recursively for each subset (child node), considering only the data that reaches that node.\n",
    "   - This process continues until a stopping criterion is met. This criterion could be a maximum tree depth, a minimum number of samples in a node, or the purity of the node's class distribution.\n",
    "\n",
    "2. **Stopping Criteria:**\n",
    "   - Decision trees can easily overfit the training data, so it's crucial to have stopping criteria to prevent this. Overfitting occurs when the tree is too complex and captures noise in the data.\n",
    "   - Common stopping criteria include a maximum depth for the tree, a minimum number of samples in a leaf node, or a threshold for the purity of the node's class distribution.\n",
    "\n",
    "3. **Assigning Class Labels:**\n",
    "   - Once the tree construction is complete, each leaf node is assigned a class label. This label is typically determined by a majority vote of the samples in that leaf node. For example, if most of the samples belong to class A in a particular leaf, that leaf is labeled as class A.\n",
    "\n",
    "4. **Prediction:**\n",
    "   - To make a prediction for a new data point, the algorithm starts at the root node of the tree and traverses down the tree based on the feature values of the data point.\n",
    "   - At each internal node, the algorithm checks the value of the corresponding feature and goes to the left or right child node based on whether the value is less than or greater than the threshold.\n",
    "   - The traversal continues until a leaf node is reached, and the class label associated with that leaf node is assigned to the new data point.\n",
    "\n",
    "Decision trees have several advantages, including their ability to handle both numerical and categorical data, their interpretability, and their potential to capture complex relationships in the data. However, they can also be prone to overfitting, especially with deep trees. Techniques like pruning (removing branches) and using ensemble methods like Random Forests or Gradient Boosting Trees are commonly employed to address these issues.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08623eb5",
   "metadata": {},
   "source": [
    " how a decision tree works to make predictions using a step-by-step example:\n",
    "\n",
    "Suppose we have a dataset with two features: \"Age\" and \"Income\", and we're trying to predict whether a person will buy a product or not (binary classification).\n",
    "\n",
    "Here's how the decision tree makes predictions:\n",
    "\n",
    "1. **Tree Construction:**\n",
    "   - The algorithm starts with the entire dataset at the root node.\n",
    "   - It selects a feature and a threshold that best splits the data into subsets.\n",
    "   - Let's say the first split is based on the \"Age\" feature, and the threshold is 30 years.\n",
    "   - The data is split into two subsets: one with people younger than 30 and one with people 30 or older.\n",
    "\n",
    "2. **Continue Splitting:**\n",
    "   - The algorithm continues to recursively split each subset based on different features and thresholds.\n",
    "   - For example, for the subset of people younger than 30, it might select the \"Income\" feature and a threshold of $50,000.\n",
    "   - This further divides the data into more subsets, perhaps based on whether their income is above or below $50,000.\n",
    "\n",
    "3. **Leaf Nodes and Class Labels:**\n",
    "   - As the algorithm keeps splitting, it eventually reaches points where it stops due to a stopping criterion (e.g., maximum depth or minimum samples).\n",
    "   - At these stopping points, the algorithm assigns a class label to each leaf node based on the majority class of the samples in that node.\n",
    "\n",
    "4. **Traversal for Prediction:**\n",
    "   - To make a prediction for a new data point, start at the root node.\n",
    "   - Check the feature value of the data point against the threshold at the root node.\n",
    "   - If the value is below the threshold, move to the left child node; otherwise, move to the right child node.\n",
    "   - Repeat this process as you traverse down the tree, comparing the feature values with the thresholds at each internal node.\n",
    "   - Eventually, you'll reach a leaf node.\n",
    "\n",
    "5. **Prediction:**\n",
    "   - The class label assigned to the leaf node reached during traversal is the predicted class for the new data point.\n",
    "\n",
    "For example, if we have a new data point with an age of 25 and an income of $40,000, we would follow the decisions in the tree to make a prediction:\n",
    "- Start at the root node (age < 30)\n",
    "- Move to the left child node (income < $50,000)\n",
    "- Reach a leaf node labeled \"Buy\"\n",
    "\n",
    "So, the prediction for this data point would be that the person will buy the product.\n",
    "\n",
    "This process of traversing the decision tree based on feature values and thresholds is how the decision tree algorithm makes predictions for new data points. Keep in mind that interpretability is one of the strengths of decision trees, as you can visually trace the path from the root to the leaf to understand the decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f503748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24ac0728",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ec3ffd",
   "metadata": {},
   "source": [
    " mathematical intuition behind decision tree classification using a simple example. We'll consider the Gini impurity as the criterion for selecting the best feature and threshold for splitting the data.\n",
    "\n",
    "Suppose we have a binary classification problem with the following dataset:\n",
    "\n",
    "| Age | Income | Buy |\n",
    "|-----|--------|-----|\n",
    "| 25  | 40000  | No  |\n",
    "| 30  | 60000  | Yes |\n",
    "| 28  | 55000  | Yes |\n",
    "| 35  | 45000  | No  |\n",
    "| 40  | 70000  | Yes |\n",
    "\n",
    "We'll use the Gini impurity to evaluate potential splits in the decision tree. The Gini impurity measures the degree of impurity in a set of samples. It's calculated as follows for a binary classification problem:\n",
    "\n",
    "\\[\n",
    "Gini(p) = 1 - (p_Yes^2 + p_No^2)\n",
    "\\]\n",
    "\n",
    "Where \\( p_Yes \\) is the proportion of positive class samples, and \\( p_No \\) is the proportion of negative class samples in the set.\n",
    "\n",
    "**Step-by-step process:**\n",
    "\n",
    "1. **Calculate Initial Gini Impurity:**\n",
    "   - Calculate the initial Gini impurity for the entire dataset.\n",
    "   - The dataset has 2 \"Yes\" and 3 \"No\" samples.\n",
    "   - \\( p_\n",
    "   Yes = 2 / 5 \\) and \\( p_No = 3 / 5 \\)\n",
    "   - Initial Gini impurity \\( Gini_initial =  1 -(2/5)^2 - (3/5)^2 \\)\n",
    "\n",
    "2. **Evaluate Possible Splits:**\n",
    "   - For each feature, evaluate possible splits by sorting the data and considering midpoints between consecutive samples.\n",
    "   - Calculate the Gini impurity for each split.\n",
    "\n",
    "3. **Select the Best Split:**\n",
    "   - Choose the split with the lowest Gini impurity. This split effectively separates the data into subsets that are as pure as possible.\n",
    "\n",
    "4. **Repeat for Child Nodes:**\n",
    "   - For each resulting subset, repeat the process recursively to find the best split for child nodes.\n",
    "\n",
    "5. **Stopping Criteria:**\n",
    "   - Continue this process until a stopping criterion is met, such as reaching a maximum tree depth or having a minimum number of samples in a node.\n",
    "\n",
    "In our example, let's say the algorithm chooses to split based on the \"Age\" feature with a threshold of 27.5 (between 25 and 30).\n",
    "\n",
    "- Subset 1 (Age < 27.5):\n",
    "  - 1 \"Yes\" and 1 \"No\" sample.\n",
    "  - \\( p_Yes = 1 / 2  \\) and \\( p_No = 1 / 2 \\)\n",
    "  - Gini impurity \\( Gini_subset1 = 1 -(0.5)^2 - (0.5)^2  )\n",
    "\n",
    "- Subset 2 (Age >= 27.5):\n",
    "  - 1 \"Yes\" and 2 \"No\" samples.\n",
    "  - \\( p_Yes = 1 / 3 \\) and \\( p_No =  2 /3  \\)\n",
    "  - Gini impurity \\( Gini_subset2 = 1 - 1 -(1/3 )^2 - (2/3)^2 \\)\n",
    "\n",
    "By calculating the Gini impurities for the subsets, we can see which split results in lower impurity, and that's the one the algorithm would choose.\n",
    "\n",
    "\n",
    "This step-by-step process, repeated recursively, helps the decision tree algorithm find the splits that best separate the data into pure subsets, which in turn aids in making accurate predictions for new data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a5cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06d5006d",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cec4a7",
   "metadata": {},
   "source": [
    "A decision tree classifier can be used to solve a binary classification problem by making a series of decisions based on the features of the input data. Here's a step-by-step explanation of how it works:\n",
    "\n",
    "**Step 1: Data Preparation:**\n",
    "- Gather your dataset, which should consist of samples with labeled class outcomes (e.g., \"Yes\" or \"No,\" \"1\" or \"0\").\n",
    "- Each sample should have features (attributes) that you will use to make predictions.\n",
    "\n",
    "**Step 2: Tree Construction:**\n",
    "1. **Select a Feature:**\n",
    "   - Choose a feature from your dataset that you believe could help in making predictions. This feature is used to split the dataset.\n",
    "\n",
    "2. **Determine the Split Threshold:**\n",
    "   - For numerical features, select a threshold value that will be used to split the data into two subsets.\n",
    "   - The choice of threshold is determined by finding the value that minimizes impurity (e.g., Gini impurity or entropy) after the split.\n",
    "\n",
    "3. **Split the Data:**\n",
    "   - Divide the dataset into two subsets based on the chosen feature and threshold.\n",
    "   - One subset contains samples that satisfy the condition (e.g., age < 30), and the other contains samples that don't satisfy the condition.\n",
    "\n",
    "4. **Repeat for Child Nodes:**\n",
    "   - For each subset created by the split, repeat the process (select feature, determine threshold, split data) to further partition the data.\n",
    "   - Continue this process recursively until a stopping criterion is met, such as reaching a maximum depth or a minimum number of samples in a node.\n",
    "\n",
    "**Step 3: Assign Class Labels:**\n",
    "- Once the tree construction is complete (or a stopping criterion is met), assign a class label to each leaf node based on the majority class of the samples in that node.\n",
    "- For instance, if most samples in a leaf node belong to class \"Yes,\" that node will be labeled as \"Yes.\"\n",
    "\n",
    "**Step 4: Prediction:**\n",
    "- To make a prediction for a new data point, start at the root node of the decision tree.\n",
    "- Evaluate the feature values of the data point against the feature and threshold at the root node.\n",
    "- Follow the appropriate branch (left or right) based on whether the condition is satisfied.\n",
    "- Repeat this process by traversing down the tree, following branches according to the feature values and thresholds.\n",
    "- Once you reach a leaf node, the class label associated with that node is your prediction for the new data point.\n",
    "\n",
    "**Step 5: Evaluation:**\n",
    "- After building the decision tree, use it to predict class labels for a validation or test dataset.\n",
    "- Compare the predicted labels with the actual labels in the dataset to assess the accuracy of the model.\n",
    "\n",
    "Decision trees are intuitive and easy to visualize, making them a popular choice for solving binary classification problems. However, they can be prone to overfitting, especially with complex and deep trees. Techniques like pruning (removing branches) or using ensemble methods like Random Forests can help mitigate this issue and improve the performance of decision tree classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132ccff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8e49a4",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ef4d83",
   "metadata": {},
   "source": [
    "The geometric intuition behind decision tree classification lies in the idea of recursively partitioning the feature space into regions corresponding to different class labels. Each decision or split in the tree corresponds to a division of the feature space along a specific feature axis, creating boundaries that separate data points of different classes. Let's explore this intuition further:\n",
    "\n",
    "**1. Splitting the Feature Space:**\n",
    "Imagine you have a two-dimensional feature space with two features, such as \"Age\" and \"Income.\" Each data point corresponds to a coordinate in this space. When you build a decision tree classifier, you start by choosing a feature and a threshold to split the data into two regions (left and right or top and bottom).\n",
    "\n",
    "**2. Decision Boundaries:**\n",
    "The split you make creates a decision boundary perpendicular to the selected feature axis. For example, if you choose \"Age\" as the feature and a threshold of 30, the decision boundary is a vertical line at age 30. Data points on one side of the boundary satisfy the condition (e.g., age < 30), and those on the other side do not.\n",
    "\n",
    "**3. Recursive Partitioning:**\n",
    "As you build the tree, you continue to split the feature space into smaller regions. Each split adds another decision boundary, further dividing the space. The goal is to create regions that are as pure as possible in terms of class labels.\n",
    "\n",
    "**4. Regions and Class Labels:**\n",
    "At each leaf node of the decision tree, you assign a class label based on the majority class of the data points within that region. The tree's structure and the arrangement of decision boundaries create distinct regions in the feature space, each associated with a specific class label.\n",
    "\n",
    "**5. Visualizing Decision Trees:**\n",
    "Visualizing a decision tree involves plotting the decision boundaries in the feature space. In a simple example with just two features, the decision boundaries are lines or curves. Each internal node of the tree corresponds to a decision boundary, and each leaf node corresponds to a region with a predicted class label.\n",
    "\n",
    "**6. Complexity and Overfitting:**\n",
    "The geometric intuition helps understand the complexity of decision trees. A complex tree with many decision boundaries can fit the training data very well, but it might also capture noise and lead to overfitting. Overfitting occurs when the tree adapts too closely to the training data and doesn't generalize well to new, unseen data.\n",
    "\n",
    "**7. Pruning and Regularization:**\n",
    "To avoid overfitting, it's often necessary to prune or trim the decision tree. Pruning involves removing certain branches or decision boundaries that don't contribute significantly to improving the overall predictive performance. This simplifies the tree's geometry and helps it generalize better.\n",
    "\n",
    "In summary, the geometric intuition behind decision tree classification involves creating decision boundaries in the feature space to partition it into regions associated with different class labels. This intuitive representation allows us to understand how the tree makes decisions and separates data points based on their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e2fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5174e145",
   "metadata": {},
   "source": [
    " how a decision tree classifier can be used to make predictions using a simple example.\n",
    "\n",
    "Suppose we have a binary classification problem where we want to predict whether a customer will buy a product based on two features: \"Age\" and \"Income\". We've built a decision tree with the following structure:\n",
    "\n",
    "```\n",
    "        Age < 30\n",
    "       /         \\\n",
    "  Income < 50000  Income >= 50000\n",
    "   /        \\           /         \\\n",
    "  Yes        No        No         Yes\n",
    "```\n",
    "\n",
    "Here's how we can use this tree to make predictions:\n",
    "\n",
    "1. **Starting at the Root:**\n",
    "   - Begin at the root node, which asks whether the customer's \"Age\" is less than 30.\n",
    "   - Let's say we have a new customer with an age of 25 and an income of $40,000.\n",
    "\n",
    "2. **Following the Decision Path:**\n",
    "   - Since the customer's age (25) is indeed less than 30, we follow the left branch to the next node, which asks whether their \"Income\" is less than $50,000.\n",
    "   - The customer's income is $40,000, which is indeed less than $50,000.\n",
    "\n",
    "3. **Reaching a Leaf Node:**\n",
    "   - We continue following the left branch to a leaf node with the label \"Yes\".\n",
    "   - This means the decision tree predicts that the customer will buy the product.\n",
    "\n",
    "In this example, the decision tree uses the customer's age and income to guide its decision-making process. It asks questions at each node based on the features and thresholds, and based on the answers, it traverses down the appropriate branches until it reaches a leaf node with a predicted class label.\n",
    "\n",
    "To generalize this process:\n",
    "\n",
    "- Start at the root node.\n",
    "- Evaluate the condition based on the feature and threshold.\n",
    "- Follow the corresponding branch (left or right) based on whether the condition is met.\n",
    "- Continue this process, moving through the tree until a leaf node is reached.\n",
    "- The class label associated with the leaf node is the prediction for the new data point.\n",
    "\n",
    "This approach makes the prediction process interpretable and allows you to trace the path of decisions that led to a particular prediction. However, keep in mind that while decision trees are easy to understand, they can sometimes be sensitive to small changes in the data and prone to overfitting. Regularization techniques like pruning or using ensemble methods like Random Forests can help mitigate these issues and improve the overall predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a7279d",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cfa454",
   "metadata": {},
   "source": [
    "The confusion matrix is a tabular representation that provides a comprehensive summary of the performance of a classification algorithm on a dataset. It's particularly useful for evaluating the performance of binary classification models (those that predict between two classes). The confusion matrix breaks down the predicted and actual class labels into four categories, allowing you to calculate various metrics for assessing the model's accuracy.\n",
    "\n",
    "In a confusion matrix, the four categories are:\n",
    "\n",
    "1. **True Positives (TP):**\n",
    "   - The number of instances that were correctly predicted as the positive class.\n",
    "\n",
    "2. **True Negatives (TN):**\n",
    "   - The number of instances that were correctly predicted as the negative class.\n",
    "\n",
    "3. **False Positives (FP):**\n",
    "   - The number of instances that were incorrectly predicted as the positive class (when they are actually negative).\n",
    "\n",
    "4. **False Negatives (FN):**\n",
    "   - The number of instances that were incorrectly predicted as the negative class (when they are actually positive).\n",
    "\n",
    "The confusion matrix is usually presented in the following format:\n",
    "\n",
    "```\n",
    "                  Predicted Positive   Predicted Negative\n",
    "Actual Positive       TP                  FN\n",
    "Actual Negative       FP                  TN\n",
    "```\n",
    "\n",
    "Using the values from the confusion matrix, you can calculate various evaluation metrics to assess the performance of a binary classification model:\n",
    "\n",
    "- **Accuracy:** The ratio of correct predictions (TP and TN) to the total number of instances. It provides an overall measure of the model's correctness.\n",
    "   - Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "- **Precision:** The ratio of true positive predictions to all positive predictions (both correct and incorrect). It measures how many of the predicted positive instances were actually positive.\n",
    "   - Precision = TP / (TP + FP)\n",
    "\n",
    "- **Recall (Sensitivity or True Positive Rate):** The ratio of true positive predictions to all actual positive instances. It measures how many of the actual positive instances were correctly predicted.\n",
    "   - Recall = TP / (TP + FN)\n",
    "\n",
    "- **F1-Score:** The harmonic mean of precision and recall. It's a balanced metric that takes both false positives and false negatives into account.\n",
    "   - F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "- **Specificity (True Negative Rate):** The ratio of true negative predictions to all actual negative instances. It measures how well the model predicts the negative class.\n",
    "   - Specificity = TN / (TN + FP)\n",
    "\n",
    "The confusion matrix and the associated metrics provide a more detailed understanding of a classifier's performance beyond simple accuracy, allowing you to assess its strengths and weaknesses in handling different classes and error types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1901b",
   "metadata": {},
   "source": [
    "The confusion matrix is a fundamental tool for evaluating the performance of a classification model. It provides detailed insights into how well the model is making predictions, where it's succeeding, and where it's making errors. Here's how you can use the confusion matrix to evaluate a classification model:\n",
    "\n",
    "**Step 1: Obtain Predictions and Actual Labels:**\n",
    "- First, you need the predictions made by your classification model and the actual class labels from your dataset.\n",
    "\n",
    "**Step 2: Construct the Confusion Matrix:**\n",
    "- Using the predictions and actual labels, populate the confusion matrix by categorizing the instances into four groups: True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "**Step 3: Calculate Evaluation Metrics:**\n",
    "Based on the values in the confusion matrix, you can calculate various evaluation metrics that provide insights into different aspects of the model's performance:\n",
    "\n",
    "1. **Accuracy:**\n",
    "   - Accuracy = (TP + TN) / Total Predictions\n",
    "   - It gives an overall measure of how often the model's predictions are correct.\n",
    "\n",
    "2. **Precision:**\n",
    "   - Precision = TP / (TP + FP)\n",
    "   - Precision focuses on the proportion of positive predictions that were correct. It's valuable when you want to avoid false positives.\n",
    "\n",
    "3. **Recall (Sensitivity or True Positive Rate):**\n",
    "   - Recall = TP / (TP + FN)\n",
    "   - Recall emphasizes the proportion of actual positive instances that were correctly predicted. It's useful when you want to avoid false negatives.\n",
    "\n",
    "4. **F1-Score:**\n",
    "   - F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "   - The F1-Score balances precision and recall. It's particularly useful when the classes are imbalanced or when both false positives and false negatives are equally important.\n",
    "\n",
    "5. **Specificity (True Negative Rate):**\n",
    "   - Specificity = TN / (TN + FP)\n",
    "   - Specificity highlights the model's ability to correctly predict the negative class.\n",
    "\n",
    "**Step 4: Interpretation:**\n",
    "- Analyze the evaluation metrics to understand how well the model is performing in terms of different aspects: overall accuracy, handling of positive and negative cases, trade-offs between precision and recall, and more.\n",
    "\n",
    "**Step 5: Consider the Context:**\n",
    "- The choice of evaluation metrics depends on the specific problem and the consequences of false positives and false negatives in your domain. For instance, in a medical context, false negatives might be more concerning than false positives.\n",
    "\n",
    "**Step 6: Iterate and Optimize:**\n",
    "- If the model's performance isn't satisfactory, you can adjust the model parameters, feature engineering, or even consider using different algorithms. The confusion matrix and associated metrics provide guidance on where improvements are needed.\n",
    "\n",
    "In summary, the confusion matrix is a versatile tool for evaluating classification models. It allows you to understand the model's strengths and weaknesses, and the associated metrics offer a comprehensive view of its performance across different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b721a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fc703ba",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 6 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8733f8",
   "metadata": {},
   "source": [
    " consider a binary classification problem where we want to predict whether an email is spam (positive class) or not spam (negative class). Here's an example confusion matrix based on the predictions made by a classification model:\n",
    "\n",
    "Assume we have 100 emails in our dataset, and the model makes the following predictions:\n",
    "\n",
    "- True Positives (TP): 30 emails were correctly predicted as spam.\n",
    "- True Negatives (TN): 50 emails were correctly predicted as not spam.\n",
    "- False Positives (FP): 10 emails were incorrectly predicted as spam when they were not.\n",
    "- False Negatives (FN): 10 emails were incorrectly predicted as not spam when they were actually spam.\n",
    "\n",
    "The confusion matrix would look like this:\n",
    "\n",
    "```\n",
    "                Predicted Spam   Predicted Not Spam\n",
    "Actual Spam          30                   10\n",
    "Actual Not Spam      10                   50\n",
    "```\n",
    "\n",
    "Based on this confusion matrix, we can calculate various evaluation metrics:\n",
    "\n",
    "- **Accuracy:** (TP + TN) / Total = (30 + 50) / 100 = 0.8 or 80%\n",
    "- **Precision:** TP / (TP + FP) = 30 / (30 + 10) = 0.75 or 75%\n",
    "- **Recall (Sensitivity):** TP / (TP + FN) = 30 / (30 + 10) = 0.75 or 75%\n",
    "- **F1-Score:** 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.75 * 0.75) / (0.75 + 0.75) = 0.75 or 75%\n",
    "- **Specificity (True Negative Rate):** TN / (TN + FP) = 50 / (50 + 10) = 0.8333 or 83.33%\n",
    "\n",
    "Interpreting these metrics:\n",
    "- The model has an overall accuracy of 80%, which means 80% of the predictions are correct.\n",
    "- The precision of 75% indicates that when the model predicts an email as spam, it's correct 75% of the time.\n",
    "- The recall of 75% indicates that the model identifies 75% of the actual spam emails.\n",
    "- The F1-Score considers both precision and recall, providing a balanced view of the model's performance.\n",
    "- The specificity of 83.33% means the model correctly identifies 83.33% of the actual not spam emails.\n",
    "\n",
    "This example illustrates how the confusion matrix and associated metrics help you evaluate the performance of a classification model in a binary classification scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de1276",
   "metadata": {},
   "source": [
    "Precision, Recall, and F1-Score are three important metrics used to evaluate the performance of classification models, especially in cases where the classes are imbalanced or where false positives and false negatives have different consequences. These metrics can be calculated from the confusion matrix.\n",
    "\n",
    "Let's consider the confusion matrix again:\n",
    "\n",
    "```\n",
    "                Predicted Spam   Predicted Not Spam\n",
    "Actual Spam          30                   10\n",
    "Actual Not Spam      10                   50\n",
    "```\n",
    "\n",
    "**1. Precision:**\n",
    "Precision measures how many of the instances predicted as positive (spam) are actually positive. It focuses on the correctness of positive predictions.\n",
    "\n",
    "Precision = TP / (TP + FP) = 30 / (30 + 10) = 0.75 or 75%\n",
    "\n",
    "In this case, out of the 40 instances predicted as spam, 30 were truly spam, resulting in a precision of 75%.\n",
    "\n",
    "**2. Recall (Sensitivity):**\n",
    "Recall measures how many of the actual positive instances were correctly predicted. It emphasizes the model's ability to identify all relevant instances.\n",
    "\n",
    "Recall = TP / (TP + FN) = 30 / (30 + 10) = 0.75 or 75%\n",
    "\n",
    "Out of the 40 actual spam instances, the model correctly identified 30, resulting in a recall of 75%.\n",
    "\n",
    "**3. F1-Score:**\n",
    "The F1-Score is the harmonic mean of precision and recall. It provides a balanced measure that takes both false positives and false negatives into account.\n",
    "\n",
    "F1-Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "         = 2 * (0.75 * 0.75) / (0.75 + 0.75) = 0.75 or 75%\n",
    "\n",
    "The F1-Score is useful when you want to balance the trade-off between precision and recall. It's especially valuable when you have imbalanced classes and you want to avoid situations where the model is good at predicting one class but not the other.\n",
    "\n",
    "In summary, precision, recall, and F1-Score provide insights into different aspects of a classification model's performance. Precision tells you how reliable the positive predictions are, recall tells you how well the model identifies positive instances, and F1-Score provides a balanced view of these two metrics. These metrics help you make informed decisions about the model's suitability for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6abfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056e58dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e85adfd2",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 7 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589a514c",
   "metadata": {},
   "source": [
    "Choosing an appropriate evaluation metric for a classification problem is crucial because it directly impacts how you assess and interpret the performance of your model. Different evaluation metrics highlight various aspects of the model's behavior, and the choice should align with the specific goals, characteristics of the problem, and potential consequences of incorrect predictions. Here's why selecting the right evaluation metric is so important:\n",
    "\n",
    "**1. Reflects Problem Context:**\n",
    "- Different classification problems have different priorities. For instance, in a medical diagnosis scenario, a false negative (missed diagnosis) might be more critical than a false positive (false alarm). The choice of metric should reflect the problem's context and the costs associated with different types of errors.\n",
    "\n",
    "**2. Addresses Class Imbalance:**\n",
    "- If your classes are imbalanced (one class has significantly more instances than the other), accuracy might not be an appropriate metric. In such cases, metrics like precision, recall, and F1-Score become more relevant, as they focus on the performance of individual classes.\n",
    "\n",
    "**3. Considers Trade-offs:**\n",
    "- Precision and recall trade off against each other. As you increase one, the other might decrease. The F1-Score strikes a balance between precision and recall, offering a single metric that considers both aspects. The choice of metric depends on how much emphasis you want to give to false positives and false negatives.\n",
    "\n",
    "**4. Prioritizes High-Stakes Situations:**\n",
    "- In situations where false negatives are costly (e.g., medical diagnoses), recall is crucial because it minimizes the chances of missing positive instances.\n",
    "- In scenarios where false positives are costly (e.g., fraud detection), precision is important to reduce false alarms.\n",
    "\n",
    "**5. Guides Model Selection and Tuning:**\n",
    "- The evaluation metric you choose often guides the process of model selection and hyperparameter tuning. Different models might excel based on different metrics. For example, an algorithm that achieves high accuracy might not necessarily perform well in terms of precision or recall.\n",
    "\n",
    "**6. Avoids Biased or Misleading Conclusions:**\n",
    "- Relying solely on one metric might lead to biased conclusions about a model's performance. For example, a model with high accuracy might have achieved it by favoring the majority class, while completely ignoring the minority class.\n",
    "\n",
    "**7. Measures Real-World Impact:**\n",
    "- An appropriate metric should align with the real-world impact of your predictions. It's not just about how well the model performs in isolation, but how well its predictions translate to tangible outcomes.\n",
    "\n",
    "**8. Communicates Results Effectively:**\n",
    "- When presenting your results to stakeholders or decision-makers, using an appropriate metric ensures that you convey the model's performance accurately and understandably.\n",
    "\n",
    "In summary, the importance of choosing the right evaluation metric for a classification problem cannot be overstated. It affects your understanding of the model's strengths and weaknesses, guides your decision-making process, and ultimately determines whether the model's predictions align with your problem's objectives and constraints. Always consider the problem's context, class distribution, and the potential consequences of different types of errors when selecting an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd6c6f",
   "metadata": {},
   "source": [
    " Choosing the right evaluation metric for a classification problem involves a thoughtful consideration of your problem's context, priorities, and the trade-offs between different aspects of the model's performance. Here's a step-by-step approach to guide you through the process:\n",
    "\n",
    "**Step 1: Understand the Problem Context:**\n",
    "- Begin by gaining a deep understanding of the problem you're trying to solve. Consider the domain, the nature of the classes, and the potential consequences of false positives and false negatives.\n",
    "\n",
    "**Step 2: Define Evaluation Goals:**\n",
    "- Clearly define what you want to achieve with your classification model. Are you more concerned about minimizing false negatives, false positives, or achieving a balance between the two?\n",
    "\n",
    "**Step 3: Identify the Key Metric:**\n",
    "- Based on your goals, identify the key metric that aligns with your priorities:\n",
    "  - **Accuracy:** Suitable when the class distribution is roughly balanced and false positives and false negatives have similar costs.\n",
    "  - **Precision:** Useful when false positives are costly or when you want to minimize incorrect positive predictions.\n",
    "  - **Recall (Sensitivity):** Important when false negatives are costly or when you want to ensure you're capturing most positive instances.\n",
    "  - **F1-Score:** A balanced metric that combines precision and recall, useful when you want to consider both types of errors.\n",
    "\n",
    "**Step 4: Consider Class Imbalance:**\n",
    "- If your classes are imbalanced, accuracy might not be a reliable metric. Focus on precision, recall, and F1-Score, as they account for the performance of individual classes.\n",
    "\n",
    "**Step 5: Understand the Trade-offs:**\n",
    "- Understand the trade-offs between different metrics. For example, increasing recall might lead to lower precision and vice versa. Evaluate which trade-offs are acceptable given your problem's priorities.\n",
    "\n",
    "**Step 6: Evaluate Consequences:**\n",
    "- Evaluate the consequences of making incorrect predictions for both classes. Consider which type of error (false positive or false negative) has more significant real-world implications.\n",
    "\n",
    "**Step 7: Experiment and Compare:**\n",
    "- It's often a good idea to try multiple metrics and observe how they behave on your specific dataset. Experiment with different models and parameters to see how the metrics change.\n",
    "\n",
    "**Step 8: Document Your Decision:**\n",
    "- Clearly document your choice of evaluation metric and the reasoning behind it. This documentation is important for transparency, especially when communicating results to stakeholders.\n",
    "\n",
    "**Step 9: Iterate and Refine:**\n",
    "- As you gain more insights into your problem and experiment with different models, be open to refining your choice of metric. It's okay to reevaluate your decision as you learn more.\n",
    "\n",
    "**Step 10: Consider the Big Picture:**\n",
    "- Remember that choosing an appropriate evaluation metric is not just about finding the best number. It's about aligning your model's performance with the real-world impact and goals of your project.\n",
    "\n",
    "In summary, choosing the right evaluation metric involves a thoughtful process that considers the problem's context, class distribution, and consequences of different errors. It ensures that your model's performance evaluation is meaningful and relevant to the problem you're trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b169a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f479cd0",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 8 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cc594b",
   "metadata": {},
   "source": [
    "One example of a classification problem where precision is the most important metric is fraud detection in financial transactions. In this scenario, the goal is to accurately identify fraudulent transactions while minimizing the number of false positives (legitimate transactions incorrectly flagged as fraud). Precision becomes crucial in this context due to the following reasons:\n",
    "\n",
    "**Problem Context:**\n",
    "- In fraud detection, the cost of false positives can be substantial. If a legitimate transaction is mistakenly classified as fraud, it could inconvenience the user, potentially leading to account suspension, declined transactions, or customer dissatisfaction.\n",
    "\n",
    "**Priority:**\n",
    "- The primary concern is to prevent false alarms. The financial institution wants to minimize the disruption caused by incorrectly flagging legitimate transactions as fraud.\n",
    "\n",
    "**Consequences of False Positives:**\n",
    "- False positives can lead to frustration for customers, loss of business, and a negative impact on customer relationships.\n",
    "\n",
    "**Imbalanced Class Distribution:**\n",
    "- The number of fraudulent transactions is usually much lower than the number of legitimate transactions. This imbalance makes precision more important as it ensures that the identified fraud cases are highly likely to be accurate.\n",
    "\n",
    "**Mitigating False Positives:**\n",
    "- By focusing on precision, the model aims to be confident in its predictions before flagging a transaction as fraudulent. This cautious approach reduces the chances of false positives.\n",
    "\n",
    "**Example:**\n",
    "Suppose a credit card company wants to build a fraud detection system. Out of a dataset of 100,000 transactions, only 1,000 are actually fraudulent. The company doesn't want to inconvenience customers with false alarms, so they prioritize precision.\n",
    "\n",
    "They train a model that achieves the following confusion matrix:\n",
    "\n",
    "```\n",
    "                Predicted Fraud   Predicted Not Fraud\n",
    "Actual Fraud          900                   100\n",
    "Actual Not Fraud      200                  98,800\n",
    "```\n",
    "\n",
    "From the confusion matrix:\n",
    "\n",
    "- Precision = TP / (TP + FP) = 900 / (900 + 200) â‰ˆ 0.8182 or 81.82%\n",
    "\n",
    "The high precision indicates that when the model predicts a transaction as fraud, it's correct about 81.82% of the time. This means that only a relatively small number of the transactions flagged as fraud are actually false positives. The focus on precision helps the financial institution maintain customer trust and minimize unnecessary disruptions.\n",
    "\n",
    "In this example, the high precision achieved by the model is aligned with the problem's context, priorities, and the goal of reducing the impact of false positives in fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bce148",
   "metadata": {},
   "source": [
    "Precision is a critical metric in the context of fraud detection for several important reasons:\n",
    "\n",
    "1. **Cost of False Positives:** In fraud detection scenarios, false positives have tangible consequences. When a legitimate transaction is incorrectly flagged as fraudulent, it can lead to customer frustration, inconvenience, and potential financial repercussions. Customers might experience declined transactions, account suspensions, or even reputational damage for the financial institution.\n",
    "\n",
    "2. **Customer Experience:** Ensuring a positive customer experience is vital for financial institutions. False positives can erode customer trust and satisfaction. Customers may become hesitant to use their cards or conduct transactions if they fear their legitimate activities might be wrongly flagged as fraud.\n",
    "\n",
    "3. **Imbalanced Class Distribution:** Fraudulent transactions are typically a tiny fraction of the total transactions. This class imbalance means that a model that predicts every transaction as legitimate could have a high accuracy, but it would be utterly ineffective for fraud detection. Precision becomes crucial in such cases to ensure that the flagged fraud cases are highly reliable.\n",
    "\n",
    "4. **Resource Allocation:** Investigating and resolving flagged transactions requires human intervention and resources. Having a high-precision model means that investigators will spend more of their time on cases that are genuinely suspicious, improving the overall efficiency of fraud prevention efforts.\n",
    "\n",
    "5. **Regulatory Compliance:** Financial institutions often have obligations to accurately identify and report fraudulent activities to regulatory bodies. A high-precision model reduces the risk of incorrect reporting and compliance issues.\n",
    "\n",
    "6. **Loss Prevention:** False positives can lead to lost business opportunities if legitimate customers are discouraged from using their cards due to fears of fraud detection. High precision helps prevent these losses by minimizing false alarms.\n",
    "\n",
    "In summary, in fraud detection, precision takes center stage due to the substantial consequences of false positives. Prioritizing precision ensures that the flagged cases are more likely to be actual instances of fraud, reducing customer impact, maintaining trust, optimizing resource allocation, and complying with regulatory requirements. It's a strategic choice that aligns with the goal of protecting both customers and the financial institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6a26a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "512dff4a",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 9 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c4c36",
   "metadata": {},
   "source": [
    "A prime example of a classification problem where recall is the most important metric is medical diagnoses for a rare and severe disease. Let's consider a scenario involving the early detection of a life-threatening illness, where prompt identification is crucial for effective treatment and patient outcomes. In this context, recall takes precedence for several reasons:\n",
    "\n",
    "**Problem Context:**\n",
    "- Early detection of a severe disease can significantly impact patient prognosis and outcomes. Missing even a single case could lead to dire consequences.\n",
    "\n",
    "**Priority:**\n",
    "- The primary concern is to identify all cases of the disease, even if it means potentially having more false positives. The emphasis is on catching as many positive instances as possible.\n",
    "\n",
    "**Consequences of False Negatives:**\n",
    "- False negatives in this context can be catastrophic. If a patient with the disease is mistakenly classified as negative, they might not receive timely medical intervention, leading to worsened health or even death.\n",
    "\n",
    "**Class Imbalance:**\n",
    "- Severe diseases are often rare, resulting in an imbalanced dataset. In such cases, a high recall ensures that the model detects the rare positive cases effectively.\n",
    "\n",
    "**Example:**\n",
    "Suppose there's a rare and aggressive cancer that affects only 1 in 1,000 patients. The medical community wants to develop a diagnostic model that can identify this cancer early. They prioritize recall to ensure that as many cases as possible are detected, even if it means potentially raising false alarms.\n",
    "\n",
    "They train a model that produces the following confusion matrix:\n",
    "\n",
    "```\n",
    "                Predicted Positive   Predicted Negative\n",
    "Actual Positive       800                  200\n",
    "Actual Negative       1                    998\n",
    "```\n",
    "\n",
    "From the confusion matrix:\n",
    "\n",
    "- Recall = TP / (TP + FN) = 800 / (800 + 1) â‰ˆ 0.99875 or 99.88%\n",
    "\n",
    "The high recall indicates that the model effectively identifies 99.88% of the actual positive cases. In this context, it's critical to prioritize recall, as the goal is to identify every positive instance, even if it leads to some false positives. The focus on recall helps ensure that potentially life-saving interventions are initiated for as many patients as possible.\n",
    "\n",
    "In this example, recall is the most important metric because it aligns with the critical goal of early disease detection and intervention, mitigating the risks associated with false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d25592",
   "metadata": {},
   "source": [
    "In the context of a classification problem involving the early detection of a rare and severe disease, prioritizing recall as the most important metric is driven by the critical nature of the problem and the potential consequences of missing positive cases. Here's why recall takes precedence in such scenarios:\n",
    "\n",
    "1. **Severe Consequences of Missed Cases:** In medical situations where the disease is severe, rare, and potentially life-threatening, missing a positive case (false negative) can have dire consequences. Early detection and intervention can significantly improve patient outcomes and survival rates. Thus, ensuring that as many positive cases as possible are detected becomes of paramount importance.\n",
    "\n",
    "2. **Patient Well-being:** In medical contexts, the primary goal is to provide the best possible care to patients. Prioritizing recall means that the model aims to identify all true positive cases, ensuring that patients receive the necessary treatment and care in a timely manner.\n",
    "\n",
    "3. **Class Imbalance and Rarity:** Diseases that are rare are often associated with imbalanced datasets, where the number of positive cases is much smaller than the number of negative cases. In such cases, a high recall ensures that even the rare positive cases are not missed, mitigating the impact of class imbalance.\n",
    "\n",
    "4. **Trade-off with False Positives:** In prioritizing recall, the model might be more sensitive to potential false positives (incorrectly classifying a negative case as positive). While false positives are a concern, they can be managed with further testing and follow-up, which is a more acceptable scenario compared to missing a true positive case.\n",
    "\n",
    "5. **Ethical and Moral Imperatives:** In the medical field, there is an ethical responsibility to prioritize patient well-being. Ensuring that patients who might be affected by a severe disease are identified and treated early aligns with these ethical principles.\n",
    "\n",
    "6. **Medical Guidelines and Practices:** In many medical fields, guidelines and practices are centered around early detection and intervention for severe diseases. Prioritizing recall aligns with these guidelines and ensures that medical professionals have the best chance to intervene effectively.\n",
    "\n",
    "7. **Regulatory and Legal Considerations:** Medical diagnostics often involve regulatory and legal considerations. Prioritizing recall ensures that a medical institution is fulfilling its duty of care to patients and adhering to relevant regulations.\n",
    "\n",
    "In summary, in scenarios involving rare and severe diseases, recall takes precedence as the most important metric. The goal is to maximize the detection of true positive cases, even if it means potentially accepting a higher number of false positives. This approach reflects the critical nature of the problem and the responsibility to ensure patient well-being and early intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6253f567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd6f2ae",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226d2a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac433118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ecf52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5f1fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27258b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba6f208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd9608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
