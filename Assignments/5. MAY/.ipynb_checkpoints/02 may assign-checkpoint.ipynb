{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5515dc3d",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019bc05a",
   "metadata": {},
   "source": [
    "Anomaly detection, also known as outlier detection, is a process in data analysis and machine learning that involves identifying observations or instances that deviate significantly from the expected or \"normal\" behavior within a dataset. Anomalies are data points that do not conform to the majority of the data and are often of interest due to their potential to represent unique events, errors, fraud, or important insights.\n",
    "\n",
    "The purpose of anomaly detection is to:\n",
    "\n",
    "1. **Detect Unusual Events**: Anomaly detection helps in identifying rare and unusual events that may be critical or informative, such as fraudulent transactions, defects in manufacturing, or equipment malfunctions.\n",
    "\n",
    "2. **Maintain Data Quality**: Anomalies may indicate data quality issues, measurement errors, or outliers. Detecting and addressing these anomalies is important for maintaining accurate and reliable datasets.\n",
    "\n",
    "3. **Early Warning System**: Anomaly detection can serve as an early warning system for potential issues, allowing timely interventions and preventive actions.\n",
    "\n",
    "4. **Identify Patterns**: Anomalies can reveal hidden patterns or trends in the data that might not be apparent through traditional analysis.\n",
    "\n",
    "5. **Fraud Detection**: In various domains, such as finance and cybersecurity, anomaly detection is used to identify fraudulent activities, unauthorized access, or abnormal behavior.\n",
    "\n",
    "6. **Healthcare**: Anomaly detection can help in early detection of diseases by identifying unusual patient data or medical conditions.\n",
    "\n",
    "7. **Environmental Monitoring**: Anomalies in environmental or sensor data can signal events like earthquakes, pollution spikes, or climate changes.\n",
    "\n",
    "8. **Quality Control**: In manufacturing, anomaly detection is used to identify defects or variations in product quality.\n",
    "\n",
    "Anomaly detection methods can be based on various techniques, including statistical methods, machine learning algorithms, and domain-specific rules. These methods aim to automatically identify anomalies without the need for manual inspection and can be applied to various types of data, such as time series, images, and structured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf926304",
   "metadata": {},
   "source": [
    "The purpose of anomaly detection is to identify and flag instances in a dataset that significantly deviate from the expected or normal behavior. Anomalies, also known as outliers, are data points that do not conform to the patterns or distributions observed in the majority of the data. The main goals of anomaly detection are:\n",
    "\n",
    "1. **Identify Unusual Events**: Anomaly detection helps in identifying rare and unusual events or observations that stand out from the regular patterns in the data. These events could be indicative of critical incidents, errors, fraud, or potential opportunities.\n",
    "\n",
    "2. **Maintain Data Quality**: Anomalies can be a result of data quality issues, measurement errors, or noisy data. Detecting and addressing these anomalies is crucial for maintaining the accuracy and reliability of the dataset.\n",
    "\n",
    "3. **Early Detection**: Anomaly detection serves as an early warning system by identifying deviations from normal behavior before they escalate into larger issues. This can be especially useful in industries such as finance, healthcare, and manufacturing.\n",
    "\n",
    "4. **Discover Hidden Patterns**: Anomalies can sometimes reveal hidden patterns or insights in the data that might not be apparent through traditional analysis. They might indicate new trends, emerging behaviors, or relationships.\n",
    "\n",
    "5. **Fraud Detection**: In applications like financial transactions, anomaly detection is used to identify potentially fraudulent activities. Transactions that deviate significantly from the usual spending patterns can be flagged for further investigation.\n",
    "\n",
    "6. **Quality Control**: In manufacturing and production processes, anomaly detection is used to identify defects, variations, or anomalies in products. This helps in maintaining product quality and reducing waste.\n",
    "\n",
    "7. **Cybersecurity**: Anomaly detection is crucial for identifying unusual behavior or intrusions in computer networks. It can detect unauthorized access, unusual data traffic, or abnormal user activities.\n",
    "\n",
    "8. **Healthcare**: Anomaly detection can be used to identify unusual patient data or medical conditions. It aids in early detection of diseases or health issues.\n",
    "\n",
    "9. **Environmental Monitoring**: Anomaly detection is used to identify unusual patterns in environmental data, such as pollution spikes or unusual weather events.\n",
    "\n",
    "10. **Predictive Maintenance**: In industrial settings, anomaly detection is used for predictive maintenance, identifying anomalies in sensor data to anticipate equipment failures and prevent downtime.\n",
    "\n",
    "The main idea is to automatically identify anomalies in a way that minimizes false positives and false negatives, providing actionable insights for decision-making and problem-solving. Anomaly detection methods can range from simple threshold-based approaches to complex machine learning algorithms that capture intricate patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112f881",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd7190",
   "metadata": {},
   "source": [
    "Anomaly detection presents several challenges due to its inherent complexity and the diverse nature of data. Some key challenges include:\n",
    "\n",
    "1. **Unlabeled Data**: Anomaly detection often deals with data that lacks explicit labels for anomalies. This unsupervised setting makes it challenging to train and evaluate models effectively.\n",
    "\n",
    "2. **Imbalanced Data**: Anomalies are typically rare events compared to the majority of normal data. This imbalance can lead to models biased toward normal instances and difficulties in accurately detecting anomalies.\n",
    "\n",
    "3. **Feature Selection**: Choosing relevant features for anomaly detection is crucial. However, selecting the right features that capture the underlying patterns while ignoring noise can be challenging, especially in high-dimensional data.\n",
    "\n",
    "4. **Dynamic and Evolving Patterns**: Anomalies can exhibit dynamic and evolving behaviors over time, making it difficult to define a fixed normal behavior model. The model needs to adapt to changing conditions.\n",
    "\n",
    "5. **Ambiguity**: Anomalies can be ambiguous, where some instances might appear normal in isolation but are anomalies when considered collectively. Capturing such contextual anomalies is challenging.\n",
    "\n",
    "6. **Novel Anomalies**: Anomaly detection should be able to identify novel or previously unseen anomalies. Traditional methods might struggle with such cases if they're not part of the training data.\n",
    "\n",
    "7. **Scalability**: Scalability becomes an issue when dealing with large datasets. Efficient algorithms are required to process and detect anomalies in real-time or near real-time.\n",
    "\n",
    "8. **Interpretability**: Many modern anomaly detection algorithms are complex and difficult to interpret. Understanding why a certain instance is flagged as an anomaly is important for decision-making.\n",
    "\n",
    "9. **Labeling Anomalies**: In cases where some anomalies are known, labeling them for training purposes can be challenging due to their rarity or the lack of clear criteria.\n",
    "\n",
    "10. **False Positives and Negatives**: Balancing the trade-off between false positives (normal instances flagged as anomalies) and false negatives (anomalies not detected) is crucial. Lowering one can lead to an increase in the other.\n",
    "\n",
    "11. **Domain-specific Challenges**: Different domains have unique challenges, data types, and characteristics. Adapting anomaly detection techniques to these specific contexts can be complex.\n",
    "\n",
    "12. **Anomaly Types**: Anomalies can vary widely in their nature, including point anomalies, contextual anomalies, collective anomalies, and more. One method may not be suitable for all types of anomalies.\n",
    "\n",
    "13. **Feature Engineering**: Choosing appropriate features and engineering them effectively is essential. Features that work well for one dataset might not be suitable for another.\n",
    "\n",
    "14. **Evaluation Metrics**: Defining appropriate evaluation metrics for anomaly detection can be challenging, especially when dealing with imbalanced data and different types of anomalies.\n",
    "\n",
    "Addressing these challenges often requires a combination of domain expertise, careful data preprocessing, selecting appropriate algorithms, and fine-tuning model parameters. Anomaly detection is an ongoing research area, and new methods and techniques are constantly being developed to tackle these challenges effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be62292",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "529bdc35",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85bcd07",
   "metadata": {},
   "source": [
    "Unsupervised and supervised anomaly detection approaches differ in their approach to identifying anomalies within a dataset:\n",
    "\n",
    "1. **Unsupervised Anomaly Detection:**\n",
    "   - **Training Data:** Unsupervised anomaly detection operates on datasets without explicit anomaly labels. It assumes that anomalies are rare and deviate from the norm.\n",
    "   - **Learning Behavior:** Models learn the inherent structure of the data, focusing on patterns and relationships among instances. Common techniques include clustering, density-based methods, and reconstruction-based methods like autoencoders.\n",
    "   - **Detection Process:** Unsupervised methods detect anomalies by identifying instances that deviate significantly from the expected data distribution. They often rely on statistical measures, density estimation, or reconstruction errors to flag anomalies.\n",
    "   - **Applicability:** Unsupervised methods are useful when you have limited or no labeled anomaly data and want to discover unknown anomalies or novel patterns.\n",
    "\n",
    "2. **Supervised Anomaly Detection:**\n",
    "   - **Training Data:** Supervised anomaly detection uses datasets with explicit labels, indicating which instances are anomalies and which are not.\n",
    "   - **Learning Behavior:** Models learn from labeled data to discriminate between normal and anomalous instances. Common techniques include classification algorithms like decision trees, random forests, and support vector machines (SVMs).\n",
    "   - **Detection Process:** Supervised methods predict whether a given instance is normal or anomalous based on the learned classification model. They assign a binary label (normal/anomalous) to each instance.\n",
    "   - **Applicability:** Supervised methods are useful when you have labeled data for both normal and anomalous instances. They are suitable for known types of anomalies and when you want to achieve high accuracy in detecting anomalies.\n",
    "\n",
    "Key Differences:\n",
    "- **Labeled Data:** Unsupervised methods don't require labeled anomaly data, making them suitable for discovering novel or previously unknown anomalies. Supervised methods rely on labeled data for training.\n",
    "- **Learning Approach:** Unsupervised methods learn patterns and structures within the entire dataset without explicitly distinguishing between normal and anomalous instances. Supervised methods focus on learning the specific characteristics of anomalies from labeled examples.\n",
    "- **Detection Output:** Unsupervised methods provide scores or measures of anomaly likelihood based on deviations from the expected distribution. Supervised methods provide binary classifications (normal or anomalous) for each instance.\n",
    "- **Novelty vs. Known Anomalies:** Unsupervised methods excel at detecting novel anomalies, while supervised methods are better suited for detecting known types of anomalies.\n",
    "- **Data Availability:** Unsupervised methods can be applied to scenarios where labeled anomaly data is scarce or unavailable. Supervised methods require sufficient labeled data for training.\n",
    "- **Performance:** Supervised methods can achieve higher accuracy for known types of anomalies, provided there's enough labeled data. Unsupervised methods might have limitations in accurately identifying subtle or contextual anomalies.\n",
    "\n",
    "The choice between unsupervised and supervised anomaly detection depends on factors such as the availability of labeled data, the nature of anomalies, and the desired trade-offs between novelty detection and precision. In some cases, a combination of both approaches (semi-supervised methods) can be used to leverage the benefits of both worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da9b704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0e68fb7",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae0f2b",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be categorized into the following main categories:\n",
    "\n",
    "1. **Statistical Methods:**\n",
    "   - **Z-Score/Standard Deviation:** Measures the number of standard deviations an instance is away from the mean. Instances with high z-scores are considered anomalies.\n",
    "   - **Percentile/Quantile:** Determines the threshold at a certain percentile of the data distribution. Instances beyond this threshold are considered anomalies.\n",
    "   - **Box Plot/IQR (Interquartile Range):** Uses the median and the spread between quartiles to identify outliers beyond a certain range.\n",
    "\n",
    "2. **Density-Based Methods:**\n",
    "   - **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** Clusters dense regions and treats low-density regions as anomalies.\n",
    "   - **LOF (Local Outlier Factor):** Compares the density of instances around a point to the density around its neighbors. Points with significantly lower densities are anomalies.\n",
    "\n",
    "3. **Clustering Methods:**\n",
    "   - **K-Means Clustering:** Instances far from cluster centers are considered anomalies.\n",
    "   - **Hierarchical Clustering:** Instances in small or isolated clusters may be considered anomalies.\n",
    "\n",
    "4. **Distance-Based Methods:**\n",
    "   - **Mahalanobis Distance:** Measures the distance of a point from the center of the distribution in terms of standard deviations.\n",
    "   - **K-Nearest Neighbors (KNN):** Measures the distance to its k-nearest neighbors. Instances with high distances can be considered anomalies.\n",
    "\n",
    "5. **Reconstruction-Based Methods:**\n",
    "   - **Autoencoders:** Neural networks that learn to reconstruct input data. Instances with high reconstruction errors are anomalies.\n",
    "   - **PCA Reconstruction Error:** Uses the reconstruction error of PCA-transformed data to detect anomalies.\n",
    "\n",
    "6. **Model-Based Methods:**\n",
    "   - **One-Class SVM:** Trains an SVM on the normal instances and classifies others as anomalies.\n",
    "   - **Isolation Forest:** Builds random forests to isolate instances faster and in fewer splits.\n",
    "\n",
    "7. **Ensemble Methods:**\n",
    "   - **Isolation Forest and Random Forest:** Combine multiple models to identify anomalies.\n",
    "   - **Robust Random Cut Forest:** Uses a collection of trees to detect anomalies in high-dimensional data.\n",
    "\n",
    "8. **Deep Learning Methods:**\n",
    "   - **Variational Autoencoders (VAEs):** Uses generative models to learn the distribution of normal data and detects anomalies based on deviations.\n",
    "   - **Generative Adversarial Networks (GANs):** Detects anomalies as instances that cannot be well-reconstructed by the generator network.\n",
    "\n",
    "Each category has its strengths and weaknesses, and the choice of algorithm depends on factors such as the nature of the data, the type of anomalies, available labeled data, computational resources, and desired accuracy. In practice, a combination of different algorithms or techniques may be used to achieve more robust anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f73048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0980e2b4",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6137fb12",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make the following main assumptions:\n",
    "\n",
    "1. **Distribution of Data:** These methods assume that the majority of the data instances are part of the normal distribution, while anomalies are significantly different from this distribution.\n",
    "\n",
    "2. **Distance Metric:** They assume that a meaningful distance metric can be used to measure the similarity or dissimilarity between instances. Common distance metrics include Euclidean distance, Manhattan distance, and Mahalanobis distance.\n",
    "\n",
    "3. **Normal Data Concentration:** These methods assume that normal data instances are concentrated in certain regions of the feature space, while anomalies are sparse or distributed differently.\n",
    "\n",
    "4. **Threshold Setting:** Many distance-based methods involve setting a distance threshold beyond which instances are considered anomalies. This assumes that there is a clear separation between normal and anomalous instances in the distance metric.\n",
    "\n",
    "5. **Cluster Separation:** Some distance-based methods, like K-Nearest Neighbors (KNN), assume that anomalies are located in regions of lower-density or farther away from clusters of normal instances.\n",
    "\n",
    "6. **Local Density Estimation:** Algorithms like Local Outlier Factor (LOF) assume that normal instances have similar local densities in their neighborhoods, while anomalies have significantly different local densities.\n",
    "\n",
    "7. **Data Distribution:** For methods that use statistical properties like z-scores or percentiles, they assume that the data distribution is known or can be approximated by a known distribution (e.g., Gaussian distribution).\n",
    "\n",
    "It's important to note that these assumptions may not hold true for all datasets and scenarios. Distance-based methods can be sensitive to the choice of distance metric and parameters, and their effectiveness can vary based on the underlying distribution of the data. Therefore, careful analysis and validation are necessary when applying distance-based anomaly detection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c2d844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b8b8288",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 6 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43ef31",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores by comparing the density of a data instance to the densities of its k-nearest neighbors. Here's how LOF computes anomaly scores:\n",
    "\n",
    "1. **Calculate Local Reachability Density (LRD):**\n",
    "   For each data point, calculate its Local Reachability Density (LRD). LRD measures the density of a point compared to its neighbors. It's defined as the inverse of the average reachability distance of a point to its k-nearest neighbors. The reachability distance between two points is the maximum of their Euclidean distance and the distance of the k-nearest neighbor to the second point.\n",
    "\n",
    "2. **Calculate Local Outlier Factor (LOF):**\n",
    "   For each data point, calculate its Local Outlier Factor (LOF). LOF compares the LRD of a point to the average LRD of its k-nearest neighbors. A point with an LOF significantly greater than 1 indicates that its density is lower than that of its neighbors, making it a potential outlier.\n",
    "\n",
    "   LOF(Point) = (Sum of LRD of k-nearest neighbors) / (LRD of Point * k)\n",
    "\n",
    "3. **Interpretation of LOF Scores:**\n",
    "   - LOF ≈ 1: The point's density is similar to its neighbors.\n",
    "   - LOF > 1: The point's density is lower than its neighbors, potentially indicating an outlier.\n",
    "   - LOF < 1: The point's density is higher than its neighbors, indicating it is in a dense region.\n",
    "\n",
    "Higher LOF values suggest that the point is less similar to its neighbors in terms of density, making it more likely to be an anomaly. The LOF value provides a measure of how much the point's behavior deviates from that of its neighbors.\n",
    "\n",
    "It's important to choose an appropriate value for the parameter k (number of nearest neighbors) based on the dataset characteristics. Additionally, LOF is sensitive to the choice of distance metric and may require careful parameter tuning for optimal results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ca463c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed591944",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 7 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5d9628",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm has two main hyperparameters:\n",
    "\n",
    "1. **n_estimators (default = 100):**\n",
    "   This parameter determines the number of isolation trees to create. Each isolation tree is a sub-sample of the original dataset and contributes to the final anomaly score. Increasing the number of trees can improve the performance of the algorithm but also increases computation time.\n",
    "\n",
    "2. **max_samples (default = 'auto'):**\n",
    "   This parameter controls the number of samples to be used for building each individual isolation tree. It can be set as an integer value (e.g., 256) or as a float (e.g., 0.5), which represents the fraction of the dataset to be used. Smaller values can lead to faster execution but might result in less accurate anomaly scores.\n",
    "\n",
    "Other optional parameters include:\n",
    "\n",
    "3. **contamination (default = 'auto'):**\n",
    "   This parameter specifies the expected proportion of anomalies in the dataset. If set to 'auto', the algorithm estimates the contamination based on the proportion of outliers in the data. You can also set it to a specific value if you have prior knowledge about the proportion of anomalies.\n",
    "\n",
    "4. **max_features (default = 1.0):**\n",
    "   This parameter controls the maximum number of features to consider when splitting a node in an isolation tree. Setting it to a smaller value can speed up the training process.\n",
    "\n",
    "5. **bootstrap (default = False):**\n",
    "   If set to True, the samples used for building individual trees are drawn with replacement.\n",
    "\n",
    "6. **random_state:**\n",
    "   This parameter controls the random seed for reproducibility.\n",
    "\n",
    "7. **n_jobs:**\n",
    "   The number of parallel jobs to run for building trees. Setting it to -1 uses all available processors.\n",
    "\n",
    "Keep in mind that the appropriate values for these parameters depend on the characteristics of your dataset and the specific problem you're trying to solve. Experimentation and cross-validation can help you find the best parameter values for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26e320f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9bd8c533",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 8 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515dae53",
   "metadata": {},
   "source": [
    "In the context of KNN-based anomaly detection, the anomaly score for a data point is often calculated based on the distance to its K-nearest neighbors. In your case, if a data point has only 2 neighbors of the same class within a radius of 0.5, and you're using KNN with K=10, here's how you could calculate its anomaly score:\n",
    "\n",
    "1. Calculate the distances from the data point to its 10 nearest neighbors.\n",
    "2. Since the data point has only 2 neighbors within a radius of 0.5, the other 8 neighbors would be farther away than the radius. Let's assume the distances to the 10 neighbors are [0.3, 0.4, 0.6, ..., 0.8].\n",
    "3. Normalize the distances so that they are within the range [0, 1]. You can use the Min-Max scaling formula for normalization.\n",
    "4. Calculate the anomaly score based on the normalized distances. One common approach is to use the mean or median of the normalized distances as the anomaly score. Lower scores indicate more anomalous points.\n",
    "\n",
    "Let's assume you use the mean of the normalized distances as the anomaly score:\n",
    "\n",
    "```\n",
    "Normalized Distances: [0.0, 0.1, 0.3, 0.5, 0.7, 0.8, 1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "Anomaly Score = Mean(Normalized Distances) = (0.0 + 0.1 + 0.3 + 0.5 + 0.7 + 0.8 + 1.0 + 1.0 + 1.0 + 1.0) / 10 = 0.63\n",
    "```\n",
    "\n",
    "In this example, the calculated anomaly score for the data point would be 0.63, indicating that it is relatively anomalous compared to its K-nearest neighbors. Keep in mind that this is a simplified calculation, and different variations of KNN-based anomaly detection algorithms might use slightly different formulas for calculating anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6753d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98954137",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 9 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394599c",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score for a data point is calculated based on its average path length in the isolation trees. The idea is that anomalies will have shorter average path lengths compared to normal data points. The average path length is a measure of how quickly a data point is isolated from other points in the trees.\n",
    "\n",
    "Given that you have 100 trees and a dataset of 3000 data points, and you want to calculate the anomaly score for a data point with an average path length of 5.0, compared to the average path length of the trees, you can use the following steps:\n",
    "\n",
    "1. Calculate the average path length for the trees: In each tree, you traverse the tree from the root node to a leaf node, and the average path length is the average depth of the leaf nodes reached by the data points across all trees.\n",
    "\n",
    "2. Compare the average path length of the data point with the average path length of the trees: If the average path length of the data point is significantly shorter than the average path length of the trees, it suggests that the data point is more likely to be an anomaly.\n",
    "\n",
    "3. Calculate the anomaly score: The anomaly score can be calculated as the inverse of the average path length of the data point normalized by the average path length of the trees. Smaller normalized average path lengths indicate more anomalous data points.\n",
    "\n",
    "The formula for the anomaly score would be:\n",
    "\n",
    "Anomaly Score = 1 - (AvgPathLengthDataPoint / AvgPathLengthTrees)\n",
    "\n",
    "Keep in mind that the exact formulation and scaling might vary based on the specific implementation of the Isolation Forest algorithm you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1ab93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbde5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b7646e2",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cbe44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825893b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044c1df0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a4de3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e526b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d49820b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424f3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
