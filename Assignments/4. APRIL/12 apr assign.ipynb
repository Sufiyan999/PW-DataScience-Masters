{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71597a8",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7901a",
   "metadata": {},
   "source": [
    "Bagging (Bootstrap Aggregation) is an ensemble learning technique that helps reduce overfitting in decision trees and other base models. It works by training multiple base models on different subsets of the training data and then combining their predictions to make a final prediction. Bagging specifically addresses overfitting in decision trees through the following mechanisms:\n",
    "\n",
    "1. **Variance Reduction:** Decision trees are prone to high variance, which can lead to overfitting. Bagging aims to reduce the variance of the overall model by averaging the predictions of multiple trees trained on different subsets of the data. By combining the predictions of multiple trees, the extreme and noisy variations in individual predictions tend to cancel out, resulting in a more stable and accurate ensemble prediction.\n",
    "\n",
    "2. **Decreasing Model Complexity:** Bagging indirectly reduces the complexity of individual decision trees by training them on different bootstrapped subsets of the data. A typical decision tree is grown deeply to fit the training data well, which can lead to overfitting. In contrast, the bootstrapped subsets are smaller and more diverse, leading to shallower and less complex trees in the ensemble. The averaging of multiple simple trees provides a balanced model that captures general patterns without memorizing noise.\n",
    "\n",
    "3. **Reducing Bias:** While bagging doesn't directly address bias, it tends to reduce the bias introduced by individual decision trees. Since decision trees often partition the feature space in a greedy and localized manner, individual trees can be biased towards certain regions of the data. By training multiple trees on different subsets, bagging helps capture different perspectives of the data and collectively reduces the bias.\n",
    "\n",
    "4. **Smoothing Decision Boundaries:** The ensemble of multiple decision trees generated by bagging results in a more robust and generalized decision boundary. The combination of various decision boundaries learned from different bootstrapped subsets helps to create smoother and more stable decision boundaries, which are less likely to capture noise in the training data.\n",
    "\n",
    "Overall, bagging reduces overfitting in decision trees by averaging out noise, decreasing model complexity, reducing bias, and creating smoother decision boundaries. It improves the generalization capability of the model and enhances its performance on unseen data. One popular implementation of bagging is the Random Forest algorithm, which uses bagging to build multiple decision trees and further introduces randomization in the tree-building process for improved ensemble diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458f36f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b4182d8",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a122dc1",
   "metadata": {},
   "source": [
    "In bagging, the choice of base learners (individual models that are trained on different subsets of the data) can have an impact on the overall performance of the ensemble. Different types of base learners have their own advantages and disadvantages. Here's a breakdown of some common types of base learners and their associated pros and cons in the context of bagging:\n",
    "\n",
    "1. **Decision Trees:**\n",
    "   - **Advantages:** Decision trees are versatile and can capture complex relationships in the data. They can handle both categorical and numerical features, require minimal data preprocessing, and are robust to outliers. They can be effective base learners for bagging due to their diversity in feature selection.\n",
    "   - **Disadvantages:** Without proper control, individual decision trees can still overfit. Bagging helps mitigate this, but deep trees might still have high variance. Random Forests, an extension of bagging with decision trees, further address these issues.\n",
    "\n",
    "2. **Random Forests (Decision Trees with Randomization):**\n",
    "   - **Advantages:** Random Forests combine the benefits of bagging with additional randomization during tree building, such as random feature selection. This randomness improves the diversity of individual trees, which leads to a more robust ensemble that is less prone to overfitting. They tend to have excellent predictive performance.\n",
    "   - **Disadvantages:** Random Forests can be computationally more expensive due to the training of multiple trees with random feature subsets. Interpreting individual trees within the ensemble might also be challenging.\n",
    "\n",
    "3. **K-Nearest Neighbors (KNN):**\n",
    "   - **Advantages:** KNN is a simple and non-parametric algorithm that can capture local patterns in the data. When used as base learners in bagging, KNN can provide diversity in predictions and perform well when the decision boundary is nonlinear and intricate.\n",
    "   - **Disadvantages:** KNN can be computationally intensive, especially with large datasets. The choice of the number of neighbors (k) and distance metrics can influence the results. It might not perform well in high-dimensional spaces.\n",
    "\n",
    "4. **Linear Models (e.g., Logistic Regression, Linear SVM):**\n",
    "   - **Advantages:** Linear models are fast to train and suitable for high-dimensional data. They can be effective in cases where the relationship between features and target is relatively linear. When used as base learners, they can provide diversity to the ensemble.\n",
    "   - **Disadvantages:** Linear models might struggle with capturing complex nonlinear relationships. Bagging might help somewhat, but they might not perform as well as more flexible models for capturing intricate patterns.\n",
    "\n",
    "5. **Support Vector Machines (SVMs):**\n",
    "   - **Advantages:** SVMs can capture complex relationships and work well in high-dimensional spaces. They can be used as base learners in bagging to provide diversity and improved generalization.\n",
    "   - **Disadvantages:** SVMs can be computationally expensive, especially with large datasets. Choosing the right kernel and hyperparameters can be challenging. SVMs are sensitive to the choice of the regularization parameter (C).\n",
    "\n",
    "6. **Neural Networks:**\n",
    "   - **Advantages:** Neural networks can capture complex patterns in data and are known for their representation learning capabilities. When used as base learners in bagging, they can provide diversity and enhanced predictive power.\n",
    "   - **Disadvantages:** Neural networks can be computationally expensive and require careful tuning of various hyperparameters. Training neural networks might require a larger amount of data compared to other algorithms.\n",
    "\n",
    "In summary, the choice of base learners depends on the nature of the data, the problem at hand, and computational constraints. A diverse set of base learners often leads to a more robust and accurate ensemble. While bagging can help mitigate some limitations of individual base learners, it's important to consider the trade-offs and characteristics of each learner when constructing the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a183c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9102e15",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d90136",
   "metadata": {},
   "source": [
    "The choice of base learner can have a significant impact on the bias-variance tradeoff in bagging. The bias-variance tradeoff refers to the balance between a model's ability to fit the training data well (low bias) and its ability to generalize to unseen data (low variance). Different types of base learners influence this tradeoff in various ways:\n",
    "\n",
    "1. **Low-Bias Models (Complex Models):**\n",
    "   - **Examples:** Decision trees, neural networks, k-nearest neighbors with low k.\n",
    "   - **Effect on Bias-Variance Tradeoff:** Low-bias models are capable of capturing complex relationships in the data, even if they are noisy or intricate. When used as base learners in bagging, these models can lead to low bias in the ensemble. However, their individual predictions might be highly sensitive to noise in the training data, leading to high variance.\n",
    "\n",
    "2. **High-Bias Models (Simple Models):**\n",
    "   - **Examples:** Linear models, k-nearest neighbors with high k.\n",
    "   - **Effect on Bias-Variance Tradeoff:** High-bias models tend to generalize well but might underfit the training data. When used as base learners in bagging, these models can contribute to an ensemble with lower variance since their predictions are more stable across different subsets of data. However, the ensemble's overall bias might still be relatively high.\n",
    "\n",
    "3. **Tradeoff with Bagging:**\n",
    "   - Bagging works by averaging predictions from multiple base learners trained on different subsets of the data.\n",
    "   - Low-bias models, while prone to high variance, can benefit from the variance reduction of bagging. The ensemble's predictions are more stable and less sensitive to noise.\n",
    "   - High-bias models, while more stable individually, can benefit from the diversity introduced by bagging. The ensemble's bias might be reduced due to the combination of different perspectives.\n",
    "\n",
    "4. **Ensemble Diversity:**\n",
    "   - Bagging's effectiveness in reducing variance and improving generalization largely depends on the diversity of the base learners.\n",
    "   - If base learners are too similar (e.g., many identical decision trees), the ensemble might still have high variance.\n",
    "   - Diverse base learners, regardless of their bias, contribute to an ensemble that benefits from both individual models' strengths.\n",
    "\n",
    "5. **Complexity and Interpretability:**\n",
    "   - Models with high complexity (low bias) can capture intricate patterns, but they might also memorize noise from the training data, leading to overfitting.\n",
    "   - Models with low complexity (high bias) might underfit but provide more interpretable and understandable results.\n",
    "\n",
    "In summary, the choice of base learner influences the bias-variance tradeoff in bagging. Low-bias models contribute to lower bias in the ensemble but might increase variance. High-bias models contribute to a more stable ensemble but might not capture complex relationships. Bagging aims to strike a balance between these factors by combining the strengths of different base learners and reducing the ensemble's overall variance while controlling bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c95274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a7cf295",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609a5b3",
   "metadata": {},
   "source": [
    "Yes, bagging can be used for both classification and regression tasks. Bagging is an ensemble technique that aims to improve the performance of base models (such as decision trees or other algorithms) by training multiple instances of these models on different subsets of the training data. The key idea behind bagging is to reduce variance and enhance the generalization ability of the models.\n",
    "\n",
    "Here's how bagging can be used for both classification and regression tasks:\n",
    "\n",
    "1. **Bagging for Classification:**\n",
    "   - In classification tasks, bagging involves training multiple instances of a base classifier (e.g., decision tree, support vector machine, etc.) on different subsets of the training data.\n",
    "   - Each base classifier generates its predictions for the test instances, and the final ensemble prediction is obtained by combining the predictions of all individual classifiers (e.g., through majority voting for binary classification).\n",
    "   - Popular ensemble algorithms that use bagging for classification include Random Forests, which use decision trees as base classifiers.\n",
    "\n",
    "2. **Bagging for Regression:**\n",
    "   - In regression tasks, bagging involves training multiple instances of a base regressor (e.g., decision tree, linear regression, etc.) on different subsets of the training data.\n",
    "   - Each base regressor generates its predictions for the test instances, and the final ensemble prediction is obtained by averaging or otherwise aggregating the predictions of all individual regressors.\n",
    "   - Bagging for regression can help reduce the impact of outliers and improve the stability of predictions.\n",
    "\n",
    "Bagging is a versatile technique that can be applied to a wide range of algorithms and tasks. It helps mitigate overfitting and enhances the generalization of base models by leveraging the diversity and averaging of predictions from multiple instances. In both classification and regression scenarios, bagging helps create more robust and accurate ensemble models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d470aad",
   "metadata": {},
   "source": [
    "While the fundamental concept of bagging remains the same for both classification and regression tasks, there are certain differences in how bagging is applied and how the predictions are aggregated based on the nature of the task. Here's how bagging differs in each case:\n",
    "\n",
    "**Bagging for Classification:**\n",
    "\n",
    "1. **Base Models:** In classification, the base models are classifiers. These can be decision trees, support vector machines, k-nearest neighbors, or any other classification algorithm.\n",
    "\n",
    "2. **Ensemble Prediction Aggregation:** The predictions of individual base classifiers are typically aggregated using majority voting. For binary classification, the class with the most votes is considered the final prediction. For multiclass classification, the class with the highest number of votes may be chosen.\n",
    "\n",
    "3. **Performance Metric:** The performance of a bagged ensemble in classification tasks is often evaluated using metrics like accuracy, precision, recall, F1 score, and ROC curves.\n",
    "\n",
    "4. **Ensemble Diversity:** Bagging aims to improve ensemble performance by reducing variance. In classification, diverse base classifiers can lead to more accurate predictions because they capture different decision boundaries.\n",
    "\n",
    "**Bagging for Regression:**\n",
    "\n",
    "1. **Base Models:** In regression, the base models are regressors. These can be decision trees, linear regression models, k-nearest neighbors (with continuous output), or other regression algorithms.\n",
    "\n",
    "2. **Ensemble Prediction Aggregation:** The predictions of individual base regressors are typically aggregated by averaging their predictions. Other aggregation methods, such as weighted averaging, can also be used.\n",
    "\n",
    "3. **Performance Metric:** The performance of a bagged ensemble in regression tasks is often evaluated using metrics like mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared.\n",
    "\n",
    "4. **Ensemble Diversity:** Bagging aims to improve ensemble performance by reducing the impact of outliers and improving the stability of predictions. In regression, diverse base regressors can lead to more accurate predictions by considering different aspects of the input space.\n",
    "\n",
    "In summary, while the core idea of bagging remains consistent between classification and regression tasks (i.e., training multiple base models on different subsets of data and aggregating their predictions), the methods of prediction aggregation and the evaluation metrics used differ based on whether the task is classification or regression. The choice of base models and the nature of the output (categorical or continuous) also influence the application of bagging in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90173a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88475ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beeed0f4",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392388d",
   "metadata": {},
   "source": [
    "The ensemble size in bagging refers to the number of base models or learners that are trained and combined to create the ensemble. In other words, it represents how many instances of the base algorithm are used to make predictions and contribute to the final aggregated prediction of the ensemble. The role of ensemble size in bagging is significant and can impact the performance and characteristics of the ensemble model.\n",
    "\n",
    "Here's how the ensemble size affects bagging:\n",
    "\n",
    "1. **Increased Diversity:** As the ensemble size increases, the diversity among the base models also tends to increase. This is because each base model is trained on a different subset of the data due to the random sampling with replacement in the bagging process. A larger ensemble size ensures a broader representation of different training data subsets, leading to more diverse predictions.\n",
    "\n",
    "2. **Reduced Variance:** One of the primary goals of bagging is to reduce the variance of individual base models by combining their predictions. As the ensemble size grows, the variance of the ensemble's predictions tends to decrease. This means that the ensemble becomes more stable and less sensitive to fluctuations in the training data.\n",
    "\n",
    "3. **Improved Generalization:** With a larger ensemble size, the aggregated prediction tends to generalize better to unseen data. This is because the ensemble captures a more robust and comprehensive representation of the underlying patterns in the data. A larger ensemble is more likely to capture the true underlying distribution of the data.\n",
    "\n",
    "4. **Diminishing Returns:** While increasing the ensemble size can lead to better performance and more stable predictions, there is a point of diminishing returns. After a certain point, adding more base models might not significantly improve the ensemble's performance and might even lead to increased computational complexity.\n",
    "\n",
    "5. **Computational Resources:** The ensemble size directly impacts the computational resources required for training and prediction. Larger ensemble sizes require more memory and processing power. Therefore, practical considerations, such as available resources and time constraints, should also be taken into account.\n",
    "\n",
    "6. **Overfitting Consideration:** Although bagging aims to reduce overfitting by combining diverse base models, an extremely large ensemble size might still result in overfitting if the base models are too complex. Regularization techniques, such as limiting the depth of decision trees, can help mitigate this risk.\n",
    "\n",
    "In summary, the ensemble size in bagging plays a crucial role in determining the diversity, stability, and generalization ability of the ensemble model. A larger ensemble size generally leads to reduced variance and improved performance, but there is a balance to strike between the benefits gained and the computational resources required. It's important to experiment with different ensemble sizes and monitor the ensemble's performance on validation data to find the optimal ensemble size for the given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaec6fd",
   "metadata": {},
   "source": [
    "The optimal number of models to include in an ensemble depends on several factors, including the nature of the problem, the dataset, the base learners, and available computational resources. There is no one-size-fits-all answer, but here are some guidelines to consider when deciding how many models to include in an ensemble:\n",
    "\n",
    "1. **Cross-Validation and Validation Performance:** One common approach is to use cross-validation or a validation set to assess the ensemble's performance for different ensemble sizes. Plotting the performance metrics (e.g., accuracy, MSE, etc.) against the number of models can help identify the point at which performance starts to plateau or diminish. This can give you an idea of the optimal ensemble size.\n",
    "\n",
    "2. **Bias-Variance Tradeoff:** A larger ensemble size tends to reduce variance and improve generalization by increasing diversity. However, it's important to balance this with the potential for increased bias. Adding too many complex models can lead to overfitting, especially if the base learners are already prone to overfitting.\n",
    "\n",
    "3. **Resource Constraints:** Consider the available computational resources when deciding on the ensemble size. Larger ensembles require more memory and processing power. It's essential to strike a balance between performance gains and practical limitations.\n",
    "\n",
    "4. **Base Learner Complexity:** If the base learners are complex models, you might need fewer of them in the ensemble. On the other hand, if the base learners are simple, including more of them can provide diversity and improve the ensemble's ability to capture different patterns.\n",
    "\n",
    "5. **Ensemble Diversity:** If the base models are diverse and have different strengths, you might not need as many of them to achieve good performance. If the base models are very similar, a larger ensemble might be more beneficial.\n",
    "\n",
    "6. **Ensemble Technique:** Different ensemble techniques have different characteristics. For example, Random Forests tend to perform well with a moderate number of trees, while boosting methods often create larger ensembles with a focus on model weights.\n",
    "\n",
    "7. **Problem Complexity:** More complex problems (with intricate patterns) might benefit from larger ensembles, while simpler problems might achieve good performance with fewer models.\n",
    "\n",
    "8. **Validation and Monitoring:** Continuously monitor the performance of the ensemble as you increase the ensemble size. If you notice diminishing returns or performance starts to degrade, you might have reached an optimal point.\n",
    "\n",
    "9. **Ensemble Size vs. Performance Tradeoff:** Remember that adding more models might not always lead to substantial improvements in performance. There is a tradeoff, and excessively large ensembles might lead to overfitting or computational inefficiency.\n",
    "\n",
    "In practice, it's often helpful to start with a relatively small ensemble size and gradually increase it while monitoring performance on validation data. Once you observe that performance plateaus or diminishes, you've likely reached an optimal ensemble size. Keep in mind that the optimal size can vary from one problem to another, so experimentation and careful consideration of the factors mentioned above are key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ae145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0e81a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77c19034",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 6 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be825dc",
   "metadata": {},
   "source": [
    "real-world application of bagging in machine learning is in the field of medical diagnostics, particularly in the classification of medical images for disease detection. Let's consider the example of detecting diabetic retinopathy, a leading cause of blindness in diabetic patients. Bagging can play a crucial role in improving the accuracy and reliability of automated diagnostic systems for this condition.\n",
    "\n",
    "**Application: Diabetic Retinopathy Detection**\n",
    "\n",
    "**Problem:** Diabetic retinopathy is a complication of diabetes that affects the blood vessels in the retina. Early detection and diagnosis are critical to prevent vision loss. One approach to detecting diabetic retinopathy is to analyze retinal images using machine learning techniques.\n",
    "\n",
    "**Bagging in Action:**\n",
    "1. **Data Collection:** A dataset of retinal images is collected, with each image labeled as either \"normal\" or having a certain level of diabetic retinopathy severity.\n",
    "\n",
    "2. **Base Classifier:** A base classifier, such as a decision tree or a convolutional neural network (CNN), is chosen to classify retinal images. However, a single classifier might have limitations in terms of capturing all possible patterns and variations in the images.\n",
    "\n",
    "3. **Bagging Ensemble:** Multiple instances of the base classifier are trained, each on a different random subset of the training data (bagging). For example, each base classifier may be trained on a bootstrapped sample of the original dataset.\n",
    "\n",
    "4. **Predictions Aggregation:** To make predictions, the bagging ensemble aggregates the predictions of individual base classifiers. For classification tasks, the final prediction can be determined by majority voting, where the class with the most votes across the base classifiers is chosen.\n",
    "\n",
    "5. **Improved Performance:** The bagging ensemble's predictions tend to be more accurate and robust compared to a single classifier. Bagging helps reduce overfitting and improve generalization by leveraging the diversity of the base classifiers' predictions.\n",
    "\n",
    "**Advantages of Bagging:**\n",
    "- **Improved Robustness:** Bagging reduces the impact of noisy or outlier images that might lead to incorrect predictions in a single classifier.\n",
    "- **Better Generalization:** The ensemble is less likely to overfit to specific features or patterns in the data, leading to better generalization to unseen images.\n",
    "- **Reduced Variance:** By combining diverse predictions, bagging reduces the variance of the ensemble's predictions.\n",
    "\n",
    "**Limitations:**\n",
    "- **Computational Complexity:** Training and maintaining multiple base classifiers can be computationally intensive, especially if complex models like CNNs are used.\n",
    "- **Ensemble Size:** Determining the optimal ensemble size requires experimentation and validation to avoid overfitting.\n",
    "\n",
    "**Outcome:** By using bagging to create an ensemble of classifiers for diabetic retinopathy detection, the diagnostic system can achieve higher accuracy and reliability. This can assist medical professionals in making more accurate and timely decisions for patient care and treatment.\n",
    "\n",
    "In this application, bagging enhances the performance of the base classifier by leveraging the diversity of retinal images and patterns present in the data. It showcases how bagging can be used to improve the accuracy and robustness of machine learning models in important real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d00165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503b309d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db6fb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bd1370c",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f799a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31beece9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521d5d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ccadf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6484351c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
