{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a160e01a",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238af5c",
   "metadata": {},
   "source": [
    "This problem can be solved using conditional probability. We are asked to find the probability that an employee is a smoker given that he/she uses the health insurance plan. This can be expressed as:\n",
    "\n",
    "\\[ P(Smoker | Uses Insurance Plan  ) \\]\n",
    "\n",
    "We are given the following information:\n",
    "- \\( P( Uses Insurance Plan ) = 0.70 \\) (probability that an employee uses the insurance plan)\n",
    "- \\( P(Smoker | Uses Insurance Plan ) = 0.40 \\) (probability that an employee is a smoker given that they use the insurance plan)\n",
    "\n",
    "Using the formula for conditional probability:\n",
    "\n",
    "\\[ P(A|B) = P(A ∩ B) / P(B) \\]\n",
    "\n",
    "Where \\( A \\) represents \"Smoker\" and \\( B \\) represents \"Uses Insurance Plan.\"\n",
    "\n",
    "We can plug in the given values:\n",
    "\n",
    "\\[ P(Smoker | Uses Insurance Plan ) = P(Smoker ∩ Uses Insurance Plan) / P( Uses Insurance Plan) \\]\n",
    "\n",
    "We are not given \\( P(\\text{Smoker} \\cap \\text{Uses Insurance Plan}) \\), but we can calculate it using the information given:\n",
    "\\[ P(Smoker ∩ Uses Insurance Plan) = P(Uses Insurance Plan) * P(Smoker | Uses Insurance Plan ) \\]\n",
    "\\[ P(Smoker ∩ Uses Insurance Plan) = 0.70 * 0.40 = 0.28 \\]\n",
    "\n",
    "Now we can calculate the conditional probability:\n",
    "\\[ P(Smoker | ses Insurance Plan) =0.28 / 0.70 =  0.4 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.4 or 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c456e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that an employee is a smoker given they use the health insurance plan: 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_uses_insurance_plan = 0.70  # Probability that an employee uses the insurance plan\n",
    "P_smoker_given_uses_plan = 0.40  # Probability that an employee is a smoker given they use the insurance plan\n",
    "\n",
    "# Calculate the probability of smoker given uses insurance plan\n",
    "P_smoker_and_uses_plan = P_uses_insurance_plan * P_smoker_given_uses_plan\n",
    "P_smoker_given_uses_plan = P_smoker_and_uses_plan / P_uses_insurance_plan\n",
    "\n",
    "print(\"Probability that an employee is a smoker given they use the health insurance plan:\",P_smoker_given_uses_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4c8bb",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c1c56",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes algorithm, each designed to handle different types of data and scenarios.\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - Bernoulli Naive Bayes is used for binary classification tasks, where the features are binary variables (0 or 1).\n",
    "   - It's commonly used for text classification tasks, where each feature represents the presence or absence of a word in a document.\n",
    "   - Assumes that the features are independent of each other given the class label.\n",
    "   - It models the presence or absence of features using a Bernoulli distribution.\n",
    "   - A common application is spam detection, where each feature could represent the presence of certain words in an email.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Multinomial Naive Bayes is used for text classification tasks where the features represent the frequency or count of words.\n",
    "   - It's suitable for cases where the features are discrete and represent counts of occurrences.\n",
    "   - Assumes that the features follow a multinomial distribution.\n",
    "   - Often used in natural language processing tasks like sentiment analysis, topic classification, and document categorization.\n",
    "   - It's an extension of the Bernoulli Naive Bayes that accounts for the frequency of terms in addition to their presence/absence.\n",
    "\n",
    "In summary, the main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed to handle. Bernoulli Naive Bayes is used for binary features (presence/absence), while Multinomial Naive Bayes is used for features representing counts or frequencies. The choice between them depends on the nature of your data and the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bb27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38155233",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bda40",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by assuming that missing values are equivalent to the absence of a feature. In other words, when a feature value is missing for a particular instance, Bernoulli Naive Bayes treats it as if the feature is not present for that instance. This assumption aligns with the binary nature of Bernoulli Naive Bayes, where features are binary variables (0 or 1) representing the presence or absence of certain characteristics.\n",
    "\n",
    "When performing classification using Bernoulli Naive Bayes, the algorithm calculates probabilities based on the presence or absence of features in each instance. If a feature's value is missing for an instance, the algorithm assumes that the feature is absent (assigned a value of 0) for that instance.\n",
    "\n",
    "Here's how Bernoulli Naive Bayes handles missing values during classification:\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - During the training phase, the algorithm estimates the probabilities of features being present (1) or absent (0) for each class based on the training data.\n",
    "   - If a feature value is missing in the training data, it's treated as if the feature is absent (assigned a value of 0) for that instance during probability estimation.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - When making predictions for a new instance with missing feature values, Bernoulli Naive Bayes assumes those features are absent (assigned a value of 0).\n",
    "   - The algorithm calculates the probability of each class given the presence (1) or absence (0) of features in the new instance.\n",
    "\n",
    "It's important to note that this handling of missing values simplifies the calculation process, but it assumes that missing values have the same meaning as the absence of a feature. If missing values carry a different meaning or significance, more sophisticated approaches may be necessary, such as imputation techniques or more advanced variations of Naive Bayes, like Gaussian Naive Bayes or Multinomial Naive Bayes, which can handle different types of data and missing values more flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916dcccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a46e974d",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7ca75",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. It's particularly suitable for continuous numerical data. While it's often associated with binary classification, it can also be extended to handle multi-class classification problems.\n",
    "\n",
    "The approach for using Gaussian Naive Bayes for multi-class classification is straightforward:\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - For each class in the dataset, calculate the mean and standard deviation of each feature.\n",
    "   - These statistics describe the distribution of the feature values for each class.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - Given a new instance with feature values, calculate the likelihood of the instance's feature values under each class's Gaussian distribution using the mean and standard deviation obtained from the training phase.\n",
    "   - Multiply the likelihoods with the prior probabilities of each class.\n",
    "   - Normalize the probabilities to get the posterior probabilities.\n",
    "   - Predict the class with the highest posterior probability.\n",
    "\n",
    "Gaussian Naive Bayes can handle multi-class problems by applying the same principles of conditional probability and Bayes' theorem as in binary classification. Each class's distribution is modeled as a Gaussian distribution, and the likelihoods of feature values under each class's distribution are calculated to determine the most likely class.\n",
    "\n",
    "While Gaussian Naive Bayes can work for multi-class problems, it's important to consider the assumptions of the Gaussian distribution and the independence assumption of Naive Bayes. If the data significantly deviates from these assumptions, other algorithms or variations of Naive Bayes (such as Multinomial or Complement Naive Bayes) might be more appropriate for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f50dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7899cb33",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "079b535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffece2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bfe8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bd1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7fba0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| SPAM E-MAIL DATABASE ATTRIBUTES (in .names format)\n",
      "|\n",
      "| 48 continuous real [0,100] attributes of type word_freq_WORD \n",
      "| = percentage of words in the e-mail that match WORD,\n",
      "| i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
      "| total number of words in e-mail.  A \"word\" in this case is any \n",
      "| string of alphanumeric characters bounded by non-alphanumeric \n",
      "| characters or end-of-string.\n",
      "|\n",
      "| 6 continuous real [0,100] attributes of type char_freq_CHAR\n",
      "| = percentage of characters in the e-mail that match CHAR,\n",
      "| i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
      "|\n",
      "| 1 continuous real [1,...] attribute of type capital_run_length_average\n",
      "| = average length of uninterrupted sequences of capital letters\n",
      "|\n",
      "| 1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
      "| = length of longest uninterrupted sequence of capital letters\n",
      "|\n",
      "| 1 continuous integer [1,...] attribute of type capital_run_length_total\n",
      "| = sum of length of uninterrupted sequences of capital letters\n",
      "| = total number of capital letters in the e-mail\n",
      "|\n",
      "| 1 nominal {0,1} class attribute of type spam\n",
      "| = denotes whether the e-mail was considered spam (1) or not (0), \n",
      "| i.e. unsolicited commercial e-mail.  \n",
      "|\n",
      "| For more information, see file 'spambase.DOCUMENTATION' at the\n",
      "| UCI Machine Learning Repository: http://www.ics.uci.edu/~mlearn/MLRepository.html\n",
      "\n",
      "\n",
      "1, 0.    | spam, non-spam classes\n",
      "\n",
      "word_freq_make:         continuous.\n",
      "word_freq_address:      continuous.\n",
      "word_freq_all:          continuous.\n",
      "word_freq_3d:           continuous.\n",
      "word_freq_our:          continuous.\n",
      "word_freq_over:         continuous.\n",
      "word_freq_remove:       continuous.\n",
      "word_freq_internet:     continuous.\n",
      "word_freq_order:        continuous.\n",
      "word_freq_mail:         continuous.\n",
      "word_freq_receive:      continuous.\n",
      "word_freq_will:         continuous.\n",
      "word_freq_people:       continuous.\n",
      "word_freq_report:       continuous.\n",
      "word_freq_addresses:    continuous.\n",
      "word_freq_free:         continuous.\n",
      "word_freq_business:     continuous.\n",
      "word_freq_email:        continuous.\n",
      "word_freq_you:          continuous.\n",
      "word_freq_credit:       continuous.\n",
      "word_freq_your:         continuous.\n",
      "word_freq_font:         continuous.\n",
      "word_freq_000:          continuous.\n",
      "word_freq_money:        continuous.\n",
      "word_freq_hp:           continuous.\n",
      "word_freq_hpl:          continuous.\n",
      "word_freq_george:       continuous.\n",
      "word_freq_650:          continuous.\n",
      "word_freq_lab:          continuous.\n",
      "word_freq_labs:         continuous.\n",
      "word_freq_telnet:       continuous.\n",
      "word_freq_857:          continuous.\n",
      "word_freq_data:         continuous.\n",
      "word_freq_415:          continuous.\n",
      "word_freq_85:           continuous.\n",
      "word_freq_technology:   continuous.\n",
      "word_freq_1999:         continuous.\n",
      "word_freq_parts:        continuous.\n",
      "word_freq_pm:           continuous.\n",
      "word_freq_direct:       continuous.\n",
      "word_freq_cs:           continuous.\n",
      "word_freq_meeting:      continuous.\n",
      "word_freq_original:     continuous.\n",
      "word_freq_project:      continuous.\n",
      "word_freq_re:           continuous.\n",
      "word_freq_edu:          continuous.\n",
      "word_freq_table:        continuous.\n",
      "word_freq_conference:   continuous.\n",
      "char_freq_;:            continuous.\n",
      "char_freq_(:            continuous.\n",
      "char_freq_[:            continuous.\n",
      "char_freq_!:            continuous.\n",
      "char_freq_$:            continuous.\n",
      "char_freq_#:            continuous.\n",
      "capital_run_length_average: continuous.\n",
      "capital_run_length_longest: continuous.\n",
      "capital_run_length_total:   continuous.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to the downloaded .names file\n",
    "names_file_path =  r\"D:\\Data Science\\Assignments\\4. APRIL\\spambase\\spambase.names\"  # Replace with the actual path to the downloaded file\n",
    "\n",
    "# Read and print the contents of the .names file\n",
    "with open(names_file_path, 'r') as names_file:\n",
    "    contents = names_file.read()\n",
    "    print(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32af2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [ 'word_freq_make' ,\n",
    "\"word_freq_address\", \n",
    "'word_freq_all'  ,\n",
    "'word_freq_3d'  ,\n",
    "'word_freq_our' ,\n",
    "'word_freq_over'  ,\n",
    "'word_freq_remove: ' ,\n",
    "'word_freq_internet: ' ,\n",
    "'word_freq_order' ,    \n",
    "'word_freq_mail' ,\n",
    "'word_freq_receive', \n",
    "'word_freq_will', \n",
    "'word_freq_people', \n",
    "'word_freq_report', \n",
    "'word_freq_addresses', \n",
    "'word_freq_free', \n",
    "'word_freq_business', \n",
    "'word_freq_email', \n",
    "'word_freq_you', \n",
    "'word_freq_credit', \n",
    "'word_freq_your', \n",
    "'word_freq_font', \n",
    "'word_freq_000', \n",
    "'word_freq_money', \n",
    "'word_freq_hp', \n",
    "'word_freq_hpl', \n",
    "'word_freq_george', \n",
    "'word_freq_650', \n",
    "'word_freq_lab', \n",
    "'word_freq_labs', \n",
    "'word_freq_telnet', \n",
    "'word_freq_857', \n",
    "'word_freq_data', \n",
    "'word_freq_415', \n",
    "'word_freq_85', \n",
    "'word_freq_technology', \n",
    "'word_freq_1999', \n",
    "'word_freq_parts', \n",
    "'word_freq_pm', \n",
    "'word_freq_direct', \n",
    "'word_freq_cs', \n",
    "'word_freq_meeting', \n",
    "'word_freq_original', \n",
    "'word_freq_project', \n",
    "'word_freq_re', \n",
    "'word_freq_edu', \n",
    "'word_freq_table', \n",
    "'word_freq_conference', \n",
    "'char_freq_;', \n",
    "'char_freq_(', \n",
    "'char_freq_[', \n",
    "'char_freq_!', \n",
    "'char_freq_$', \n",
    "'char_freq_#', \n",
    "'capital_run_length_average', \n",
    "'capital_run_length_longest', \n",
    "'capital_run_length_total', \n",
    " \"spam\"      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ab3ce2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4596</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.142</td>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.353</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.555</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.404</td>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.250</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0               0.00               0.64           0.64           0.0   \n",
       "1               0.21               0.28           0.50           0.0   \n",
       "2               0.06               0.00           0.71           0.0   \n",
       "3               0.00               0.00           0.00           0.0   \n",
       "4               0.00               0.00           0.00           0.0   \n",
       "...              ...                ...            ...           ...   \n",
       "4596            0.31               0.00           0.62           0.0   \n",
       "4597            0.00               0.00           0.00           0.0   \n",
       "4598            0.30               0.00           0.30           0.0   \n",
       "4599            0.96               0.00           0.00           0.0   \n",
       "4600            0.00               0.00           0.65           0.0   \n",
       "\n",
       "      word_freq_our  word_freq_over  word_freq_remove:   word_freq_internet:   \\\n",
       "0              0.32            0.00                0.00                  0.00   \n",
       "1              0.14            0.28                0.21                  0.07   \n",
       "2              1.23            0.19                0.19                  0.12   \n",
       "3              0.63            0.00                0.31                  0.63   \n",
       "4              0.63            0.00                0.31                  0.63   \n",
       "...             ...             ...                 ...                   ...   \n",
       "4596           0.00            0.31                0.00                  0.00   \n",
       "4597           0.00            0.00                0.00                  0.00   \n",
       "4598           0.00            0.00                0.00                  0.00   \n",
       "4599           0.32            0.00                0.00                  0.00   \n",
       "4600           0.00            0.00                0.00                  0.00   \n",
       "\n",
       "      word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0                0.00            0.00  ...        0.000        0.000   \n",
       "1                0.00            0.94  ...        0.000        0.132   \n",
       "2                0.64            0.25  ...        0.010        0.143   \n",
       "3                0.31            0.63  ...        0.000        0.137   \n",
       "4                0.31            0.63  ...        0.000        0.135   \n",
       "...               ...             ...  ...          ...          ...   \n",
       "4596             0.00            0.00  ...        0.000        0.232   \n",
       "4597             0.00            0.00  ...        0.000        0.000   \n",
       "4598             0.00            0.00  ...        0.102        0.718   \n",
       "4599             0.00            0.00  ...        0.000        0.057   \n",
       "4600             0.00            0.00  ...        0.000        0.000   \n",
       "\n",
       "      char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0             0.0        0.778        0.000        0.000   \n",
       "1             0.0        0.372        0.180        0.048   \n",
       "2             0.0        0.276        0.184        0.010   \n",
       "3             0.0        0.137        0.000        0.000   \n",
       "4             0.0        0.135        0.000        0.000   \n",
       "...           ...          ...          ...          ...   \n",
       "4596          0.0        0.000        0.000        0.000   \n",
       "4597          0.0        0.353        0.000        0.000   \n",
       "4598          0.0        0.000        0.000        0.000   \n",
       "4599          0.0        0.000        0.000        0.000   \n",
       "4600          0.0        0.125        0.000        0.000   \n",
       "\n",
       "      capital_run_length_average  capital_run_length_longest  \\\n",
       "0                          3.756                          61   \n",
       "1                          5.114                         101   \n",
       "2                          9.821                         485   \n",
       "3                          3.537                          40   \n",
       "4                          3.537                          40   \n",
       "...                          ...                         ...   \n",
       "4596                       1.142                           3   \n",
       "4597                       1.555                           4   \n",
       "4598                       1.404                           6   \n",
       "4599                       1.147                           5   \n",
       "4600                       1.250                           5   \n",
       "\n",
       "      capital_run_length_total  spam  \n",
       "0                          278     1  \n",
       "1                         1028     1  \n",
       "2                         2259     1  \n",
       "3                          191     1  \n",
       "4                          191     1  \n",
       "...                        ...   ...  \n",
       "4596                        88     0  \n",
       "4597                        14     0  \n",
       "4598                       118     0  \n",
       "4599                        78     0  \n",
       "4600                        40     0  \n",
       "\n",
       "[4601 rows x 58 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to the downloaded .data file\n",
    "data_file_path =  r\"D:\\Data Science\\Assignments\\4. APRIL\\spambase\\spambase.data\"  # Replace with the actual path to the downloaded file\n",
    "# Load the data using Pandas and assign column names\n",
    "data = pd.read_csv(data_file_path, header=None, names=column_names)\n",
    "\n",
    "# Display the first few rows of the loaded data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26860033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29b77b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Accuracy: 0.8805646036916395\n",
      "Multinomial Naive Bayes Accuracy: 0.7861020629750272\n",
      "Gaussian Naive Bayes Accuracy: 0.8208469055374593\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame with features and target variable\n",
    "X = data.drop(columns=['spam'])  # Features\n",
    "y = data['spam']  # Target variable\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_nb.fit(X_train, y_train)\n",
    "y_pred_bernoulli = bernoulli_nb.predict(X_test)\n",
    "accuracy_bernoulli = accuracy_score(y_test, y_pred_bernoulli)\n",
    "print(\"Bernoulli Naive Bayes Accuracy:\", accuracy_bernoulli)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_nb.fit(X_train, y_train)\n",
    "y_pred_multinomial = multinomial_nb.predict(X_test)\n",
    "accuracy_multinomial = accuracy_score(y_test, y_pred_multinomial)\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", accuracy_multinomial)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_nb.fit(X_train, y_train)\n",
    "y_pred_gaussian = gaussian_nb.predict(X_test)\n",
    "accuracy_gaussian = accuracy_score(y_test, y_pred_gaussian)\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", accuracy_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24d38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "382c4f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bernoulli Naive Bayes Cross-Validation Scores: [0.88503254 0.9173913  0.9        0.90869565 0.89130435 0.92826087\n",
      " 0.92608696 0.89130435 0.80869565 0.7826087 ]\n",
      "\n",
      "Average Accuracy: 0.8839380364047911\n",
      "\n",
      "Multinomial Naive Bayes Cross-Validation Scores: [0.79175705 0.79347826 0.80869565 0.83478261 0.82826087 0.77826087\n",
      " 0.77826087 0.81304348 0.69347826 0.74347826]\n",
      "\n",
      "Average Accuracy: 0.7863496180326323\n",
      "\n",
      "Gaussian Naive Bayes Cross-Validation Scores: [0.84381779 0.86304348 0.87826087 0.8673913  0.88478261 0.82826087\n",
      " 0.8326087  0.8673913  0.63478261 0.7173913 ]\n",
      "\n",
      "Average Accuracy: 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Assuming 'data' is your DataFrame with features and target variable\n",
    "X = data.drop(columns=['spam'])  # Features\n",
    "y = data['spam']  # Target variable\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n",
    "print(\"\\nBernoulli Naive Bayes Cross-Validation Scores:\", bernoulli_scores)\n",
    "print(\"\\nAverage Accuracy:\", bernoulli_scores.mean())\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy')\n",
    "print(\"\\nMultinomial Naive Bayes Cross-Validation Scores:\", multinomial_scores)\n",
    "print(\"\\nAverage Accuracy:\", multinomial_scores.mean())\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy')\n",
    "print(\"\\nGaussian Naive Bayes Cross-Validation Scores:\", gaussian_scores)\n",
    "print(\"\\nAverage Accuracy:\", gaussian_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a22a2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bernoulli Naive Bayes Cross-Validation Scores: [0.88503254 0.9173913  0.9        0.90869565 0.89130435 0.92826087\n",
      " 0.92608696 0.89130435 0.80869565 0.7826087 ]\n",
      "\n",
      "Average Accuracy: 0.8839380364047911\n",
      "\n",
      "Multinomial Naive Bayes Cross-Validation Scores: [0.79175705 0.79347826 0.80869565 0.83478261 0.82826087 0.77826087\n",
      " 0.77826087 0.81304348 0.69347826 0.74347826]\n",
      "\n",
      "Average Accuracy: 0.7863496180326323\n",
      "\n",
      "Gaussian Naive Bayes Cross-Validation Scores: [0.84381779 0.86304348 0.87826087 0.8673913  0.88478261 0.82826087\n",
      " 0.8326087  0.8673913  0.63478261 0.7173913 ]\n",
      "\n",
      "Average Accuracy: 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Assuming 'data' is your DataFrame with features and target variable\n",
    "X = data.drop(columns=['spam'])  # Features\n",
    "y = data['spam']  # Target variable\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=10, scoring='accuracy')\n",
    "print(\"\\nBernoulli Naive Bayes Cross-Validation Scores:\", bernoulli_scores)\n",
    "print(\"\\nAverage Accuracy:\", bernoulli_scores.mean())\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "multinomial_nb = MultinomialNB()\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=10, scoring='accuracy')\n",
    "print(\"\\nMultinomial Naive Bayes Cross-Validation Scores:\", multinomial_scores)\n",
    "print(\"\\nAverage Accuracy:\", multinomial_scores.mean())\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gaussian_nb = GaussianNB()\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=10, scoring='accuracy')\n",
    "print(\"\\nGaussian Naive Bayes Cross-Validation Scores:\", gaussian_scores)\n",
    "print(\"\\nAverage Accuracy:\", gaussian_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53357e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0be9c23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB\n",
      "Average Accuracy: 0.8839380364047911\n",
      "Average Precision: 0.8869617393737383\n",
      "Average Recall: 0.8152389047416673\n",
      "Average F1 Score: 0.8481249015095276\n",
      "\n",
      "MultinomialNB\n",
      "Average Accuracy: 0.7863496180326323\n",
      "Average Precision: 0.7393175533565436\n",
      "Average Recall: 0.7214983911116508\n",
      "Average F1 Score: 0.7282909724016348\n",
      "\n",
      "GaussianNB\n",
      "Average Accuracy: 0.8217730830896915\n",
      "Average Precision: 0.7103733928118492\n",
      "Average Recall: 0.9569516119239877\n",
      "Average F1 Score: 0.8130660909542995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming 'data' is your DataFrame with features and target variable\n",
    "X = data.drop(columns=['spam'])  # Features\n",
    "y = data['spam']  # Target variable\n",
    "\n",
    "# Create instances of the classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Dictionary to store metric scores for each classifier\n",
    "metric_scores = {'BernoulliNB': [], 'MultinomialNB': [], 'GaussianNB': []}\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for classifier in [bernoulli_nb, multinomial_nb, gaussian_nb]:\n",
    "    accuracy_scores = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
    "    precision_scores = cross_val_score(classifier, X, y, cv=10, scoring='precision')\n",
    "    recall_scores = cross_val_score(classifier, X, y, cv=10, scoring='recall')\n",
    "    f1_scores = cross_val_score(classifier, X, y, cv=10, scoring='f1')\n",
    "    \n",
    "    metric_scores[classifier.__class__.__name__].append(accuracy_scores.mean())\n",
    "    metric_scores[classifier.__class__.__name__].append(precision_scores.mean())\n",
    "    metric_scores[classifier.__class__.__name__].append(recall_scores.mean())\n",
    "    metric_scores[classifier.__class__.__name__].append(f1_scores.mean())\n",
    "\n",
    "# Print the performance metrics for each classifier\n",
    "for classifier, scores in metric_scores.items():\n",
    "    print(classifier)\n",
    "    print(\"Average Accuracy:\", scores[0])\n",
    "    print(\"Average Precision:\", scores[1])\n",
    "    print(\"Average Recall:\", scores[2])\n",
    "    print(\"Average F1 Score:\", scores[3])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53247fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12ad52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5df61af",
   "metadata": {},
   "source": [
    "Based on the results obtained from the cross-validation, we can discuss the performance of the three variants of Naive Bayes classifiers:\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - Average Accuracy: 0.884\n",
    "   - Average Precision: 0.887\n",
    "   - Average Recall: 0.815\n",
    "   - Average F1 Score: 0.848\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - Average Accuracy: 0.786\n",
    "   - Average Precision: 0.739\n",
    "   - Average Recall: 0.721\n",
    "   - Average F1 Score: 0.728\n",
    "\n",
    "3. **Gaussian Naive Bayes:**\n",
    "   - Average Accuracy: 0.822\n",
    "   - Average Precision: 0.710\n",
    "   - Average Recall: 0.957\n",
    "   - Average F1 Score: 0.813\n",
    "\n",
    "Among the three variants, Bernoulli Naive Bayes performed the best in terms of both accuracy and F1 score. This is likely because the Bernoulli Naive Bayes classifier is well-suited for binary data, which matches the nature of spam classification. It assumes that features are binary (occur or not occur) and models the occurrence of words in spam or non-spam emails. Since the dataset contains word frequencies (binary values), Bernoulli Naive Bayes leverages this information effectively.\n",
    "\n",
    "Gaussian Naive Bayes also performed well in terms of recall. However, it assumes that the features follow a Gaussian distribution, which might not be the case for all features in the dataset. The Gaussian variant could be capturing the patterns of some features that exhibit a Gaussian distribution.\n",
    "\n",
    "Limitations of Naive Bayes that can be observed from these results include:\n",
    "\n",
    "1. **Assumption of Feature Independence:** Naive Bayes assumes that all features are independent given the class label. This assumption might not hold in real-world scenarios where features are correlated. For example, certain words might co-occur frequently in spam emails, violating the independence assumption.\n",
    "\n",
    "2. **Limited Expressiveness:** Naive Bayes might struggle with complex relationships and interactions among features. It's a simple probabilistic model, which can result in suboptimal performance when the relationships are intricate.\n",
    "\n",
    "3. **Sensitivity to Feature Distribution:** Gaussian Naive Bayes assumes a Gaussian distribution for continuous features. If features do not follow this distribution, the performance might be compromised.\n",
    "\n",
    "4. **Data Sparsity:** In cases of rare events or low-frequency features, Naive Bayes might have difficulties estimating accurate probabilities, leading to biased predictions.\n",
    "\n",
    "5. **Lack of Model Exploration:** Naive Bayes makes strong assumptions about feature independence, which means it might not capture complex interactions present in the data.\n",
    "\n",
    "In conclusion, Bernoulli Naive Bayes outperformed the other variants in this scenario, possibly due to the binary nature of the dataset's features. However, it's important to note that the choice of classifier should also take into account other factors such as computational efficiency, interpretability, and potential for further feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e8a95b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28bb0ecb",
   "metadata": {},
   "source": [
    "In this analysis, we evaluated three variants of Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) on a spam classification dataset. The goal was to predict whether an email is spam or not based on various features. We conducted 10-fold cross-validation and examined four performance metrics: accuracy, precision, recall, and F1 score.\n",
    "\n",
    "**Findings:**\n",
    "- Bernoulli Naive Bayes exhibited the highest average accuracy and F1 score among the three variants, indicating its suitability for binary feature data like word frequencies.\n",
    "- Gaussian Naive Bayes had a relatively high recall, likely due to its assumption of feature distribution, which might fit well with certain features.\n",
    "- Multinomial Naive Bayes performed reasonably, but its performance lagged behind the other two variants.\n",
    "\n",
    "**Suggestions for Future Work:**\n",
    "1. **Feature Engineering:** Investigate feature engineering techniques to enhance the predictive power of the models. Feature extraction and selection methods might help capture more relevant information from the data.\n",
    "2. **Ensemble Methods:** Explore ensemble methods like Random Forests or Gradient Boosting in combination with Naive Bayes to harness the strengths of both approaches and improve classification performance.\n",
    "3. **Tuning Hyperparameters:** Fine-tune hyperparameters for each variant of Naive Bayes to achieve better performance. Hyperparameter optimization techniques like grid search or random search could be employed.\n",
    "4. **Addressing Class Imbalance:** If the dataset is imbalanced, consider techniques such as oversampling, undersampling, or using more advanced approaches like Synthetic Minority Over-sampling Technique (SMOTE) to balance the class distribution.\n",
    "5. **Comparative Analysis:** Extend the analysis to include other classification algorithms (e.g., Decision Trees, Support Vector Machines, Neural Networks) to identify the best-performing model for this specific problem.\n",
    "6. **Text Preprocessing:** If the dataset contains raw text data, explore more advanced text preprocessing techniques such as stemming, lemmatization, or TF-IDF representation to better capture the essence of the text.\n",
    "7. **Model Interpretability:** Investigate the reasons behind model decisions by employing techniques like feature importance analysis or SHAP (SHapley Additive exPlanations) to understand the contribution of each feature.\n",
    "8. **Handling Missing Data:** Address the issue of missing values in the dataset using appropriate techniques like imputation or building models that can handle missing values effectively.\n",
    "\n",
    "Ultimately, a thorough exploration of feature engineering, model selection, and parameter tuning could yield improvements in the performance of the classifiers. It's also important to consider the context and specific objectives of the problem when making decisions about which approach to pursue further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd458534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7355b9b",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff08a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15d3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c168b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696170b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca496011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
