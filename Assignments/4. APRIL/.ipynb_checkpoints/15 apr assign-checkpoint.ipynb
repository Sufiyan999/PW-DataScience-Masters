{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f8fb070",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30507812",
   "metadata": {},
   "source": [
    "## Basic  pipeline  RandomForestClassifier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess your data, split it into X_train, X_test, y_train, y_test\n",
    "# ...\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', ColumnTransformer([\n",
    "        ('num_imputer', SimpleImputer(strategy='mean'), numeric_features),  # Fill missing values with mean\n",
    "        ('cat_imputer', SimpleImputer(strategy='most_frequent'), categorical_features),  # Fill missing values with mode\n",
    "        ('scaler', StandardScaler(), numeric_features)  # Scale numeric features\n",
    "    ])),\n",
    "    ('feature_selector', SelectKBest(score_func=f_classif, k=10)),  # Select top K important features\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # Random Forest classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba02bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5c355c3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This pipeline does the following:\n",
    "1. Preprocesses the data: Fills missing values in numeric features with the mean, fills missing values in categorical features with the mode, and scales numeric features.\n",
    "2. Uses SelectKBest to select the top K important features based on the f_classif score.\n",
    "3. Applies a RandomForestClassifier as the final model.\n",
    "\n",
    "You can adjust the parameters (e.g., the number of selected features, the classifier hyperparameters) to better suit your data and project requirements. Additionally, you need to define `numeric_features` and `categorical_features` based on your dataset's column names and their types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf679fd5",
   "metadata": {},
   "source": [
    "as per the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9875d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and preprocess your data, split it into X_train, X_test, y_train, y_test\n",
    "# ...\n",
    "\n",
    "# Numerical Pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean\n",
    "    ('scaler', StandardScaler())  # Scale numerical columns\n",
    "])\n",
    "\n",
    "# Categorical Pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with mode\n",
    "    ('encoder', OneHotEncoder())  # One-hot encode categorical columns\n",
    "])\n",
    "\n",
    "# Combine Numerical and Categorical Pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),  # Apply numerical pipeline to numerical features\n",
    "    ('cat', categorical_pipeline, categorical_features)  # Apply categorical pipeline to categorical features\n",
    "])\n",
    "\n",
    "# Final Pipeline with Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocess data\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # Random Forest classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb957e",
   "metadata": {},
   "source": [
    "In this pipeline:\n",
    "\n",
    "The numerical pipeline imputes missing values using the mean and scales the numerical features using standardization.\n",
    "The categorical pipeline imputes missing values using the most frequent value and one-hot encodes categorical features.\n",
    "The ColumnTransformer combines the results of both pipelines.\n",
    "The final pipeline includes preprocessing using the combined transformers and a Random Forest Classifier.\n",
    "Interpretation:\n",
    "You can interpret the results by analyzing the accuracy on the test dataset. Higher accuracy indicates a well-performing model. Additionally, you can use feature importances from the Random Forest model to understand which features contribute most to the prediction.\n",
    "\n",
    "Possible Improvements:\n",
    "\n",
    "Experiment with different imputation strategies (e.g., median) to handle missing values.\n",
    "Fine-tune hyperparameters of the Random Forest Classifier for better performance.\n",
    "Explore other classification algorithms and compare their performance with the Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f2e0cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03844e6e",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f4d471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d88887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13533c78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "iris.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75bf5854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c386f38b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.feature_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f89e65b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53e28c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create individual classifiers\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Create a Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('logistic_regression', lr_classifier)\n",
    "], voting='soft')  # Use 'soft' for probabilities-based voting\n",
    "\n",
    "# Build a pipeline with the Voting Classifier\n",
    "pipeline = Pipeline([\n",
    "    ('classifier', voting_classifier)\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfe02083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8723ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46dc9590",
   "metadata": {},
   "source": [
    "An accuracy of 1.0 (or 100%) on the test dataset is a perfect score, which means that your model's predictions perfectly match the true labels in the test set. While achieving a perfect accuracy score is desirable, it's important to critically assess your results and understand the potential reasons for such a high accuracy:\n",
    "\n",
    "1. **Small and Well-Balanced Dataset**: The Iris dataset is a relatively small dataset with only 150 samples and 3 classes. Additionally, the dataset is well-balanced, meaning that the number of samples for each class is roughly equal. A small and balanced dataset can make it easier for the model to learn the underlying patterns and achieve high accuracy.\n",
    "\n",
    "2. **Simple Data Patterns**: The Iris dataset contains relatively simple patterns that can be easily learned by both the Random Forest and Logistic Regression classifiers. The distinct characteristics of each class (setosa, versicolor, and virginica) make it easier for the models to discriminate between them.\n",
    "\n",
    "3. **Ensemble Model**: The Voting Classifier combines predictions from multiple individual classifiers (Random Forest and Logistic Regression) using a majority vote or weighted probabilities. This ensemble approach often results in improved performance, especially when the individual classifiers are diverse and contribute complementary strengths.\n",
    "\n",
    "4. **Default Parameters**: The default hyperparameters used in both the Random Forest and Logistic Regression classifiers might have led to favorable results for this specific dataset. However, default parameters might not always perform as well on other datasets, so it's important to tune hyperparameters based on the specific problem.\n",
    "\n",
    "5. **Overfitting**: While you achieved perfect accuracy on the test set, it's also crucial to check for overfitting. Overfitting occurs when the model learns the training data's noise instead of the underlying patterns. To ensure that the model generalizes well to new, unseen data, cross-validation and further testing on different datasets are recommended.\n",
    "\n",
    "6. **Feature Engineering**: The Iris dataset's features might be inherently well-suited for the task, contributing to high accuracy. In real-world scenarios, feature engineering and selection play a crucial role in improving model performance.\n",
    "\n",
    "To understand the contributions of each model and the ensemble effect, you could analyze the individual classifiers' performance and investigate feature importances for the Random Forest model. Additionally, trying different hyperparameter settings and performing more rigorous evaluation (e.g., using cross-validation) can provide a more comprehensive view of your model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c60c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aadc938f",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593902c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560d1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729699f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16055e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d44038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
