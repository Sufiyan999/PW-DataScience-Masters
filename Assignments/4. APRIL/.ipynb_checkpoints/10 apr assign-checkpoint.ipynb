{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a160e01a",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 1 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238af5c",
   "metadata": {},
   "source": [
    "This problem can be solved using conditional probability. We are asked to find the probability that an employee is a smoker given that he/she uses the health insurance plan. This can be expressed as:\n",
    "\n",
    "\\[ P(Smoker | Uses Insurance Plan  ) \\]\n",
    "\n",
    "We are given the following information:\n",
    "- \\( P( Uses Insurance Plan ) = 0.70 \\) (probability that an employee uses the insurance plan)\n",
    "- \\( P(Smoker | Uses Insurance Plan ) = 0.40 \\) (probability that an employee is a smoker given that they use the insurance plan)\n",
    "\n",
    "Using the formula for conditional probability:\n",
    "\n",
    "\\[ P(A|B) = P(A ∩ B) / P(B) \\]\n",
    "\n",
    "Where \\( A \\) represents \"Smoker\" and \\( B \\) represents \"Uses Insurance Plan.\"\n",
    "\n",
    "We can plug in the given values:\n",
    "\n",
    "\\[ P(Smoker | Uses Insurance Plan ) = P(Smoker ∩ Uses Insurance Plan) / P( Uses Insurance Plan) \\]\n",
    "\n",
    "We are not given \\( P(\\text{Smoker} \\cap \\text{Uses Insurance Plan}) \\), but we can calculate it using the information given:\n",
    "\\[ P(Smoker ∩ Uses Insurance Plan) = P(Uses Insurance Plan) * P(Smoker | Uses Insurance Plan ) \\]\n",
    "\\[ P(Smoker ∩ Uses Insurance Plan) = 0.70 * 0.40 = 0.28 \\]\n",
    "\n",
    "Now we can calculate the conditional probability:\n",
    "\\[ P(Smoker | ses Insurance Plan) =0.28 / 0.70 =  0.4 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.4 or 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c456e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that an employee is a smoker given they use the health insurance plan: 0.39999999999999997\n"
     ]
    }
   ],
   "source": [
    "# Given probabilities\n",
    "P_uses_insurance_plan = 0.70  # Probability that an employee uses the insurance plan\n",
    "P_smoker_given_uses_plan = 0.40  # Probability that an employee is a smoker given they use the insurance plan\n",
    "\n",
    "# Calculate the probability of smoker given uses insurance plan\n",
    "P_smoker_and_uses_plan = P_uses_insurance_plan * P_smoker_given_uses_plan\n",
    "P_smoker_given_uses_plan = P_smoker_and_uses_plan / P_uses_insurance_plan\n",
    "\n",
    "print(\"Probability that an employee is a smoker given they use the health insurance plan:\",P_smoker_given_uses_plan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab4c8bb",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 2 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c1c56",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are two different variants of the Naive Bayes algorithm, each designed to handle different types of data and scenarios.\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - Bernoulli Naive Bayes is used for binary classification tasks, where the features are binary variables (0 or 1).\n",
    "   - It's commonly used for text classification tasks, where each feature represents the presence or absence of a word in a document.\n",
    "   - Assumes that the features are independent of each other given the class label.\n",
    "   - It models the presence or absence of features using a Bernoulli distribution.\n",
    "   - A common application is spam detection, where each feature could represent the presence of certain words in an email.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Multinomial Naive Bayes is used for text classification tasks where the features represent the frequency or count of words.\n",
    "   - It's suitable for cases where the features are discrete and represent counts of occurrences.\n",
    "   - Assumes that the features follow a multinomial distribution.\n",
    "   - Often used in natural language processing tasks like sentiment analysis, topic classification, and document categorization.\n",
    "   - It's an extension of the Bernoulli Naive Bayes that accounts for the frequency of terms in addition to their presence/absence.\n",
    "\n",
    "In summary, the main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed to handle. Bernoulli Naive Bayes is used for binary features (presence/absence), while Multinomial Naive Bayes is used for features representing counts or frequencies. The choice between them depends on the nature of your data and the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70bb27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38155233",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 3 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bda40",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes handles missing values by assuming that missing values are equivalent to the absence of a feature. In other words, when a feature value is missing for a particular instance, Bernoulli Naive Bayes treats it as if the feature is not present for that instance. This assumption aligns with the binary nature of Bernoulli Naive Bayes, where features are binary variables (0 or 1) representing the presence or absence of certain characteristics.\n",
    "\n",
    "When performing classification using Bernoulli Naive Bayes, the algorithm calculates probabilities based on the presence or absence of features in each instance. If a feature's value is missing for an instance, the algorithm assumes that the feature is absent (assigned a value of 0) for that instance.\n",
    "\n",
    "Here's how Bernoulli Naive Bayes handles missing values during classification:\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - During the training phase, the algorithm estimates the probabilities of features being present (1) or absent (0) for each class based on the training data.\n",
    "   - If a feature value is missing in the training data, it's treated as if the feature is absent (assigned a value of 0) for that instance during probability estimation.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - When making predictions for a new instance with missing feature values, Bernoulli Naive Bayes assumes those features are absent (assigned a value of 0).\n",
    "   - The algorithm calculates the probability of each class given the presence (1) or absence (0) of features in the new instance.\n",
    "\n",
    "It's important to note that this handling of missing values simplifies the calculation process, but it assumes that missing values have the same meaning as the absence of a feature. If missing values carry a different meaning or significance, more sophisticated approaches may be necessary, such as imputation techniques or more advanced variations of Naive Bayes, like Gaussian Naive Bayes or Multinomial Naive Bayes, which can handle different types of data and missing values more flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916dcccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a46e974d",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 4 </p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c7ca75",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. It's particularly suitable for continuous numerical data. While it's often associated with binary classification, it can also be extended to handle multi-class classification problems.\n",
    "\n",
    "The approach for using Gaussian Naive Bayes for multi-class classification is straightforward:\n",
    "\n",
    "1. **Training Phase**:\n",
    "   - For each class in the dataset, calculate the mean and standard deviation of each feature.\n",
    "   - These statistics describe the distribution of the feature values for each class.\n",
    "\n",
    "2. **Prediction Phase**:\n",
    "   - Given a new instance with feature values, calculate the likelihood of the instance's feature values under each class's Gaussian distribution using the mean and standard deviation obtained from the training phase.\n",
    "   - Multiply the likelihoods with the prior probabilities of each class.\n",
    "   - Normalize the probabilities to get the posterior probabilities.\n",
    "   - Predict the class with the highest posterior probability.\n",
    "\n",
    "Gaussian Naive Bayes can handle multi-class problems by applying the same principles of conditional probability and Bayes' theorem as in binary classification. Each class's distribution is modeled as a Gaussian distribution, and the likelihoods of feature values under each class's distribution are calculated to determine the most likely class.\n",
    "\n",
    "While Gaussian Naive Bayes can work for multi-class problems, it's important to consider the assumptions of the Gaussian distribution and the independence assumption of Naive Bayes. If the data significantly deviates from these assumptions, other algorithms or variations of Naive Bayes (such as Multinomial or Complement Naive Bayes) might be more appropriate for multi-class classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f50dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7899cb33",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> \n",
    " # <p style=\"padding:10px;background-color: #00004d ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">Ans 5 </p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b535b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffece2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7355b9b",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> \n",
    " # <p style=\"padding:10px;background-color: #01DFD7 ;margin:10;color: white ;font-family:newtimeroman;font-size:100%;text-align:center;border-radius: 10px 10px ;overflow:hidden;font-weight:50\">END</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff08a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15d3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839c168b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f696170b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca496011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
