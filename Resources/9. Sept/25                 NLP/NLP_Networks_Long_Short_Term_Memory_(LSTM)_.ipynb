{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-AQppEyu_RX",
        "outputId": "1ac815bb-ae39-4589-a4f7-73af2009e864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Epoch [1/10], Loss: 0.6935\n",
            "Sentiment Analysis Epoch [2/10], Loss: 0.6880\n",
            "Sentiment Analysis Epoch [3/10], Loss: 0.6824\n",
            "Sentiment Analysis Epoch [4/10], Loss: 0.6761\n",
            "Sentiment Analysis Epoch [5/10], Loss: 0.6683\n",
            "Sentiment Analysis Epoch [6/10], Loss: 0.6586\n",
            "Sentiment Analysis Epoch [7/10], Loss: 0.6461\n",
            "Sentiment Analysis Epoch [8/10], Loss: 0.6302\n",
            "Sentiment Analysis Epoch [9/10], Loss: 0.6100\n",
            "Sentiment Analysis Epoch [10/10], Loss: 0.5845\n",
            "NER Epoch [1/10], Loss: 2.3030\n",
            "NER Epoch [2/10], Loss: 2.3010\n",
            "NER Epoch [3/10], Loss: 2.2991\n",
            "NER Epoch [4/10], Loss: 2.2971\n",
            "NER Epoch [5/10], Loss: 2.2952\n",
            "NER Epoch [6/10], Loss: 2.2932\n",
            "NER Epoch [7/10], Loss: 2.2910\n",
            "NER Epoch [8/10], Loss: 2.2886\n",
            "NER Epoch [9/10], Loss: 2.2859\n",
            "NER Epoch [10/10], Loss: 2.2828\n",
            "Language Generation Epoch [1/10], Loss: 9.2123\n",
            "Language Generation Epoch [2/10], Loss: 9.2020\n",
            "Language Generation Epoch [3/10], Loss: 9.1914\n",
            "Language Generation Epoch [4/10], Loss: 9.1799\n",
            "Language Generation Epoch [5/10], Loss: 9.1668\n",
            "Language Generation Epoch [6/10], Loss: 9.1516\n",
            "Language Generation Epoch [7/10], Loss: 9.1335\n",
            "Language Generation Epoch [8/10], Loss: 9.1111\n",
            "Language Generation Epoch [9/10], Loss: 9.0825\n",
            "Language Generation Epoch [10/10], Loss: 9.0446\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Introduction to Long Short Term Memory (LSTM)\n",
        "class SentimentAnalysisLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
        "        super(SentimentAnalysisLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 2)  # Binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        output, _ = self.lstm(embed)\n",
        "        output = self.fc(output[:, -1, :])  # Use last time step's output for classification\n",
        "        return output\n",
        "\n",
        "# Instantiate models and provide data for training where needed\n",
        "# Sentiment Analysis using LSTM\n",
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "sentiment_model = SentimentAnalysisLSTM(vocab_size, embedding_dim, hidden_size, num_layers)\n",
        "\n",
        "# Define training data for sentiment analysis\n",
        "train_data = torch.randint(0, vocab_size, (128, 50))  # Example data with batch size 128 and sequence length 50\n",
        "train_labels = torch.randint(0, 2, (128,))  # Example binary sentiment labels\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(sentiment_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = sentiment_model(train_data)\n",
        "    loss = criterion(output, train_labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Sentiment Analysis Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Named Entity Recognition with LSTM\n",
        "class NERLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes):\n",
        "        super(NERLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        output, _ = self.lstm(embed)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Instantiate NER model and provide data for training\n",
        "num_classes = 10  # Replace with the number of classes for NER\n",
        "ner_model = NERLSTM(vocab_size, embedding_dim, hidden_size, num_layers, num_classes)\n",
        "\n",
        "# Define training data for NER\n",
        "train_data_ner = torch.randint(0, vocab_size, (128, 30))  # Example data with batch size 128 and sequence length 30\n",
        "train_labels_ner = torch.randint(0, num_classes, (128, 30))  # Example NER labels\n",
        "\n",
        "# Loss and optimizer for NER\n",
        "criterion_ner = nn.CrossEntropyLoss()\n",
        "optimizer_ner = torch.optim.Adam(ner_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop for NER\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer_ner.zero_grad()\n",
        "    output = ner_model(train_data_ner)\n",
        "    loss = criterion_ner(output.view(-1, num_classes), train_labels_ner.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer_ner.step()\n",
        "    print(f\"NER Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Language Generation with LSTM\n",
        "class LanguageGenerationLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
        "        super(LanguageGenerationLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embed = self.embedding(x)\n",
        "        output, _ = self.lstm(embed)\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "# Instantiate language generation model and provide data for training\n",
        "language_model = LanguageGenerationLSTM(vocab_size, embedding_dim, hidden_size, num_layers)\n",
        "\n",
        "# Define training data for language generation\n",
        "train_data_lang = torch.randint(0, vocab_size, (128, 20))  # Example data with batch size 128 and sequence length 20\n",
        "\n",
        "# Loss and optimizer for language generation\n",
        "criterion_lang = nn.CrossEntropyLoss()\n",
        "optimizer_lang = torch.optim.Adam(language_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop for language generation\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer_lang.zero_grad()\n",
        "    output = language_model(train_data_lang)\n",
        "    loss = criterion_lang(output.view(-1, vocab_size), train_data_lang.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer_lang.step()\n",
        "    print(f\"Language Generation Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ... (Previous code for model definitions, data, and training setup)\n",
        "\n",
        "# Function to generate text using the Language Generation model\n",
        "def generate_text(model, start_token, max_length=50):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        current_token = start_token\n",
        "        generated_text = [current_token]\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Generate the next token\n",
        "            input_tensor = torch.tensor([[current_token]])  # Convert current token to tensor\n",
        "            output = model(input_tensor)\n",
        "            next_token_probs = output[0, -1, :]  # Get the probabilities of the next token\n",
        "            next_token = torch.argmax(next_token_probs).item()  # Select the token with the highest probability\n",
        "\n",
        "            generated_text.append(next_token)\n",
        "            current_token = next_token\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text using the Language Generation model\n",
        "start_token = torch.randint(0, vocab_size, (1, 1))  # Start with a random token from the vocabulary\n",
        "generated_text = generate_text(language_model, start_token, max_length=100)  # You can adjust the max_length as needed\n",
        "print(\"Generated Text:\", generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOBBdfEUvcmD",
        "outputId": "a3a22b07-48f7-4534-8089-15524f487ed3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: [tensor([[9691]]), 4826, 5959, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765, 6765]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you have a magical machine that can write stories. You start by giving it a single word, any word you like, let's say \"apple.\" This machine then thinks for a moment and comes up with the next word that it thinks should come after \"apple.\" It might say \"tree.\" Then it thinks again and says \"grew,\" and so on.\n",
        "\n",
        "So, the output you see is like a list of words, starting with your chosen word (in this case, \"apple\") and continuing on, with each word chosen by the machine to follow the previous one. It keeps doing this until it reaches a certain length or stops when it thinks the story is complete.\n",
        "\n",
        "In technical terms, the output is a sequence of words, but in simple terms, it's like a machine telling a story one word at a time, with each word based on what it thinks should come next. The quality of the story depends on how well the machine has learned from the stories it was trained on and the word choices it makes.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WXwCqwNBwAxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggijc5nRwDTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}