{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing neccessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Doc2Vec is used for creating document embeddings. It captures the context of entire documents.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize into sentences and words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize into sentences and words\n",
      "Tokenized sentences: [['doc2vec', 'is', 'used', 'for', 'creating', 'document', 'embeddings', '.'], ['it', 'captures', 'the', 'context', 'of', 'entire', 'documents', '.']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenize into sentences and words\")\n",
    "sentences = sent_tokenize(text)\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "print(\"Tokenized sentences:\", tokenized_sentences)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tagged documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare tagged documents\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare tagged documents\")\n",
    "tagged_data = [TaggedDocument(words=words, tags=[str(idx)]) for idx, words in enumerate(tokenized_sentences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Doc2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train the Doc2Vec model\n",
      "Doc2Vec model trained successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train the Doc2Vec model\")\n",
    "model = Doc2Vec(vector_size=100, window=5, min_count=1, dm=1, epochs=20)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print(\"Doc2Vec model trained successfully\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer document vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer document vectors\n",
      "Inferred document vector: [-4.1424781e-03 -4.0114441e-04  2.3183709e-03 -4.0177065e-03\n",
      " -1.1191838e-03  1.6187805e-03 -1.1995027e-03  1.4420497e-03\n",
      " -3.1252012e-03  6.0391880e-04 -2.2552486e-03 -2.6541543e-03\n",
      " -2.8061941e-03 -3.8749811e-03  2.4329959e-03  2.7665100e-03\n",
      "  3.0000391e-03 -2.6117559e-03 -2.9998017e-03 -4.0841778e-03\n",
      " -9.7367662e-04 -1.6249843e-03  3.5360865e-03 -3.0385721e-03\n",
      " -3.4603369e-04  9.3565596e-04  3.9531058e-03  2.3074539e-03\n",
      " -3.1594697e-03 -6.5406627e-04  4.1001802e-03 -1.6420709e-03\n",
      " -4.0211721e-04  3.7291690e-04  2.9035779e-03  2.5485682e-03\n",
      "  3.2693946e-03 -3.7717199e-04 -2.9156294e-03 -2.1361746e-03\n",
      " -1.6228079e-03  9.8268050e-05 -5.9050729e-04 -1.2567226e-03\n",
      "  4.0058882e-04 -3.5795898e-04 -2.5644591e-03  3.7930338e-03\n",
      " -1.5285388e-03  4.4577629e-03  4.5979250e-04 -4.2725531e-03\n",
      "  2.6322978e-03  4.4851108e-03  5.9749582e-04  1.7769572e-03\n",
      "  2.3198908e-03  3.2828856e-04 -5.1420962e-04  2.6940787e-03\n",
      "  1.7399180e-03  7.7003997e-04  4.3889275e-03 -3.5646514e-03\n",
      "  1.4130342e-04  7.2687835e-04  4.9283486e-03 -2.4494820e-03\n",
      " -2.4492079e-03  1.4941620e-03  6.5539888e-04  1.1481899e-03\n",
      "  2.9699679e-03 -4.2652413e-03 -1.8151434e-03 -2.8109823e-03\n",
      "  3.1397271e-03 -2.6815457e-03 -4.4671916e-03 -4.0437025e-03\n",
      "  1.9741410e-03  1.8225993e-03 -9.3588198e-04 -4.3253945e-03\n",
      " -2.9208564e-05 -2.6947877e-03  3.9656786e-03 -4.9754540e-03\n",
      "  3.7630859e-03  4.5251148e-03 -3.5711993e-03 -3.5776829e-04\n",
      " -3.3149940e-03  1.2902269e-03  1.9821373e-03 -4.6929945e-03\n",
      " -7.7141327e-04  1.0693368e-03 -2.8376470e-03 -4.0943255e-03]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Infer document vectors\")\n",
    "doc_vector = model.infer_vector(word_tokenize(\"Doc2Vec is a powerful tool for document embeddings.\"))\n",
    "print(\"Inferred document vector:\", doc_vector)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
